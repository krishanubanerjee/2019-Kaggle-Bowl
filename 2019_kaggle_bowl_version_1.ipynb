{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2019 Data Science Bowl\n",
    "\n",
    "#### Description and Challenge of the project\n",
    "<br> https://www.kaggle.com/c/data-science-bowl-2019/overview\n",
    "<br> In this project game analytics data for kids given by PBS KIDS. In this app kids can play\n",
    "<br> games, watch clips and perform different activities and after some game session they are assigned \n",
    "<br> test.Kids can take multiple tests or single test or many records for kids with no test.\n",
    "<br> If they cam solve test correctly in single attempt then their accuracy group is 3, if two attempt\n",
    "<br> to solve correctly then accuracy group is 2, if multiple atempts then 1 if could not solve then 0.\n",
    "<br> Dataset is hierchical. For every apps installed, one installation id is assigned. We can consider\n",
    "<br> one kid has one installation id (assuming no shared device). One kid can play multiple game sessions\n",
    "<br> and one game session might have multiple events. Some of the variables like activity types, title, world\n",
    "<br> etc. are in game session level and some event related variables are at lower level.\n",
    "<br> Target variable is accuracy group.\n",
    "<br> The challenge is in train data accuracy group is at game session level\n",
    "<br> but we have to predict in test data at installation id level, in higher level.\n",
    "<br> Moreover only 1000 test records are given but Kaggle is measuring accuracy on 8 MM records.\n",
    "\n",
    "#### Approach for this notebook\n",
    "<br> Feature engineering is done using inetrmedate panda data frame.\n",
    "<br> For training data , assessment game sessions identified and extracted some features.\n",
    "<br> Then a look up table is created to capture all activities before assessment.\n",
    "<br> Here assumption is that activities before  assessment will impact accuracy.\n",
    "<br> After that , different features created mainly number of activities and game time.\n",
    "<br> Irrelevent features are excluded before model building. \n",
    "<br> Different models and hyperparameter tuning performed.\n",
    "<br> XGBoost and Deep Learning in Keras framework presented here.\n",
    "<br> Same feature extraction strategy applied to test data primrily but did not go very well.\n",
    "<br> For test data similar approach but at installation id levels features extraction performed.\n",
    "<br> Final scores measured using Cohen-Kappa score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing Required modules and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from matplotlib import pyplot\n",
    "from time import time\n",
    "from collections import Counter\n",
    "import datetime\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold,train_test_split\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "import gc\n",
    "import json\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ind='local'\n",
    "\n",
    "if path_ind=='kaggle':\n",
    "    file_path=\"/kaggle/input/data-science-bowl-2019\"\n",
    "else:\n",
    "    file_path=\"/Users/krishanubanerjee/Downloads/data-science-bowl-2019\"\n",
    "    \n",
    "\n",
    "train=pd.read_csv(file_path+\"/train.csv\")\n",
    "train_labels=pd.read_csv(file_path+\"/train_labels.csv\")\n",
    "test=pd.read_csv(file_path+\"/test.csv\")\n",
    "sample_submission=pd.read_csv(file_path+\"/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Engineering for Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_grp_calc(x):\n",
    "    \"\"\"\n",
    "    This function will assign accuracy to accuracy group\n",
    "    \"\"\"\n",
    "    if x==0:\n",
    "        return(0)\n",
    "    elif x==1:\n",
    "        return(3)\n",
    "    elif x==0.5:\n",
    "        return(2)\n",
    "    else:\n",
    "        return(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assessment_df(input_flg):\n",
    "    \"\"\"\n",
    "    This function will identify assessment for train or test and create some related features\n",
    "    from different assessments and related titles. This function will also derive fetures from \n",
    "    multiple assessment takers cumulatively\n",
    "    \"\"\"\n",
    "    assert ((input_flg == 'train') | (input_flg == 'test')),\"Input is not train or test\"\n",
    "    if input_flg=='train':\n",
    "        df_ass=train[train['type']=='Assessment']\\\n",
    "             [((train['event_code']==4100)&(train['title']!='Bird Measurer (Assessment)'))\\\n",
    "             |((train['event_code']==4110)&(train['title']=='Bird Measurer (Assessment)'))]\n",
    "    else:\n",
    "        df_ass=test[test['type']=='Assessment']\n",
    "        \n",
    "    df_ass['correct_attempt']=np.where(df_ass['event_data'].str.contains('true'),1,0)\n",
    "    df_ass['incorrect_attempt']=np.where(df_ass['event_data'].str.contains('false'),1,0)\n",
    "    df_ass=df_ass.groupby(['installation_id','game_session','title']).agg({'correct_attempt':'sum',\\\n",
    "                                                          'incorrect_attempt':'sum'}).reset_index()\n",
    "    df_ass['accuracy']=np.where(df_ass['correct_attempt']+df_ass['incorrect_attempt']==0,0,\\\n",
    "             round(df_ass['correct_attempt']/(df_ass['correct_attempt']+df_ass['incorrect_attempt']),1))\n",
    "    df_ass['accuracy_group']=df_ass['accuracy'].apply(lambda x:acc_grp_calc(x))\n",
    "    df_ass['accumulated_correct']=df_ass.groupby('installation_id').agg({'correct_attempt':np.cumsum})\n",
    "    df_ass['accumulated_incorrect']=df_ass.groupby('installation_id').agg({'incorrect_attempt':np.cumsum})\n",
    "    # correct and incorrect attempt by type of assessment\n",
    "    df_ass['Mushroom_sorter_correct']=df_ass[df_ass['title']=='Mushroom Sorter (Assessment)']\\\n",
    "                               .groupby('installation_id').agg({'correct_attempt':np.cumsum})\n",
    "    df_ass['Mushroom_sorter_incorrect']=df_ass[df_ass['title']=='Mushroom Sorter (Assessment)']\\\n",
    "                               .groupby('installation_id').agg({'incorrect_attempt':np.cumsum})\n",
    "    df_ass['Bird_Measurer_correct']=df_ass[df_ass['title']=='Bird Measurer (Assessment)']\\\n",
    "                               .groupby('installation_id').agg({'correct_attempt':np.cumsum})\n",
    "    df_ass['Bird_Measurer_incorrect']=df_ass[df_ass['title']=='Bird Measurer (Assessment)']\\\n",
    "                               .groupby('installation_id').agg({'incorrect_attempt':np.cumsum})\n",
    "    df_ass['Cauldron_Filler_correct']=df_ass[df_ass['title']=='Cauldron Filler (Assessment)']\\\n",
    "                               .groupby('installation_id').agg({'correct_attempt':np.cumsum})\n",
    "    df_ass['Cauldron_Filler_incorrect']=df_ass[df_ass['title']=='Cauldron Filler (Assessment)']\\\n",
    "                               .groupby('installation_id').agg({'incorrect_attempt':np.cumsum})\n",
    "    df_ass['Cart_Balancer_correct']=df_ass[df_ass['title']=='Cart Balancer (Assessment)']\\\n",
    "                               .groupby('installation_id').agg({'correct_attempt':np.cumsum})\n",
    "    df_ass['Cart_Balancer_incorrect']=df_ass[df_ass['title']=='Cart Balancer (Assessment)']\\\n",
    "                               .groupby('installation_id').agg({'incorrect_attempt':np.cumsum})\n",
    "    df_ass['Chest_Sorter_correct']=df_ass[df_ass['title']=='Chest Sorter (Assessment)']\\\n",
    "                               .groupby('installation_id').agg({'correct_attempt':np.cumsum})\n",
    "    df_ass['Chest_Sorter_incorrect']=df_ass[df_ass['title']=='Chest Sorter (Assessment)']\\\n",
    "                               .groupby('installation_id').agg({'incorrect_attempt':np.cumsum})\n",
    "    df_ass=df_ass.drop(['title'],axis=1)\n",
    "    #  Cumulative accuracy group\n",
    "    df_ass['accuracy_0']=np.where(df_ass['accuracy_group']==0,1,0)\n",
    "    df_ass['accuracy_0']=df_ass.groupby('installation_id').agg({'accuracy_0':np.cumsum})\n",
    "    df_ass['accuracy_1']=np.where(df_ass['accuracy_group']==1,1,0)\n",
    "    df_ass['accuracy_1']=df_ass.groupby('installation_id').agg({'accuracy_1':np.cumsum})\n",
    "    df_ass['accuracy_2']=np.where(df_ass['accuracy_group']==2,1,0)\n",
    "    df_ass['accuracy_2']=df_ass.groupby('installation_id').agg({'accuracy_2':np.cumsum})\n",
    "    df_ass['accuracy_3']=np.where(df_ass['accuracy_group']==3,1,0)\n",
    "    df_ass['accuracy_3']=df_ass.groupby('installation_id').agg({'accuracy_3':np.cumsum})\n",
    "    \n",
    "    \n",
    "    df_ass['cumcnt']=df_ass.groupby('installation_id').cumcount()\n",
    "    df_ass=df_ass.drop(['correct_attempt','incorrect_attempt','accuracy'],axis=1)\n",
    "\n",
    "    df_ass=df_ass.fillna(0)\n",
    "    if input_flg=='train':\n",
    "        df_ass=df_ass.drop(['cumcnt'],axis=1)\n",
    "    else:\n",
    "        df_ass=df_ass.drop(['cumcnt','accuracy_group'],axis=1)\n",
    "    return(df_ass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assessment_test_df(input_flg):\n",
    "    \"\"\"\n",
    "    This function will get assessment for Test and extract related features\n",
    "    \"\"\"\n",
    "    assert (input_flg == 'test'),\"Input is not test\"\n",
    "    df_ass=test[test['type']=='Assessment']   \n",
    "    df_ass['correct_attempt']=np.where(df_ass['event_data'].str.contains('true'),1,0)\n",
    "    df_ass['incorrect_attempt']=np.where(df_ass['event_data'].str.contains('false'),1,0)\n",
    "    df_ass=df_ass.groupby(['installation_id','title']).agg({'correct_attempt':'sum',\\\n",
    "                                                          'incorrect_attempt':'sum'}).reset_index()\n",
    "    df_ass['accuracy']=np.where(df_ass['correct_attempt']+df_ass['incorrect_attempt']==0,0,\\\n",
    "             round(df_ass['correct_attempt']/(df_ass['correct_attempt']+df_ass['incorrect_attempt']),1))\n",
    "    df_ass['accuracy_group']=df_ass['accuracy'].apply(lambda x:acc_grp_calc(x)) \n",
    "    df_ass['accumulated_correct']=df_ass.groupby('installation_id').agg({'correct_attempt':np.cumsum})\n",
    "    df_ass['accumulated_incorrect']=df_ass.groupby('installation_id').agg({'incorrect_attempt':np.cumsum})\n",
    "    # correct and incorrect attempt by type of assessment\n",
    "    df_ass['Mushroom_sorter_correct']=df_ass[df_ass['title']=='Mushroom Sorter (Assessment)']\\\n",
    "                               .groupby('installation_id').agg({'correct_attempt':np.cumsum})\n",
    "    df_ass['Mushroom_sorter_incorrect']=df_ass[df_ass['title']=='Mushroom Sorter (Assessment)']\\\n",
    "                               .groupby('installation_id').agg({'incorrect_attempt':np.cumsum})\n",
    "    df_ass['Bird_Measurer_correct']=df_ass[df_ass['title']=='Bird Measurer (Assessment)']\\\n",
    "                               .groupby('installation_id').agg({'correct_attempt':np.cumsum})\n",
    "    df_ass['Bird_Measurer_incorrect']=df_ass[df_ass['title']=='Bird Measurer (Assessment)']\\\n",
    "                               .groupby('installation_id').agg({'incorrect_attempt':np.cumsum})\n",
    "    df_ass['Cauldron_Filler_correct']=df_ass[df_ass['title']=='Cauldron Filler (Assessment)']\\\n",
    "                               .groupby('installation_id').agg({'correct_attempt':np.cumsum})\n",
    "    df_ass['Cauldron_Filler_incorrect']=df_ass[df_ass['title']=='Cauldron Filler (Assessment)']\\\n",
    "                               .groupby('installation_id').agg({'incorrect_attempt':np.cumsum})\n",
    "    df_ass['Cart_Balancer_correct']=df_ass[df_ass['title']=='Cart Balancer (Assessment)']\\\n",
    "                               .groupby('installation_id').agg({'correct_attempt':np.cumsum})\n",
    "    df_ass['Cart_Balancer_incorrect']=df_ass[df_ass['title']=='Cart Balancer (Assessment)']\\\n",
    "                               .groupby('installation_id').agg({'incorrect_attempt':np.cumsum})\n",
    "    df_ass['Chest_Sorter_correct']=df_ass[df_ass['title']=='Chest Sorter (Assessment)']\\\n",
    "                               .groupby('installation_id').agg({'correct_attempt':np.cumsum})\n",
    "    df_ass['Chest_Sorter_incorrect']=df_ass[df_ass['title']=='Chest Sorter (Assessment)']\\\n",
    "                               .groupby('installation_id').agg({'incorrect_attempt':np.cumsum})\n",
    "    #  Cumulative accuracy group\n",
    "    df_ass['accuracy_0']=np.where(df_ass['accuracy_group']==0,1,0)\n",
    "    df_ass['accuracy_0']=df_ass.groupby('installation_id').agg({'accuracy_0':np.cumsum})\n",
    "    df_ass['accuracy_1']=np.where(df_ass['accuracy_group']==1,1,0)\n",
    "    df_ass['accuracy_1']=df_ass.groupby('installation_id').agg({'accuracy_1':np.cumsum})\n",
    "    df_ass['accuracy_2']=np.where(df_ass['accuracy_group']==2,1,0)\n",
    "    df_ass['accuracy_2']=df_ass.groupby('installation_id').agg({'accuracy_2':np.cumsum})\n",
    "    df_ass['accuracy_3']=np.where(df_ass['accuracy_group']==3,1,0)\n",
    "    df_ass['accuracy_3']=df_ass.groupby('installation_id').agg({'accuracy_3':np.cumsum})\n",
    "    \n",
    "    df_ass=df_ass.drop(['correct_attempt','incorrect_attempt','accuracy','accuracy_group','title'],axis=1)\n",
    "    df_ass=df_ass.groupby(['installation_id']).agg({k:'sum' for k in df_ass.columns if k!='installation_id' }).reset_index()\n",
    "\n",
    "    df_ass=df_ass.fillna(0)\n",
    "    return(df_ass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset_tm_event_cnt(df_input):\n",
    "    \"\"\"\n",
    "    This function will create features for maximum event and game time for all game sessions\n",
    "    \"\"\"\n",
    "    subset_tm_cnt=df_input.groupby(['installation_id','game_session'])\\\n",
    "                                    .agg({'game_time':['max'],\\\n",
    "                                          'event_count':['max']}).reset_index()\n",
    "    subset_tm_cnt.columns=['installation_id','game_session','gm_tm_max','evnt_max']\n",
    "    subset_tm_cnt=subset_tm_cnt.fillna(0)\n",
    "\n",
    "    return(subset_tm_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset_tm_event_cnt_test(df_input):\n",
    "    \"\"\"\n",
    "    Extract average game time and event featires for test specific\n",
    "    \"\"\"\n",
    "    subset_tm_cnt=df_input.groupby(['installation_id','game_session'])\\\n",
    "                                    .agg({'game_time':['max'],\\\n",
    "                                          'event_count':['max']}).reset_index()\n",
    "    subset_tm_cnt.columns=['installation_id','game_session','gm_tm_max','evnt_max']\n",
    "    subset_tm_cnt=subset_tm_cnt.fillna(0)\n",
    "    subset_tm_cnt=subset_tm_cnt.groupby(['installation_id'])\\\n",
    "                            .agg({'gm_tm_max':['max','mean','std'],'evnt_max':['max','mean','std']}).reset_index()\n",
    "    return(subset_tm_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lkp_sessons_bfr_assignement(df_input,df_ass):\n",
    "    \"\"\"\n",
    "    Look up for train to identify all game sessions before assessment.\n",
    "    This will be used for other feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    df_exp=df_input[['timestamp','game_session','installation_id']]\n",
    "    df_exp=df_exp.groupby(['game_session','installation_id']).agg({'timestamp':'max'}).reset_index()\n",
    "\n",
    "    df_gm_ass=df_exp[(df_exp['installation_id'].isin(list(df_ass['installation_id'].unique())))&\\\n",
    "                    (df_exp['game_session'].isin(list(df_ass['game_session'].unique())))]\n",
    "    df_gm_ass=df_gm_ass.rename(columns={\"game_session\": \"target_game_session\", \"timestamp\": \"target_timestamp\"})\n",
    "    df_gm_ass['order']=df_gm_ass.sort_values('target_timestamp').groupby('installation_id').cumcount()\n",
    "    df_gm_ass_max=df_gm_ass.groupby(['installation_id']).agg({'target_timestamp':'max'}).reset_index()\n",
    "    \n",
    "    df_lkp=pd.merge(df_exp,df_gm_ass_max,left_on='installation_id',right_on='installation_id')\n",
    "    df_lkp=df_lkp[df_lkp['timestamp']<=df_lkp['target_timestamp']]\n",
    "    df_lkp=df_lkp.drop(['target_timestamp'],axis=1)\n",
    "    df_lkp=pd.merge(df_lkp,df_gm_ass,left_on=['installation_id','game_session'],\n",
    "                     right_on=['installation_id','target_game_session'],how='left')\\\n",
    "                        .sort_values(by=['installation_id','timestamp'])\n",
    "    df_lkp['target_game_session']=df_lkp['target_game_session'].fillna(method='bfill')\n",
    "    df_lkp['order']=df_lkp['order'].fillna(method='bfill').astype(int)\n",
    "    df_lkp=df_lkp.drop(['timestamp','target_timestamp'],axis=1)\n",
    "    return(df_lkp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_gm_tm_event_cnt(tm_event_cnt,df_lkp):\n",
    "    \"\"\"\n",
    "    Get average event and game related features\n",
    "    \"\"\"\n",
    "    df_avg_tm_event_cnt=pd.merge(tm_event_cnt,df_lkp,on=['installation_id','game_session'])\n",
    "    df_avg_tm_event_cnt=df_avg_tm_event_cnt.drop(['game_session'],axis=1)\n",
    "    df_avg_tm_event_cnt=df_avg_tm_event_cnt.groupby(['installation_id','target_game_session'])\\\n",
    "                        .agg({'gm_tm_max':['max','mean','std'],'evnt_max':['max','mean','std']}).reset_index()\n",
    "    df_avg_tm_event_cnt=df_avg_tm_event_cnt.rename(columns={\"target_game_session\": \"game_session\"})\n",
    "    return(df_avg_tm_event_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_history_data(df_input,df_lkp,val):\n",
    "    \"\"\"\n",
    "    Activity history by installation id,game session\n",
    "    \"\"\" \n",
    "    df_subset=df_input[['installation_id','game_session',val]]\n",
    "    df_subset=df_subset.groupby(['installation_id','game_session',val]).agg({val:'count'})\n",
    "    df_subset=df_subset.rename(columns={val:'total'}).reset_index()\n",
    "    df_subset[val]=df_subset[val].astype(str)\n",
    "\n",
    "    df_pivot=df_subset.pivot_table(index=['installation_id','game_session'],columns=val,values='total').reset_index()\n",
    "    df_pivot=df_pivot.fillna(0)\n",
    "    \n",
    "    train_col=[str(x) for x in list(df_pivot.columns) if x not in ['installation_id','game_session']]\n",
    "    df_pivot=pd.merge(df_pivot,df_lkp,on=['installation_id','game_session'])\n",
    "    df_pivot=df_pivot.drop(['game_session'],axis=1)\n",
    "    df_pivot=df_pivot.groupby(['installation_id','target_game_session','order'])\\\n",
    "                            .agg({i:'sum' for i in train_col }).reset_index()\n",
    "    df_pivot=df_pivot.rename(columns={\"target_game_session\": \"game_session\"})\n",
    "    for i in train_col:\n",
    "        df_pivot[str('cumcnt_')+i]=df_pivot.sort_values('order').groupby(['installation_id']).agg({i:np.cumsum})\n",
    "    df_pivot=df_pivot.drop(list(set(train_col)|{'order'}),axis=1)\n",
    "    return(df_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_history_data_test(df_input,val):\n",
    "    \"\"\"\n",
    "    Activity history for test at installation id level\n",
    "    \"\"\"\n",
    "    df_subset=df_input[['installation_id',val]]\n",
    "    df_subset=df_subset.groupby(['installation_id',val]).agg({val:'count'})\n",
    "    df_subset=df_subset.rename(columns={val:'total'}).reset_index()\n",
    "    df_subset[val]=df_subset[val].astype(str)\n",
    "    df_pivot=df_subset.pivot_table(index=['installation_id'],columns=val,values='total').reset_index()\n",
    "    df_pivot=df_pivot.fillna(0)\n",
    "    df_pivot=df_pivot.rename(columns={k:str('cumcnt_')+str(k) for k in \\\n",
    "                                      list(df_pivot.columns) if k !='installation_id'})\n",
    "    return(df_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity_history(df_input,df_lkp,tm_event_cnt):\n",
    "    \"\"\"\n",
    "    This function will get activity history-number of attempt and time spend for title,type and world\n",
    "    \"\"\"\n",
    "    df_subset=df_input[['installation_id','game_session','title','type','world']]\n",
    "    df_subset=df_subset.drop_duplicates()\n",
    "    df_subset=pd.merge(df_subset,df_lkp,on=['installation_id','game_session'])\n",
    "    df_subset_act=df_subset.drop(['game_session'],axis=1)\n",
    "    df_subset_act=df_subset_act.rename(columns={\"target_game_session\": \"game_session\"})\n",
    "    \n",
    "    def get_category_cnt(val):\n",
    "        \"\"\"\n",
    "        Aggregate and pivot for attempts for variables\n",
    "        \"\"\"\n",
    "        df=df_subset_act.groupby(['installation_id','game_session','order',val]).agg({val:'count'})\n",
    "        df=df.rename(columns={val:'total'}).reset_index()\n",
    "        df=df.pivot_table(index=['installation_id','game_session','order'],\\\n",
    "                          columns=val,values=['total']).fillna(0)\n",
    "        df.columns=df.columns.droplevel(0)\n",
    "        df=df.reset_index()\n",
    "        train_col=[str(x) for x in list(df.columns) if x not in ['installation_id','game_session','order']]\n",
    "        for i in train_col:\n",
    "            df[str('cumcnt_')+i]=df.sort_values('order').groupby(['installation_id']).agg({i:np.cumsum})\n",
    "            \n",
    "        df=df.drop(list(set(train_col)|{'order'}),axis=1)\n",
    "        #df=df.rename(columns={'cnt_installation_id':'installation_id','cnt_game_session':'game_session'})\n",
    "        return(df)\n",
    "    \n",
    "    df_type=get_category_cnt('type')\n",
    "    df_world=get_category_cnt('world')\n",
    "    df_activity=pd.merge(df_world,df_type,on=['installation_id','game_session'])\n",
    "    \n",
    "    \n",
    "    # activity time\n",
    "    df_subset_gm_tm=pd.merge(df_subset,tm_event_cnt,on=['installation_id','game_session'])\n",
    "    df_subset_gm_tm=df_subset_gm_tm.drop(['evnt_max','game_session'],axis=1)\n",
    "    df_subset_gm_tm=df_subset_gm_tm.rename(columns={\"target_game_session\": \"game_session\"})\n",
    "    \n",
    "    def get_category_tm(val):\n",
    "        \"\"\"\n",
    "        Aggregate and pivot game time\n",
    "        \"\"\"\n",
    "        df=df_subset_gm_tm.groupby(['installation_id','game_session','order',val]).agg({'gm_tm_max':'mean'})\n",
    "        df=df.rename(columns={val:'gm_tm_max'}).reset_index()\n",
    "        df['gm_tm_max']=round(df['gm_tm_max']/1000,0)\n",
    "        df=df.pivot_table(index=['installation_id','game_session','order'],\\\n",
    "                        columns=val,values=['gm_tm_max']).fillna(0)\n",
    "        df.columns=df.columns.droplevel(0)\n",
    "        df=df.reset_index()\n",
    "        train_col=[str(x) for x in list(df.columns) if x not in ['installation_id','game_session','order']]\n",
    "        for i in train_col:\n",
    "            df[str('cum_gm_tm_')+i]=df.sort_values('order').groupby(['installation_id']).agg({i:np.cumsum})\n",
    "\n",
    "        df=df.drop(list(set(train_col)|{'order'}),axis=1)\n",
    "        #df=df.rename(columns={'tm_installation_id':'installation_id','tm_game_session':'game_session'})\n",
    "        return(df)\n",
    "    \n",
    "    title_gm=get_category_tm('title')\n",
    "    type_gm=get_category_tm('type')\n",
    "    world_gm=get_category_tm('world')\n",
    "    \n",
    "    df_tm=pd.merge(title_gm,type_gm,on=['installation_id','game_session'])\n",
    "    df_tm=pd.merge(df_tm,world_gm,on=['installation_id','game_session'])\n",
    "    df_activity=pd.merge(df_activity,df_tm,on=['installation_id','game_session'])\n",
    "    return(df_activity)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity_history_test(df_input,tm_event_cnt):\n",
    "    \"\"\"\n",
    "    This function will get activity history-number of attempt and time spend for title,type and world\n",
    "    at installation id level for test\n",
    "    \"\"\"\n",
    "    df_subset=df_input[['installation_id','game_session','title','type','world']]\n",
    "    df_subset=df_subset.drop_duplicates()\n",
    "    def get_category_cnt(val):\n",
    "        \"\"\"\n",
    "        Aggregate and pivot for attemts at installation id level\n",
    "        \"\"\"\n",
    "        df=df_subset.groupby(['installation_id',val]).agg({val:'count'})\n",
    "        df=df.rename(columns={val:'total'}).reset_index()\n",
    "        df=df.pivot_table(index=['installation_id'],\\\n",
    "                              columns=val,values=['total']).fillna(0)\n",
    "        df.columns=df.columns.droplevel(0)\n",
    "        df=df.reset_index()\n",
    "        df=df.rename(columns={k:str('cumcnt_')+str(k) for k in list(df.columns) if k !='installation_id'})\n",
    "        return(df)\n",
    "    \n",
    "    df_title=get_category_cnt('title')\n",
    "    df_type=get_category_cnt('type')\n",
    "    df_world=get_category_cnt('world')\n",
    "    df_activity=pd.merge(df_title,df_type,on=['installation_id'])\n",
    "    df_activity=pd.merge(df_activity,df_type,on=['installation_id'])\n",
    "\n",
    "    # activity time\n",
    "    df_subset_gm_tm=pd.merge(df_subset,tm_event_cnt,on=['installation_id','game_session'])\n",
    "    df_subset_gm_tm=df_subset_gm_tm.drop(['evnt_max','game_session'],axis=1)\n",
    "\n",
    "    def get_category_tm(val):\n",
    "        \"\"\"\n",
    "        Aggregate and pivot game time at installation id level\n",
    "        \"\"\"\n",
    "        df=df_subset_gm_tm.groupby(['installation_id',val]).agg({'gm_tm_max':'mean'})\n",
    "        df=df.rename(columns={val:'gm_tm_max'}).reset_index()\n",
    "        df['gm_tm_max']=round(df['gm_tm_max']/1000,0)\n",
    "        df=df.pivot_table(index=['installation_id'],columns=val,values=['gm_tm_max']).fillna(0)\n",
    "        df.columns=df.columns.droplevel(0)\n",
    "        df=df.reset_index()\n",
    "        df=df.rename(columns={k:str('cum_gm_tm_')+str(k) for k in list(df.columns) if k !='installation_id'})\n",
    "        return(df)\n",
    "\n",
    "    title_gm=get_category_tm('title')\n",
    "    type_gm=get_category_tm('type')\n",
    "    world_gm=get_category_tm('world')\n",
    "\n",
    "    df_tm=pd.merge(title_gm,type_gm,on=['installation_id'])\n",
    "    df_tm=pd.merge(df_tm,world_gm,on=['installation_id'])\n",
    "    df_activity=pd.merge(df_activity,df_tm,on=['installation_id'])\n",
    "    return(df_activity)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_event_code_cnt(df_input,df_lkp):\n",
    "    \"\"\"\n",
    "    Extract a combined feature for title and event code and aggregated per assessment\n",
    "    \"\"\"\n",
    "    df=df_input[['installation_id','game_session','event_code','title']]\n",
    "    df['title_event_code']=df['event_code'].astype(str)+str('_')+df['title'].astype(str)\n",
    "    df=df.drop(['event_code','title'],axis=1)\n",
    "    df=pd.merge(df,df_lkp,on=['installation_id','game_session'])\n",
    "    df=df.drop(['game_session'],axis=1)\n",
    "    df=df.rename(columns={\"target_game_session\": \"game_session\"})\n",
    "    df=df.groupby(['installation_id','game_session','order','title_event_code']).agg({'title_event_code':'size'})\n",
    "    df=df.rename(columns={'title_event_code':'title_event_code_cnt'}).reset_index()\n",
    "    df=df.pivot_table(index=['installation_id','game_session','order'],\\\n",
    "                        columns='title_event_code',values=['title_event_code_cnt']).fillna(0)\n",
    "    df.columns=df.columns.droplevel(0)\n",
    "    df=df.reset_index()\n",
    "    train_col=[str(x) for x in list(df.columns) if x not in ['installation_id','game_session','order']]\n",
    "    for i in train_col:\n",
    "        df[str('cm_cnt_')+i]=df.sort_values('order').groupby(['installation_id']).agg({i:np.cumsum})\n",
    "\n",
    "    df=df.drop(list(set(train_col)|{'order'}),axis=1)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_event_code_cnt_test(df_input):\n",
    "    \"\"\"\n",
    "    Extract a combined feature for title and event code and aggregated per installation id\n",
    "    \"\"\"\n",
    "    df=df_input[['installation_id','event_code','title']]\n",
    "    df['title_event_code']=df['event_code'].astype(str)+str('_')+df['title'].astype(str)\n",
    "    df=df.drop(['event_code','title'],axis=1)\n",
    "    df=df.groupby(['installation_id','title_event_code']).agg({'title_event_code':'size'})\n",
    "    df=df.rename(columns={'title_event_code':'title_event_code_cnt'}).reset_index()\n",
    "    df=df.pivot_table(index=['installation_id'],\\\n",
    "                        columns='title_event_code',values=['title_event_code_cnt']).fillna(0)\n",
    "    df.columns=df.columns.droplevel(0)\n",
    "    df=df.reset_index()\n",
    "    df=df.rename(columns={k:str('cm_cnt_')+str(k) for k in list(df.columns) if k !='installation_id'})\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modified_title_cumcnt(df_input,df_lkp):\n",
    "    \"\"\"\n",
    "    Need to extract cumulative title count \n",
    "    \"\"\"\n",
    "    df_subset=df_input[['installation_id','game_session','title']]\n",
    "    df_subset=pd.merge(df_subset,df_lkp,on=['installation_id','game_session'])\n",
    "    df_subset_act=df_subset.drop(['game_session'],axis=1)\n",
    "    df_subset_act=df_subset_act.rename(columns={\"target_game_session\": \"game_session\"})\n",
    "    df=df_subset_act.groupby(['installation_id','game_session','order','title']).agg({'title':'count'})\n",
    "    df=df.rename(columns={'title':'total'}).reset_index()\n",
    "    df=df.pivot_table(index=['installation_id','game_session','order'],\\\n",
    "                  columns='title',values=['total']).fillna(0)\n",
    "    df.columns=df.columns.droplevel(0)\n",
    "    df=df.reset_index()\n",
    "    train_col=[str(x) for x in list(df.columns) if x not in ['installation_id','game_session','order']]\n",
    "    for i in train_col:\n",
    "        df[str('cumcnt_')+i]=df.sort_values('order').groupby(['installation_id']).agg({i:np.cumsum})\n",
    "\n",
    "    df=df.drop(list(set(train_col)|{'order'}),axis=1)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train(df_input,input_flg):\n",
    "    \"\"\"\n",
    "    Merge then all together and get train data\n",
    "    \"\"\"\n",
    "    df_ass=get_assessment_df(input_flg)\n",
    "    df_lkp=lkp_sessons_bfr_assignement(df_input,df_ass)\n",
    "    tm_event_cnt=get_subset_tm_event_cnt(df_input)\n",
    "    avg_tm_event_cnt=get_avg_gm_tm_event_cnt(tm_event_cnt,df_lkp)\n",
    "    df_event_id=get_event_history_data(df_input,df_lkp,'event_id')\n",
    "    df_event_code=get_event_history_data(df_input,df_lkp,'event_code')\n",
    "    df_activity=get_activity_history(df_input,df_lkp,tm_event_cnt)\n",
    "    df_title_event_code=title_event_code_cnt(df_input,df_lkp)\n",
    "    df_mod_title=get_modified_title_cumcnt(df_input,df_lkp)\n",
    "    df=pd.merge(df_ass,avg_tm_event_cnt,on=['installation_id','game_session'])\n",
    "    df=pd.merge(df,df_event_code,on=['installation_id','game_session'])\n",
    "    df=pd.merge(df,df_event_id,on=['installation_id','game_session'])\n",
    "    df=pd.merge(df,df_activity,on=['installation_id','game_session'])\n",
    "    df=pd.merge(df,df_title_event_code,on=['installation_id','game_session'])\n",
    "    df=pd.merge(df,df_mod_title,on=['installation_id','game_session'])\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 41s, sys: 1min 8s, total: 3min 50s\n",
      "Wall time: 4min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train=get_train(train,'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17690, 952)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test(df_input,input_flg):\n",
    "    \"\"\"\n",
    "    Merge then all together and get test data\n",
    "    \"\"\"\n",
    "    df_ass=get_assessment_test_df(input_flg)\n",
    "    tm_event_cnt=get_subset_tm_event_cnt(df_input)\n",
    "    avg_tm_event_cnt=get_subset_tm_event_cnt_test(df_input)\n",
    "    df_event_id=get_event_history_data_test(df_input,'event_id')\n",
    "    df_event_code=get_event_history_data_test(df_input,'event_code')\n",
    "    df_activity=get_activity_history_test(df_input,tm_event_cnt)\n",
    "    df_title_event_code=title_event_code_cnt_test(df_input)\n",
    "    df=pd.merge(df_ass,avg_tm_event_cnt,on=['installation_id'])\n",
    "    df=pd.merge(df,df_event_code,on=['installation_id'])\n",
    "    df=pd.merge(df,df_event_id,on=['installation_id'])\n",
    "    df=pd.merge(df,df_activity,on=['installation_id'])\n",
    "    df=pd.merge(df,df_title_event_code,on=['installation_id'])\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 918)\n",
      "CPU times: user 3.2 s, sys: 473 ms, total: 3.67 s\n",
      "Wall time: 3.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_test=get_test(test,'test')\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_insignificant_features(df_train,df_test):\n",
    "    \"\"\"\n",
    "    This function will drop features not in test, Remove highly corelated features\n",
    "    and exclude if mean of a feature for train/test > 10 or < 1/10 otherwise scale\n",
    "    \"\"\"\n",
    "    ajusted_test=df_test.copy()\n",
    "    # Remove all features in train but not in test\n",
    "    train_features=list(set(list(df_train.columns))-set(['installation_id','game_session','accuracy_group']))\n",
    "    test_features=list(set(list(df_test.columns))-set(['installation_id']))\n",
    "    to_drop_other=list(set(test_features)-set(train_features))\n",
    "    test_features=list(set(test_features)-set(to_drop_other))\n",
    "    to_drop=list(set(train_features)-set(test_features))\n",
    "    train_features=list(set(train_features)-set(to_drop))\n",
    "    to_reduce=[]\n",
    "    \n",
    "    for feat_a in train_features:\n",
    "        for feat_b in train_features:\n",
    "            if feat_a != feat_b and feat_a not in to_reduce and feat_b not in to_reduce:\n",
    "                c = np.corrcoef(df_train[feat_a], df_train[feat_b])[0][1]\n",
    "                if c > 0.995:\n",
    "                    to_reduce.append(feat_b)\n",
    "    train_features=list(set(train_features)-set(to_reduce))\n",
    "    \n",
    "    to_exclude=[]\n",
    "    for feature in train_features:\n",
    "        if (df_train[feature].mean()==0)|(df_test[feature].mean()==0) :\n",
    "            to_exclude.append(feature)\n",
    "        else:\n",
    "            train_mean = df_train[feature].mean()\n",
    "            test_mean = ajusted_test[feature].mean()\n",
    "            ajust_factor = train_mean / test_mean\n",
    "            if (ajust_factor >10)|(ajust_factor <.1):\n",
    "                to_exclude.append(feature)\n",
    "            else: \n",
    "                ajusted_test[feature] *= ajust_factor\n",
    "            \n",
    "    final_features=list(set(train_features)-set(to_exclude))\n",
    "    train_features=list(set(final_features)|(set(['installation_id','game_session','accuracy_group'])))\n",
    "    test_features=list(set(final_features)|(set(['installation_id'])))\n",
    "    \n",
    "    reduce_train=df_train[train_features]\n",
    "    ajusted_test=df_test[test_features]\n",
    "    return(reduce_train,ajusted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_train,ajusted_test=remove_insignificant_features(df_train,df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17690, 372)\n",
      "(1000, 370)\n"
     ]
    }
   ],
   "source": [
    "print(reduce_train.shape)\n",
    "print(ajusted_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_train=reduce_train.fillna(0)\n",
    "ajusted_test=ajusted_test.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare for model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_train=reduce_train.set_index(['installation_id','game_session'])\n",
    "ajusted_test=ajusted_test.set_index(['installation_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=reduce_train.index.values\n",
    "test_indices=ajusted_test.index.values\n",
    "labels = np.array(reduce_train['accuracy_group'])\n",
    "features= reduce_train.drop(['accuracy_group'], axis = 1)\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "train_features, test_features, train_labels, test_labels,idx_train,idx_test = \\\n",
    "            train_test_split(features, labels,indices, test_size = 0.2, random_state = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Random Grid Search with cross validation\n",
    "<br>  As XGboost is very expensive , instead of extensive grid search \n",
    "<br> we are doing random grid search here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for searching best model\n",
    "params = {\n",
    "        'min_child_weight': [3,5],\n",
    "        'subsample': [0.85],\n",
    "        'max_depth': [8,12,15]\n",
    "        }\n",
    "xgb = XGBRegressor(learning_rate=0.001, n_estimators=3000, objective='reg:squarederror',silent=True, nthread=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  15 out of  15 | elapsed: 141.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50min 17s, sys: 10.1 s, total: 50min 27s\n",
      "Wall time: 2h 34min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=<generator object _BaseKFold.split at 0x161e0a350>,\n",
       "                   error_score='raise-deprecating',\n",
       "                   estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                          colsample_bylevel=1,\n",
       "                                          colsample_bynode=1,\n",
       "                                          colsample_bytree=1, gamma=0,\n",
       "                                          importance_type='gain',\n",
       "                                          learning_rate=0.001, max_delta_step=0,\n",
       "                                          max_depth=3, min_child_weight=1,\n",
       "                                          missing=None, n_estimators=3000,\n",
       "                                          n_jobs...\n",
       "                                          objective='reg:squarederror',\n",
       "                                          random_state=0, reg_alpha=0,\n",
       "                                          reg_lambda=1, scale_pos_weight=1,\n",
       "                                          seed=None, silent=True, subsample=1,\n",
       "                                          verbosity=1),\n",
       "                   iid='warn', n_iter=3, n_jobs=4,\n",
       "                   param_distributions={'max_depth': [8, 12, 15],\n",
       "                                        'min_child_weight': [3, 5],\n",
       "                                        'subsample': [0.85]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=1001, refit=True,\n",
       "                   return_train_score=False, scoring='r2', verbose=3)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "folds = 5\n",
    "param_comb = 3\n",
    "\n",
    "Y = train_labels\n",
    "X = train_features\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "random_search = RandomizedSearchCV(xgb, param_distributions=params, \\\n",
    "                    n_iter=param_comb, scoring='r2', n_jobs=4, cv=skf.split(X,Y), verbose=3, random_state=1001 )\n",
    "random_search.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best estimator:\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "             importance_type='gain', learning_rate=0.001, max_delta_step=0,\n",
      "             max_depth=12, min_child_weight=5, missing=None, n_estimators=3000,\n",
      "             n_jobs=1, nthread=4, objective='reg:squarederror', random_state=0,\n",
      "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "             silent=True, subsample=0.85, verbosity=1)\n",
      "\n",
      " Best normalized gini score for 5-fold search with 3 parameter combinations:\n",
      "0.7486032789123291\n",
      "\n",
      " Best hyperparameters:\n",
      "{'subsample': 0.85, 'min_child_weight': 5, 'max_depth': 12}\n"
     ]
    }
   ],
   "source": [
    "print('\\n Best estimator:')\n",
    "print(random_search.best_estimator_)\n",
    "print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
    "print(random_search.best_score_ * 2 - 1)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction through best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35min 28s, sys: 8 s, total: 35min 36s\n",
      "Wall time: 9min 33s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.8, gamma=0.25,\n",
       "             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n",
       "             max_depth=15, min_child_weight=3, missing=None, n_estimators=2000,\n",
       "             n_jobs=1, nthread=4, objective='reg:squarederror', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=False, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "xgb_model=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.8, gamma=0.25,\n",
    "             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n",
    "             max_depth=15, min_child_weight=3, missing=None, n_estimators=2000,\n",
    "             n_jobs=1, nthread=4, objective='reg:squarederror', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=False, subsample=1, verbosity=1)\n",
    "xgb_model.fit(train_features,train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting regression to classification with similar distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.7594748440672956, 1: 1.6066137255080482, 2: 2.167111396789551}\n",
      "0.9381205841950103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x12486a690>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dist = Counter(reduce_train['accuracy_group'])\n",
    "for k in dist:\n",
    "    dist[k] /= len(reduce_train)\n",
    "#reduce_train['accuracy_group'].hist()\n",
    "\n",
    "preds=list(xgb_model.predict(test_features))\n",
    "acum = 0\n",
    "bound = {}\n",
    "for i in range(3):\n",
    "    acum += dist[i]\n",
    "    bound[i] = np.percentile(preds, acum * 100)\n",
    "print(bound)\n",
    "def classify(x):\n",
    "    if x <= bound[0]:\n",
    "        return 0\n",
    "    elif x <= bound[1]:\n",
    "        return 1\n",
    "    elif x <= bound[2]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "    \n",
    "test_class = list(map(classify, preds))\n",
    "print(cohen_kappa_score(test_class, test_labels, weights = 'quadratic'))\n",
    "pd.DataFrame(list(test_class)).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>cumcnt_abc5811c</td>\n",
       "      <td>0.104694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>cm_cnt_2000_Sandcastle Builder (Activity)</td>\n",
       "      <td>0.096411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bird_Measurer_incorrect</td>\n",
       "      <td>0.057845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>cm_cnt_2010_Chest Sorter (Assessment)</td>\n",
       "      <td>0.041852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Chest_Sorter_incorrect</td>\n",
       "      <td>0.038421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>accumulated_correct</td>\n",
       "      <td>0.025680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>cum_gm_tm_Crystals Rule</td>\n",
       "      <td>0.022705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>cm_cnt_3121_Bird Measurer (Assessment)</td>\n",
       "      <td>0.020990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>cumcnt_12 Monkeys</td>\n",
       "      <td>0.019046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Cauldron_Filler_incorrect</td>\n",
       "      <td>0.018637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      features  importance\n",
       "261                            cumcnt_abc5811c    0.104694\n",
       "194  cm_cnt_2000_Sandcastle Builder (Activity)    0.096411\n",
       "8                      Bird_Measurer_incorrect    0.057845\n",
       "86       cm_cnt_2010_Chest Sorter (Assessment)    0.041852\n",
       "195                     Chest_Sorter_incorrect    0.038421\n",
       "106                        accumulated_correct    0.025680\n",
       "209                    cum_gm_tm_Crystals Rule    0.022705\n",
       "89      cm_cnt_3121_Bird Measurer (Assessment)    0.020990\n",
       "251                          cumcnt_12 Monkeys    0.019046\n",
       "163                  Cauldron_Filler_incorrect    0.018637"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top_feature_importance=pd.DataFrame(zip(reduce_train.columns,xgb_model.feature_importances_),\\\n",
    "                                       columns=['features','importance'])\n",
    "df_top_feature_importance.sort_values(by='importance',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.08486233599243898, 1: 0.08896949887275696, 2: 0.5115065276622772}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x13988a390>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dist = Counter(reduce_train['accuracy_group'])\n",
    "for k in dist:\n",
    "    dist[k] /= len(reduce_train)\n",
    "#reduce_train['accuracy_group'].hist()\n",
    "\n",
    "preds=list(xgb_model.predict(np.array(ajusted_test)))\n",
    "acum = 0\n",
    "bound = {}\n",
    "for i in range(3):\n",
    "    acum += dist[i]\n",
    "    bound[i] = np.percentile(preds, acum * 100)\n",
    "print(bound)\n",
    "def classify(x):\n",
    "    if x <= bound[0]:\n",
    "        return 0\n",
    "    elif x <= bound[1]:\n",
    "        return 1\n",
    "    elif x <= bound[2]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "    \n",
    "test_class = list(map(classify, preds))\n",
    "pd.DataFrame(list(test_class)).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion from XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Looks like XGB is overfitting becasue there is huge difference between evaluation and test\n",
    "<br> Feature engineering needs to be done different ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> With same features will try Keras classification model <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow import set_random_seed\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=reduce_train.index.values\n",
    "test_indices=ajusted_test.index.values\n",
    "labels = np.array(reduce_train['accuracy_group'])\n",
    "features= reduce_train.drop(['accuracy_group'], axis = 1)\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "features_scaled = min_max_scaler.fit_transform(features)\n",
    "test_features_scaled=min_max_scaler.fit_transform(np.array(ajusted_test))\n",
    "train_features, eval_features, train_labels, eval_labels,idx_train,idx_eval = \\\n",
    "            train_test_split(features_scaled, labels,indices, test_size = 0.2, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14152 samples, validate on 3538 samples\n",
      "Epoch 1/2000\n",
      "14152/14152 [==============================] - 14s 956us/step - loss: 1.3585 - accuracy: 0.4917 - val_loss: 1.3709 - val_accuracy: 0.4955\n",
      "Epoch 2/2000\n",
      "14152/14152 [==============================] - 5s 387us/step - loss: 1.3540 - accuracy: 0.4940 - val_loss: 1.3667 - val_accuracy: 0.4958\n",
      "Epoch 3/2000\n",
      "14152/14152 [==============================] - 5s 362us/step - loss: 1.3490 - accuracy: 0.4964 - val_loss: 1.3616 - val_accuracy: 0.4963\n",
      "Epoch 4/2000\n",
      "14152/14152 [==============================] - 5s 362us/step - loss: 1.3458 - accuracy: 0.4971 - val_loss: 1.3581 - val_accuracy: 0.4972\n",
      "Epoch 5/2000\n",
      "14152/14152 [==============================] - 5s 339us/step - loss: 1.3406 - accuracy: 0.4977 - val_loss: 1.3539 - val_accuracy: 0.4975\n",
      "Epoch 6/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 1.3365 - accuracy: 0.4992 - val_loss: 1.3500 - val_accuracy: 0.4972\n",
      "Epoch 7/2000\n",
      "14152/14152 [==============================] - 5s 359us/step - loss: 1.3320 - accuracy: 0.4985 - val_loss: 1.3453 - val_accuracy: 0.4983\n",
      "Epoch 8/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 1.3281 - accuracy: 0.4993 - val_loss: 1.3437 - val_accuracy: 0.4977\n",
      "Epoch 9/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 1.3235 - accuracy: 0.5000 - val_loss: 1.3379 - val_accuracy: 0.4977\n",
      "Epoch 10/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 1.3194 - accuracy: 0.4991 - val_loss: 1.3355 - val_accuracy: 0.4983\n",
      "Epoch 11/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 1.3166 - accuracy: 0.4998 - val_loss: 1.3297 - val_accuracy: 0.4977\n",
      "Epoch 12/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 1.3121 - accuracy: 0.5005 - val_loss: 1.3277 - val_accuracy: 0.4977\n",
      "Epoch 13/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 1.3083 - accuracy: 0.5001 - val_loss: 1.3243 - val_accuracy: 0.4975\n",
      "Epoch 14/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 1.3049 - accuracy: 0.5008 - val_loss: 1.3200 - val_accuracy: 0.4975\n",
      "Epoch 15/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 1.3023 - accuracy: 0.5006 - val_loss: 1.3165 - val_accuracy: 0.4977\n",
      "Epoch 16/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 1.2994 - accuracy: 0.5006 - val_loss: 1.3141 - val_accuracy: 0.4977\n",
      "Epoch 17/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 1.2963 - accuracy: 0.5007 - val_loss: 1.3103 - val_accuracy: 0.4975\n",
      "Epoch 18/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 1.2920 - accuracy: 0.5008 - val_loss: 1.3082 - val_accuracy: 0.4975\n",
      "Epoch 19/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 1.2889 - accuracy: 0.5006 - val_loss: 1.3045 - val_accuracy: 0.4977\n",
      "Epoch 20/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 1.2875 - accuracy: 0.5006 - val_loss: 1.3013 - val_accuracy: 0.4977\n",
      "Epoch 21/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 1.2841 - accuracy: 0.5006 - val_loss: 1.2984 - val_accuracy: 0.4977\n",
      "Epoch 22/2000\n",
      "14152/14152 [==============================] - 5s 348us/step - loss: 1.2815 - accuracy: 0.5007 - val_loss: 1.2977 - val_accuracy: 0.4977\n",
      "Epoch 23/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 1.2768 - accuracy: 0.5007 - val_loss: 1.2940 - val_accuracy: 0.4975\n",
      "Epoch 24/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 1.2756 - accuracy: 0.5007 - val_loss: 1.2912 - val_accuracy: 0.4972\n",
      "Epoch 25/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 1.2740 - accuracy: 0.5007 - val_loss: 1.2889 - val_accuracy: 0.4972\n",
      "Epoch 26/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 1.2715 - accuracy: 0.5008 - val_loss: 1.2874 - val_accuracy: 0.4975\n",
      "Epoch 27/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 1.2671 - accuracy: 0.5007 - val_loss: 1.2857 - val_accuracy: 0.4972\n",
      "Epoch 28/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 1.2662 - accuracy: 0.5008 - val_loss: 1.2828 - val_accuracy: 0.4975\n",
      "Epoch 29/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 1.2643 - accuracy: 0.5006 - val_loss: 1.2804 - val_accuracy: 0.4975\n",
      "Epoch 30/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 1.2635 - accuracy: 0.5007 - val_loss: 1.2812 - val_accuracy: 0.4977\n",
      "Epoch 31/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 1.2611 - accuracy: 0.5008 - val_loss: 1.2762 - val_accuracy: 0.4975\n",
      "Epoch 32/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 1.2589 - accuracy: 0.5006 - val_loss: 1.2759 - val_accuracy: 0.4975\n",
      "Epoch 33/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 1.2569 - accuracy: 0.5007 - val_loss: 1.2745 - val_accuracy: 0.4972\n",
      "Epoch 34/2000\n",
      "14152/14152 [==============================] - 5s 368us/step - loss: 1.2563 - accuracy: 0.5007 - val_loss: 1.2723 - val_accuracy: 0.4972\n",
      "Epoch 35/2000\n",
      "14152/14152 [==============================] - 5s 329us/step - loss: 1.2539 - accuracy: 0.5007 - val_loss: 1.2717 - val_accuracy: 0.4975\n",
      "Epoch 36/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 1.2505 - accuracy: 0.5007 - val_loss: 1.2689 - val_accuracy: 0.4975\n",
      "Epoch 37/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 1.2499 - accuracy: 0.5006 - val_loss: 1.2670 - val_accuracy: 0.4972\n",
      "Epoch 38/2000\n",
      "14152/14152 [==============================] - 5s 375us/step - loss: 1.2507 - accuracy: 0.5007 - val_loss: 1.2651 - val_accuracy: 0.4975\n",
      "Epoch 39/2000\n",
      "14152/14152 [==============================] - 7s 501us/step - loss: 1.2471 - accuracy: 0.5007 - val_loss: 1.2645 - val_accuracy: 0.4975\n",
      "Epoch 40/2000\n",
      "14152/14152 [==============================] - 5s 342us/step - loss: 1.2454 - accuracy: 0.5006 - val_loss: 1.2627 - val_accuracy: 0.4975\n",
      "Epoch 41/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 1.2440 - accuracy: 0.5007 - val_loss: 1.2599 - val_accuracy: 0.4972\n",
      "Epoch 42/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 1.2420 - accuracy: 0.5007 - val_loss: 1.2580 - val_accuracy: 0.4972\n",
      "Epoch 43/2000\n",
      "14152/14152 [==============================] - 5s 345us/step - loss: 1.2418 - accuracy: 0.5007 - val_loss: 1.2568 - val_accuracy: 0.4972\n",
      "Epoch 44/2000\n",
      "14152/14152 [==============================] - 5s 333us/step - loss: 1.2392 - accuracy: 0.5007 - val_loss: 1.2569 - val_accuracy: 0.4975\n",
      "Epoch 45/2000\n",
      "14152/14152 [==============================] - 5s 335us/step - loss: 1.2371 - accuracy: 0.5007 - val_loss: 1.2533 - val_accuracy: 0.4972\n",
      "Epoch 46/2000\n",
      "14152/14152 [==============================] - 5s 354us/step - loss: 1.2360 - accuracy: 0.5008 - val_loss: 1.2536 - val_accuracy: 0.4972\n",
      "Epoch 47/2000\n",
      "14152/14152 [==============================] - 5s 335us/step - loss: 1.2355 - accuracy: 0.5008 - val_loss: 1.2519 - val_accuracy: 0.4972\n",
      "Epoch 48/2000\n",
      "14152/14152 [==============================] - 5s 333us/step - loss: 1.2332 - accuracy: 0.5007 - val_loss: 1.2498 - val_accuracy: 0.4975\n",
      "Epoch 49/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 1.2325 - accuracy: 0.5007 - val_loss: 1.2484 - val_accuracy: 0.4972\n",
      "Epoch 50/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 1.2306 - accuracy: 0.5007 - val_loss: 1.2479 - val_accuracy: 0.4975\n",
      "Epoch 51/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 1.2275 - accuracy: 0.5007 - val_loss: 1.2454 - val_accuracy: 0.4975\n",
      "Epoch 52/2000\n",
      "14152/14152 [==============================] - 5s 349us/step - loss: 1.2291 - accuracy: 0.5007 - val_loss: 1.2445 - val_accuracy: 0.4972\n",
      "Epoch 53/2000\n",
      "14152/14152 [==============================] - 5s 366us/step - loss: 1.2265 - accuracy: 0.5007 - val_loss: 1.2428 - val_accuracy: 0.4972\n",
      "Epoch 54/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 1.2259 - accuracy: 0.5008 - val_loss: 1.2404 - val_accuracy: 0.4972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 1.2240 - accuracy: 0.5008 - val_loss: 1.2402 - val_accuracy: 0.4972\n",
      "Epoch 56/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 1.2203 - accuracy: 0.5007 - val_loss: 1.2372 - val_accuracy: 0.4972\n",
      "Epoch 57/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 1.2202 - accuracy: 0.5007 - val_loss: 1.2365 - val_accuracy: 0.4972\n",
      "Epoch 58/2000\n",
      "14152/14152 [==============================] - 5s 318us/step - loss: 1.2180 - accuracy: 0.5007 - val_loss: 1.2334 - val_accuracy: 0.4972\n",
      "Epoch 59/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 1.2169 - accuracy: 0.5007 - val_loss: 1.2323 - val_accuracy: 0.4972\n",
      "Epoch 60/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 1.2169 - accuracy: 0.5007 - val_loss: 1.2310 - val_accuracy: 0.4972\n",
      "Epoch 61/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 1.2143 - accuracy: 0.5007 - val_loss: 1.2291 - val_accuracy: 0.4972\n",
      "Epoch 62/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 1.2144 - accuracy: 0.5007 - val_loss: 1.2280 - val_accuracy: 0.4972\n",
      "Epoch 63/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 1.2112 - accuracy: 0.5008 - val_loss: 1.2272 - val_accuracy: 0.4972\n",
      "Epoch 64/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 1.2110 - accuracy: 0.5007 - val_loss: 1.2257 - val_accuracy: 0.4972\n",
      "Epoch 65/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 1.2078 - accuracy: 0.5008 - val_loss: 1.2245 - val_accuracy: 0.4972\n",
      "Epoch 66/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 1.2079 - accuracy: 0.5008 - val_loss: 1.2222 - val_accuracy: 0.4972\n",
      "Epoch 67/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 1.2070 - accuracy: 0.5008 - val_loss: 1.2201 - val_accuracy: 0.4972\n",
      "Epoch 68/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 1.2061 - accuracy: 0.5008 - val_loss: 1.2215 - val_accuracy: 0.4972\n",
      "Epoch 69/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 1.2017 - accuracy: 0.5009 - val_loss: 1.2158 - val_accuracy: 0.4972\n",
      "Epoch 70/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 1.2005 - accuracy: 0.5008 - val_loss: 1.2154 - val_accuracy: 0.4972\n",
      "Epoch 71/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 1.1990 - accuracy: 0.5008 - val_loss: 1.2170 - val_accuracy: 0.4972\n",
      "Epoch 72/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 1.1981 - accuracy: 0.5010 - val_loss: 1.2126 - val_accuracy: 0.4972\n",
      "Epoch 73/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 1.1943 - accuracy: 0.5008 - val_loss: 1.2116 - val_accuracy: 0.4972\n",
      "Epoch 74/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 1.1966 - accuracy: 0.5010 - val_loss: 1.2090 - val_accuracy: 0.4977\n",
      "Epoch 75/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 1.1951 - accuracy: 0.5011 - val_loss: 1.2074 - val_accuracy: 0.4975\n",
      "Epoch 76/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 1.1931 - accuracy: 0.5008 - val_loss: 1.2072 - val_accuracy: 0.4977\n",
      "Epoch 77/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 1.1907 - accuracy: 0.5012 - val_loss: 1.2048 - val_accuracy: 0.4977\n",
      "Epoch 78/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 1.1901 - accuracy: 0.5011 - val_loss: 1.2016 - val_accuracy: 0.4977\n",
      "Epoch 79/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 1.1874 - accuracy: 0.5010 - val_loss: 1.2035 - val_accuracy: 0.4977\n",
      "Epoch 80/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 1.1863 - accuracy: 0.5010 - val_loss: 1.2014 - val_accuracy: 0.4977\n",
      "Epoch 81/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 1.1858 - accuracy: 0.5012 - val_loss: 1.1997 - val_accuracy: 0.4977\n",
      "Epoch 82/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 1.1850 - accuracy: 0.5011 - val_loss: 1.1970 - val_accuracy: 0.4980\n",
      "Epoch 83/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 1.1838 - accuracy: 0.5015 - val_loss: 1.1959 - val_accuracy: 0.4977\n",
      "Epoch 84/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 1.1825 - accuracy: 0.5018 - val_loss: 1.1944 - val_accuracy: 0.4983\n",
      "Epoch 85/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 1.1787 - accuracy: 0.5016 - val_loss: 1.1913 - val_accuracy: 0.4980\n",
      "Epoch 86/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 1.1798 - accuracy: 0.5016 - val_loss: 1.1914 - val_accuracy: 0.4983\n",
      "Epoch 87/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 1.1767 - accuracy: 0.5020 - val_loss: 1.1895 - val_accuracy: 0.4983\n",
      "Epoch 88/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 1.1772 - accuracy: 0.5016 - val_loss: 1.1878 - val_accuracy: 0.4983\n",
      "Epoch 89/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 1.1756 - accuracy: 0.5025 - val_loss: 1.1860 - val_accuracy: 0.4989\n",
      "Epoch 90/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 1.1732 - accuracy: 0.5023 - val_loss: 1.1854 - val_accuracy: 0.4994\n",
      "Epoch 91/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 1.1724 - accuracy: 0.5022 - val_loss: 1.1829 - val_accuracy: 0.4992\n",
      "Epoch 92/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 1.1712 - accuracy: 0.5024 - val_loss: 1.1804 - val_accuracy: 0.5003\n",
      "Epoch 93/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 1.1686 - accuracy: 0.5027 - val_loss: 1.1786 - val_accuracy: 0.5003\n",
      "Epoch 94/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 1.1679 - accuracy: 0.5025 - val_loss: 1.1752 - val_accuracy: 0.5000\n",
      "Epoch 95/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 1.1672 - accuracy: 0.5030 - val_loss: 1.1779 - val_accuracy: 0.5008\n",
      "Epoch 96/2000\n",
      "14152/14152 [==============================] - 5s 329us/step - loss: 1.1642 - accuracy: 0.5031 - val_loss: 1.1727 - val_accuracy: 0.5006\n",
      "Epoch 97/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 1.1632 - accuracy: 0.5033 - val_loss: 1.1718 - val_accuracy: 0.5017\n",
      "Epoch 98/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 1.1619 - accuracy: 0.5035 - val_loss: 1.1724 - val_accuracy: 0.5011\n",
      "Epoch 99/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 1.1601 - accuracy: 0.5051 - val_loss: 1.1708 - val_accuracy: 0.5014\n",
      "Epoch 100/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 1.1602 - accuracy: 0.5046 - val_loss: 1.1664 - val_accuracy: 0.5028\n",
      "Epoch 101/2000\n",
      "14152/14152 [==============================] - 5s 329us/step - loss: 1.1557 - accuracy: 0.5049 - val_loss: 1.1653 - val_accuracy: 0.5023\n",
      "Epoch 102/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 1.1551 - accuracy: 0.5058 - val_loss: 1.1646 - val_accuracy: 0.5031\n",
      "Epoch 103/2000\n",
      "14152/14152 [==============================] - 5s 329us/step - loss: 1.1544 - accuracy: 0.5061 - val_loss: 1.1640 - val_accuracy: 0.5045\n",
      "Epoch 104/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 1.1573 - accuracy: 0.5059 - val_loss: 1.1614 - val_accuracy: 0.5042\n",
      "Epoch 105/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 1.1511 - accuracy: 0.5070 - val_loss: 1.1597 - val_accuracy: 0.5051\n",
      "Epoch 106/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 1.1497 - accuracy: 0.5078 - val_loss: 1.1606 - val_accuracy: 0.5057\n",
      "Epoch 107/2000\n",
      "14152/14152 [==============================] - 5s 328us/step - loss: 1.1513 - accuracy: 0.5083 - val_loss: 1.1585 - val_accuracy: 0.5065\n",
      "Epoch 108/2000\n",
      "14152/14152 [==============================] - 5s 329us/step - loss: 1.1487 - accuracy: 0.5088 - val_loss: 1.1567 - val_accuracy: 0.5068\n",
      "Epoch 109/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14152/14152 [==============================] - 5s 332us/step - loss: 1.1479 - accuracy: 0.5097 - val_loss: 1.1554 - val_accuracy: 0.5068\n",
      "Epoch 110/2000\n",
      "14152/14152 [==============================] - 5s 354us/step - loss: 1.1456 - accuracy: 0.5105 - val_loss: 1.1535 - val_accuracy: 0.5076\n",
      "Epoch 111/2000\n",
      "14152/14152 [==============================] - 5s 346us/step - loss: 1.1443 - accuracy: 0.5102 - val_loss: 1.1524 - val_accuracy: 0.5099\n",
      "Epoch 112/2000\n",
      "14152/14152 [==============================] - 6s 390us/step - loss: 1.1436 - accuracy: 0.5117 - val_loss: 1.1530 - val_accuracy: 0.5107\n",
      "Epoch 113/2000\n",
      "14152/14152 [==============================] - 7s 516us/step - loss: 1.1417 - accuracy: 0.5122 - val_loss: 1.1500 - val_accuracy: 0.5105\n",
      "Epoch 114/2000\n",
      "14152/14152 [==============================] - 6s 455us/step - loss: 1.1415 - accuracy: 0.5134 - val_loss: 1.1480 - val_accuracy: 0.5113\n",
      "Epoch 115/2000\n",
      "14152/14152 [==============================] - 5s 377us/step - loss: 1.1426 - accuracy: 0.5143 - val_loss: 1.1467 - val_accuracy: 0.5119\n",
      "Epoch 116/2000\n",
      "14152/14152 [==============================] - 5s 387us/step - loss: 1.1411 - accuracy: 0.5138 - val_loss: 1.1464 - val_accuracy: 0.5119\n",
      "Epoch 117/2000\n",
      "14152/14152 [==============================] - 6s 394us/step - loss: 1.1363 - accuracy: 0.5160 - val_loss: 1.1449 - val_accuracy: 0.5147\n",
      "Epoch 118/2000\n",
      "14152/14152 [==============================] - 5s 369us/step - loss: 1.1362 - accuracy: 0.5172 - val_loss: 1.1437 - val_accuracy: 0.5161\n",
      "Epoch 119/2000\n",
      "14152/14152 [==============================] - 5s 382us/step - loss: 1.1350 - accuracy: 0.5189 - val_loss: 1.1432 - val_accuracy: 0.5172\n",
      "Epoch 120/2000\n",
      "14152/14152 [==============================] - 8s 585us/step - loss: 1.1332 - accuracy: 0.5207 - val_loss: 1.1414 - val_accuracy: 0.5212\n",
      "Epoch 121/2000\n",
      "14152/14152 [==============================] - 6s 447us/step - loss: 1.1299 - accuracy: 0.5213 - val_loss: 1.1414 - val_accuracy: 0.5192\n",
      "Epoch 122/2000\n",
      "14152/14152 [==============================] - 6s 409us/step - loss: 1.1305 - accuracy: 0.5195 - val_loss: 1.1378 - val_accuracy: 0.5189\n",
      "Epoch 123/2000\n",
      "14152/14152 [==============================] - 5s 344us/step - loss: 1.1289 - accuracy: 0.5224 - val_loss: 1.1384 - val_accuracy: 0.5232\n",
      "Epoch 124/2000\n",
      "14152/14152 [==============================] - 5s 382us/step - loss: 1.1287 - accuracy: 0.5240 - val_loss: 1.1350 - val_accuracy: 0.5252\n",
      "Epoch 125/2000\n",
      "14152/14152 [==============================] - 6s 412us/step - loss: 1.1297 - accuracy: 0.5241 - val_loss: 1.1330 - val_accuracy: 0.5254\n",
      "Epoch 126/2000\n",
      "14152/14152 [==============================] - 5s 373us/step - loss: 1.1262 - accuracy: 0.5257 - val_loss: 1.1329 - val_accuracy: 0.5291\n",
      "Epoch 127/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 1.1256 - accuracy: 0.5286 - val_loss: 1.1336 - val_accuracy: 0.5269\n",
      "Epoch 128/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 1.1248 - accuracy: 0.5272 - val_loss: 1.1301 - val_accuracy: 0.5300\n",
      "Epoch 129/2000\n",
      "14152/14152 [==============================] - 5s 318us/step - loss: 1.1220 - accuracy: 0.5319 - val_loss: 1.1271 - val_accuracy: 0.5328\n",
      "Epoch 130/2000\n",
      "14152/14152 [==============================] - 5s 345us/step - loss: 1.1222 - accuracy: 0.5328 - val_loss: 1.1251 - val_accuracy: 0.5300\n",
      "Epoch 131/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 1.1188 - accuracy: 0.5353 - val_loss: 1.1252 - val_accuracy: 0.5362\n",
      "Epoch 132/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 1.1197 - accuracy: 0.5342 - val_loss: 1.1258 - val_accuracy: 0.5367\n",
      "Epoch 133/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 1.1211 - accuracy: 0.5365 - val_loss: 1.1253 - val_accuracy: 0.5387\n",
      "Epoch 134/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 1.1165 - accuracy: 0.5380 - val_loss: 1.1216 - val_accuracy: 0.5382\n",
      "Epoch 135/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 1.1123 - accuracy: 0.5403 - val_loss: 1.1200 - val_accuracy: 0.5399\n",
      "Epoch 136/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 1.1137 - accuracy: 0.5430 - val_loss: 1.1187 - val_accuracy: 0.5447\n",
      "Epoch 137/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 1.1151 - accuracy: 0.5405 - val_loss: 1.1187 - val_accuracy: 0.5478\n",
      "Epoch 138/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 1.1111 - accuracy: 0.5430 - val_loss: 1.1202 - val_accuracy: 0.5432\n",
      "Epoch 139/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 1.1095 - accuracy: 0.5466 - val_loss: 1.1150 - val_accuracy: 0.5506\n",
      "Epoch 140/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 1.1124 - accuracy: 0.5462 - val_loss: 1.1149 - val_accuracy: 0.5469\n",
      "Epoch 141/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 1.1090 - accuracy: 0.5480 - val_loss: 1.1146 - val_accuracy: 0.5458\n",
      "Epoch 142/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 1.1065 - accuracy: 0.5500 - val_loss: 1.1098 - val_accuracy: 0.5483\n",
      "Epoch 143/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 1.1058 - accuracy: 0.5504 - val_loss: 1.1088 - val_accuracy: 0.5497\n",
      "Epoch 144/2000\n",
      "14152/14152 [==============================] - 5s 362us/step - loss: 1.1052 - accuracy: 0.5524 - val_loss: 1.1117 - val_accuracy: 0.5512\n",
      "Epoch 145/2000\n",
      "14152/14152 [==============================] - 6s 433us/step - loss: 1.1038 - accuracy: 0.5539 - val_loss: 1.1084 - val_accuracy: 0.5551\n",
      "Epoch 146/2000\n",
      "14152/14152 [==============================] - 6s 458us/step - loss: 1.1032 - accuracy: 0.5545 - val_loss: 1.1077 - val_accuracy: 0.5551\n",
      "Epoch 147/2000\n",
      "14152/14152 [==============================] - 6s 414us/step - loss: 1.1031 - accuracy: 0.5560 - val_loss: 1.1070 - val_accuracy: 0.5577\n",
      "Epoch 148/2000\n",
      "14152/14152 [==============================] - 5s 331us/step - loss: 1.1011 - accuracy: 0.5578 - val_loss: 1.1036 - val_accuracy: 0.5568\n",
      "Epoch 149/2000\n",
      "14152/14152 [==============================] - 5s 330us/step - loss: 1.1003 - accuracy: 0.5593 - val_loss: 1.1063 - val_accuracy: 0.5565\n",
      "Epoch 150/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 1.0989 - accuracy: 0.5613 - val_loss: 1.1037 - val_accuracy: 0.5616\n",
      "Epoch 151/2000\n",
      "14152/14152 [==============================] - 5s 363us/step - loss: 1.0982 - accuracy: 0.5644 - val_loss: 1.1023 - val_accuracy: 0.5613\n",
      "Epoch 152/2000\n",
      "14152/14152 [==============================] - 6s 422us/step - loss: 1.0980 - accuracy: 0.5650 - val_loss: 1.0972 - val_accuracy: 0.5656\n",
      "Epoch 153/2000\n",
      "14152/14152 [==============================] - 6s 418us/step - loss: 1.0927 - accuracy: 0.5642 - val_loss: 1.0997 - val_accuracy: 0.5625\n",
      "Epoch 154/2000\n",
      "14152/14152 [==============================] - 6s 413us/step - loss: 1.0963 - accuracy: 0.5637 - val_loss: 1.0958 - val_accuracy: 0.5667\n",
      "Epoch 155/2000\n",
      "14152/14152 [==============================] - 6s 401us/step - loss: 1.0919 - accuracy: 0.5668 - val_loss: 1.1010 - val_accuracy: 0.5650\n",
      "Epoch 156/2000\n",
      "14152/14152 [==============================] - 6s 406us/step - loss: 1.0930 - accuracy: 0.5661 - val_loss: 1.0938 - val_accuracy: 0.5692\n",
      "Epoch 157/2000\n",
      "14152/14152 [==============================] - 5s 365us/step - loss: 1.0906 - accuracy: 0.5683 - val_loss: 1.0918 - val_accuracy: 0.5735\n",
      "Epoch 158/2000\n",
      "14152/14152 [==============================] - 5s 366us/step - loss: 1.0899 - accuracy: 0.5698 - val_loss: 1.0910 - val_accuracy: 0.5715\n",
      "Epoch 159/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 1.0883 - accuracy: 0.5728 - val_loss: 1.0940 - val_accuracy: 0.5715\n",
      "Epoch 160/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 1.0888 - accuracy: 0.5737 - val_loss: 1.0894 - val_accuracy: 0.5752\n",
      "Epoch 161/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 1.0875 - accuracy: 0.5724 - val_loss: 1.0885 - val_accuracy: 0.5797\n",
      "Epoch 162/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 1.0865 - accuracy: 0.5744 - val_loss: 1.0911 - val_accuracy: 0.5752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/2000\n",
      "14152/14152 [==============================] - 5s 378us/step - loss: 1.0865 - accuracy: 0.5733 - val_loss: 1.0865 - val_accuracy: 0.5789\n",
      "Epoch 164/2000\n",
      "14152/14152 [==============================] - 6s 416us/step - loss: 1.0851 - accuracy: 0.5763 - val_loss: 1.0880 - val_accuracy: 0.5752\n",
      "Epoch 165/2000\n",
      "14152/14152 [==============================] - 5s 374us/step - loss: 1.0850 - accuracy: 0.5776 - val_loss: 1.0853 - val_accuracy: 0.5797\n",
      "Epoch 166/2000\n",
      "14152/14152 [==============================] - 5s 358us/step - loss: 1.0843 - accuracy: 0.5803 - val_loss: 1.0818 - val_accuracy: 0.5837\n",
      "Epoch 167/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 1.0818 - accuracy: 0.5794 - val_loss: 1.0840 - val_accuracy: 0.5831\n",
      "Epoch 168/2000\n",
      "14152/14152 [==============================] - 5s 363us/step - loss: 1.0811 - accuracy: 0.5806 - val_loss: 1.0789 - val_accuracy: 0.5837\n",
      "Epoch 169/2000\n",
      "14152/14152 [==============================] - 5s 329us/step - loss: 1.0799 - accuracy: 0.5825 - val_loss: 1.0809 - val_accuracy: 0.5865\n",
      "Epoch 170/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 1.0781 - accuracy: 0.5800 - val_loss: 1.0787 - val_accuracy: 0.5851\n",
      "Epoch 171/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 1.0766 - accuracy: 0.5822 - val_loss: 1.0799 - val_accuracy: 0.5865\n",
      "Epoch 172/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 1.0724 - accuracy: 0.5834 - val_loss: 1.0768 - val_accuracy: 0.5876\n",
      "Epoch 173/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 1.0746 - accuracy: 0.5854 - val_loss: 1.0783 - val_accuracy: 0.5851\n",
      "Epoch 174/2000\n",
      "14152/14152 [==============================] - 5s 345us/step - loss: 1.0735 - accuracy: 0.5851 - val_loss: 1.0761 - val_accuracy: 0.5885\n",
      "Epoch 175/2000\n",
      "14152/14152 [==============================] - 5s 348us/step - loss: 1.0791 - accuracy: 0.5857 - val_loss: 1.0781 - val_accuracy: 0.5848\n",
      "Epoch 176/2000\n",
      "14152/14152 [==============================] - 5s 382us/step - loss: 1.0758 - accuracy: 0.5872 - val_loss: 1.0752 - val_accuracy: 0.5916\n",
      "Epoch 177/2000\n",
      "14152/14152 [==============================] - 5s 373us/step - loss: 1.0707 - accuracy: 0.5896 - val_loss: 1.0764 - val_accuracy: 0.5888\n",
      "Epoch 178/2000\n",
      "14152/14152 [==============================] - 5s 331us/step - loss: 1.0731 - accuracy: 0.5884 - val_loss: 1.0712 - val_accuracy: 0.5921\n",
      "Epoch 179/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 1.0686 - accuracy: 0.5874 - val_loss: 1.0711 - val_accuracy: 0.5927\n",
      "Epoch 180/2000\n",
      "14152/14152 [==============================] - 5s 352us/step - loss: 1.0706 - accuracy: 0.5903 - val_loss: 1.0680 - val_accuracy: 0.5975\n",
      "Epoch 181/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 1.0712 - accuracy: 0.5902 - val_loss: 1.0688 - val_accuracy: 0.5944\n",
      "Epoch 182/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 1.0652 - accuracy: 0.5888 - val_loss: 1.0675 - val_accuracy: 0.5986\n",
      "Epoch 183/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 1.0667 - accuracy: 0.5906 - val_loss: 1.0650 - val_accuracy: 0.5958\n",
      "Epoch 184/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 1.0647 - accuracy: 0.5929 - val_loss: 1.0659 - val_accuracy: 0.5950\n",
      "Epoch 185/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 1.0642 - accuracy: 0.5930 - val_loss: 1.0694 - val_accuracy: 0.5938\n",
      "Epoch 186/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 1.0642 - accuracy: 0.5967 - val_loss: 1.0649 - val_accuracy: 0.5998\n",
      "Epoch 187/2000\n",
      "14152/14152 [==============================] - 5s 328us/step - loss: 1.0615 - accuracy: 0.5950 - val_loss: 1.0593 - val_accuracy: 0.6001\n",
      "Epoch 188/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 1.0607 - accuracy: 0.5987 - val_loss: 1.0612 - val_accuracy: 0.5978\n",
      "Epoch 189/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 1.0648 - accuracy: 0.5944 - val_loss: 1.0574 - val_accuracy: 0.6009\n",
      "Epoch 190/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 1.0599 - accuracy: 0.5989 - val_loss: 1.0586 - val_accuracy: 0.6020\n",
      "Epoch 191/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 1.0585 - accuracy: 0.5960 - val_loss: 1.0589 - val_accuracy: 0.5998\n",
      "Epoch 192/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 1.0593 - accuracy: 0.5981 - val_loss: 1.0575 - val_accuracy: 0.6009\n",
      "Epoch 193/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 1.0622 - accuracy: 0.5975 - val_loss: 1.0565 - val_accuracy: 0.6023\n",
      "Epoch 194/2000\n",
      "14152/14152 [==============================] - 5s 331us/step - loss: 1.0554 - accuracy: 0.5993 - val_loss: 1.0521 - val_accuracy: 0.6029\n",
      "Epoch 195/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 1.0565 - accuracy: 0.5999 - val_loss: 1.0553 - val_accuracy: 0.6037\n",
      "Epoch 196/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 1.0541 - accuracy: 0.6022 - val_loss: 1.0553 - val_accuracy: 0.6051\n",
      "Epoch 197/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 1.0508 - accuracy: 0.6002 - val_loss: 1.0567 - val_accuracy: 0.6032\n",
      "Epoch 198/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 1.0528 - accuracy: 0.6043 - val_loss: 1.0553 - val_accuracy: 0.6037\n",
      "Epoch 199/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 1.0555 - accuracy: 0.6039 - val_loss: 1.0490 - val_accuracy: 0.6102\n",
      "Epoch 200/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 1.0533 - accuracy: 0.6028 - val_loss: 1.0496 - val_accuracy: 0.6097\n",
      "Epoch 201/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 1.0512 - accuracy: 0.6037 - val_loss: 1.0483 - val_accuracy: 0.6097\n",
      "Epoch 202/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 1.0495 - accuracy: 0.6073 - val_loss: 1.0476 - val_accuracy: 0.6077\n",
      "Epoch 203/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 1.0475 - accuracy: 0.6058 - val_loss: 1.0499 - val_accuracy: 0.6071\n",
      "Epoch 204/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 1.0506 - accuracy: 0.6073 - val_loss: 1.0464 - val_accuracy: 0.6091\n",
      "Epoch 205/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 1.0451 - accuracy: 0.6065 - val_loss: 1.0489 - val_accuracy: 0.6066\n",
      "Epoch 206/2000\n",
      "14152/14152 [==============================] - 5s 342us/step - loss: 1.0477 - accuracy: 0.6096 - val_loss: 1.0454 - val_accuracy: 0.6133\n",
      "Epoch 207/2000\n",
      "14152/14152 [==============================] - 5s 328us/step - loss: 1.0455 - accuracy: 0.6049 - val_loss: 1.0439 - val_accuracy: 0.6125\n",
      "Epoch 208/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 1.0440 - accuracy: 0.6093 - val_loss: 1.0412 - val_accuracy: 0.6114\n",
      "Epoch 209/2000\n",
      "14152/14152 [==============================] - 5s 355us/step - loss: 1.0430 - accuracy: 0.6092 - val_loss: 1.0438 - val_accuracy: 0.6071\n",
      "Epoch 210/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 1.0429 - accuracy: 0.6086 - val_loss: 1.0400 - val_accuracy: 0.6097\n",
      "Epoch 211/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 1.0424 - accuracy: 0.6123 - val_loss: 1.0468 - val_accuracy: 0.6060\n",
      "Epoch 212/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 1.0406 - accuracy: 0.6112 - val_loss: 1.0408 - val_accuracy: 0.6119\n",
      "Epoch 213/2000\n",
      "14152/14152 [==============================] - 5s 342us/step - loss: 1.0414 - accuracy: 0.6111 - val_loss: 1.0421 - val_accuracy: 0.6114\n",
      "Epoch 214/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 1.0404 - accuracy: 0.6104 - val_loss: 1.0350 - val_accuracy: 0.6167\n",
      "Epoch 215/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 1.0390 - accuracy: 0.6132 - val_loss: 1.0331 - val_accuracy: 0.6179\n",
      "Epoch 216/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 1.0385 - accuracy: 0.6116 - val_loss: 1.0328 - val_accuracy: 0.6187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 217/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 1.0380 - accuracy: 0.6099 - val_loss: 1.0289 - val_accuracy: 0.6204\n",
      "Epoch 218/2000\n",
      "14152/14152 [==============================] - 5s 333us/step - loss: 1.0352 - accuracy: 0.6139 - val_loss: 1.0338 - val_accuracy: 0.6213\n",
      "Epoch 219/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 1.0311 - accuracy: 0.6121 - val_loss: 1.0329 - val_accuracy: 0.6181\n",
      "Epoch 220/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 1.0378 - accuracy: 0.6114 - val_loss: 1.0268 - val_accuracy: 0.6224\n",
      "Epoch 221/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 1.0353 - accuracy: 0.6140 - val_loss: 1.0327 - val_accuracy: 0.6207\n",
      "Epoch 222/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 1.0342 - accuracy: 0.6151 - val_loss: 1.0294 - val_accuracy: 0.6173\n",
      "Epoch 223/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 1.0320 - accuracy: 0.6173 - val_loss: 1.0313 - val_accuracy: 0.6159\n",
      "Epoch 224/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 1.0367 - accuracy: 0.6128 - val_loss: 1.0308 - val_accuracy: 0.6170\n",
      "Epoch 225/2000\n",
      "14152/14152 [==============================] - 5s 329us/step - loss: 1.0320 - accuracy: 0.6166 - val_loss: 1.0312 - val_accuracy: 0.6131\n",
      "Epoch 226/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 1.0323 - accuracy: 0.6134 - val_loss: 1.0238 - val_accuracy: 0.6196\n",
      "Epoch 227/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 1.0324 - accuracy: 0.6178 - val_loss: 1.0256 - val_accuracy: 0.6207\n",
      "Epoch 228/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 1.0303 - accuracy: 0.6168 - val_loss: 1.0272 - val_accuracy: 0.6153\n",
      "Epoch 229/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 1.0279 - accuracy: 0.6184 - val_loss: 1.0271 - val_accuracy: 0.6164\n",
      "Epoch 230/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 1.0309 - accuracy: 0.6162 - val_loss: 1.0237 - val_accuracy: 0.6246\n",
      "Epoch 231/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 1.0317 - accuracy: 0.6159 - val_loss: 1.0278 - val_accuracy: 0.6139\n",
      "Epoch 232/2000\n",
      "14152/14152 [==============================] - 5s 353us/step - loss: 1.0308 - accuracy: 0.6152 - val_loss: 1.0192 - val_accuracy: 0.6224\n",
      "Epoch 233/2000\n",
      "14152/14152 [==============================] - 5s 342us/step - loss: 1.0234 - accuracy: 0.6169 - val_loss: 1.0178 - val_accuracy: 0.6241\n",
      "Epoch 234/2000\n",
      "14152/14152 [==============================] - 5s 346us/step - loss: 1.0295 - accuracy: 0.6165 - val_loss: 1.0196 - val_accuracy: 0.6238\n",
      "Epoch 235/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 1.0295 - accuracy: 0.6193 - val_loss: 1.0211 - val_accuracy: 0.6215\n",
      "Epoch 236/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 1.0168 - accuracy: 0.6235 - val_loss: 1.0211 - val_accuracy: 0.6210\n",
      "Epoch 237/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 1.0264 - accuracy: 0.6183 - val_loss: 1.0161 - val_accuracy: 0.6258\n",
      "Epoch 238/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 1.0200 - accuracy: 0.6217 - val_loss: 1.0186 - val_accuracy: 0.6224\n",
      "Epoch 239/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 1.0204 - accuracy: 0.6227 - val_loss: 1.0188 - val_accuracy: 0.6249\n",
      "Epoch 240/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 1.0181 - accuracy: 0.6222 - val_loss: 1.0200 - val_accuracy: 0.6207\n",
      "Epoch 241/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 1.0177 - accuracy: 0.6208 - val_loss: 1.0176 - val_accuracy: 0.6275\n",
      "Epoch 242/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 1.0176 - accuracy: 0.6227 - val_loss: 1.0139 - val_accuracy: 0.6232\n",
      "Epoch 243/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 1.0211 - accuracy: 0.6205 - val_loss: 1.0124 - val_accuracy: 0.6232\n",
      "Epoch 244/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 1.0214 - accuracy: 0.6205 - val_loss: 1.0157 - val_accuracy: 0.6246\n",
      "Epoch 245/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 1.0154 - accuracy: 0.6222 - val_loss: 1.0120 - val_accuracy: 0.6289\n",
      "Epoch 246/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 1.0160 - accuracy: 0.6213 - val_loss: 1.0086 - val_accuracy: 0.6238\n",
      "Epoch 247/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 1.0136 - accuracy: 0.6249 - val_loss: 1.0108 - val_accuracy: 0.6292\n",
      "Epoch 248/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 1.0210 - accuracy: 0.6225 - val_loss: 1.0086 - val_accuracy: 0.6334\n",
      "Epoch 249/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 1.0166 - accuracy: 0.6205 - val_loss: 1.0088 - val_accuracy: 0.6326\n",
      "Epoch 250/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 1.0128 - accuracy: 0.6241 - val_loss: 1.0083 - val_accuracy: 0.6303\n",
      "Epoch 251/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 1.0112 - accuracy: 0.6254 - val_loss: 1.0034 - val_accuracy: 0.6286\n",
      "Epoch 252/2000\n",
      "14152/14152 [==============================] - 5s 330us/step - loss: 1.0112 - accuracy: 0.6243 - val_loss: 1.0080 - val_accuracy: 0.6269\n",
      "Epoch 253/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 1.0112 - accuracy: 0.6254 - val_loss: 1.0086 - val_accuracy: 0.6323\n",
      "Epoch 254/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 1.0101 - accuracy: 0.6227 - val_loss: 1.0062 - val_accuracy: 0.6269\n",
      "Epoch 255/2000\n",
      "14152/14152 [==============================] - 5s 330us/step - loss: 1.0118 - accuracy: 0.6221 - val_loss: 1.0058 - val_accuracy: 0.6311\n",
      "Epoch 256/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 1.0119 - accuracy: 0.6232 - val_loss: 1.0041 - val_accuracy: 0.6323\n",
      "Epoch 257/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 1.0038 - accuracy: 0.6255 - val_loss: 0.9997 - val_accuracy: 0.6337\n",
      "Epoch 258/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 1.0068 - accuracy: 0.6261 - val_loss: 1.0009 - val_accuracy: 0.6317\n",
      "Epoch 259/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 1.0084 - accuracy: 0.6251 - val_loss: 0.9992 - val_accuracy: 0.6388\n",
      "Epoch 260/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 1.0074 - accuracy: 0.6264 - val_loss: 1.0024 - val_accuracy: 0.6348\n",
      "Epoch 261/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 1.0055 - accuracy: 0.6280 - val_loss: 0.9970 - val_accuracy: 0.6345\n",
      "Epoch 262/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 1.0062 - accuracy: 0.6266 - val_loss: 1.0000 - val_accuracy: 0.6320\n",
      "Epoch 263/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 1.0063 - accuracy: 0.6239 - val_loss: 0.9992 - val_accuracy: 0.6343\n",
      "Epoch 264/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 1.0090 - accuracy: 0.6270 - val_loss: 0.9921 - val_accuracy: 0.6388\n",
      "Epoch 265/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.9990 - accuracy: 0.6302 - val_loss: 0.9984 - val_accuracy: 0.6320\n",
      "Epoch 266/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 1.0005 - accuracy: 0.6294 - val_loss: 0.9937 - val_accuracy: 0.6365\n",
      "Epoch 267/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 1.0048 - accuracy: 0.6273 - val_loss: 0.9935 - val_accuracy: 0.6388\n",
      "Epoch 268/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 1.0017 - accuracy: 0.6295 - val_loss: 0.9956 - val_accuracy: 0.6340\n",
      "Epoch 269/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 1.0013 - accuracy: 0.6300 - val_loss: 0.9927 - val_accuracy: 0.6430\n",
      "Epoch 270/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 1.0029 - accuracy: 0.6266 - val_loss: 0.9929 - val_accuracy: 0.6382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 271/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.9974 - accuracy: 0.6303 - val_loss: 0.9942 - val_accuracy: 0.6354\n",
      "Epoch 272/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 1.0012 - accuracy: 0.6283 - val_loss: 0.9915 - val_accuracy: 0.6461\n",
      "Epoch 273/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 0.9967 - accuracy: 0.6304 - val_loss: 0.9914 - val_accuracy: 0.6439\n",
      "Epoch 274/2000\n",
      "14152/14152 [==============================] - 5s 335us/step - loss: 0.9975 - accuracy: 0.6304 - val_loss: 0.9898 - val_accuracy: 0.6357\n",
      "Epoch 275/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.9981 - accuracy: 0.6311 - val_loss: 0.9877 - val_accuracy: 0.6419\n",
      "Epoch 276/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 1.0004 - accuracy: 0.6272 - val_loss: 0.9867 - val_accuracy: 0.6436\n",
      "Epoch 277/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 0.9953 - accuracy: 0.6287 - val_loss: 0.9846 - val_accuracy: 0.6416\n",
      "Epoch 278/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.9945 - accuracy: 0.6304 - val_loss: 0.9857 - val_accuracy: 0.6365\n",
      "Epoch 279/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 0.9948 - accuracy: 0.6303 - val_loss: 0.9895 - val_accuracy: 0.6371\n",
      "Epoch 280/2000\n",
      "14152/14152 [==============================] - 5s 340us/step - loss: 0.9952 - accuracy: 0.6331 - val_loss: 0.9867 - val_accuracy: 0.6441\n",
      "Epoch 281/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.9988 - accuracy: 0.6299 - val_loss: 0.9862 - val_accuracy: 0.6492\n",
      "Epoch 282/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 0.9926 - accuracy: 0.6357 - val_loss: 0.9855 - val_accuracy: 0.6433\n",
      "Epoch 283/2000\n",
      "14152/14152 [==============================] - 5s 341us/step - loss: 0.9927 - accuracy: 0.6308 - val_loss: 0.9777 - val_accuracy: 0.6478\n",
      "Epoch 284/2000\n",
      "14152/14152 [==============================] - 5s 333us/step - loss: 0.9895 - accuracy: 0.6357 - val_loss: 0.9804 - val_accuracy: 0.6447\n",
      "Epoch 285/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 0.9874 - accuracy: 0.6345 - val_loss: 0.9816 - val_accuracy: 0.6376\n",
      "Epoch 286/2000\n",
      "14152/14152 [==============================] - 4s 318us/step - loss: 0.9864 - accuracy: 0.6337 - val_loss: 0.9806 - val_accuracy: 0.6419\n",
      "Epoch 287/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 0.9904 - accuracy: 0.6308 - val_loss: 0.9795 - val_accuracy: 0.6444\n",
      "Epoch 288/2000\n",
      "14152/14152 [==============================] - 5s 329us/step - loss: 0.9849 - accuracy: 0.6358 - val_loss: 0.9850 - val_accuracy: 0.6362\n",
      "Epoch 289/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 0.9856 - accuracy: 0.6352 - val_loss: 0.9816 - val_accuracy: 0.6444\n",
      "Epoch 290/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.9884 - accuracy: 0.6349 - val_loss: 0.9812 - val_accuracy: 0.6427\n",
      "Epoch 291/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.9858 - accuracy: 0.6379 - val_loss: 0.9750 - val_accuracy: 0.6521\n",
      "Epoch 292/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 0.9847 - accuracy: 0.6331 - val_loss: 0.9783 - val_accuracy: 0.6504\n",
      "Epoch 293/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 0.9827 - accuracy: 0.6337 - val_loss: 0.9749 - val_accuracy: 0.6484\n",
      "Epoch 294/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.9815 - accuracy: 0.6359 - val_loss: 0.9769 - val_accuracy: 0.6430\n",
      "Epoch 295/2000\n",
      "14152/14152 [==============================] - 5s 335us/step - loss: 0.9829 - accuracy: 0.6357 - val_loss: 0.9776 - val_accuracy: 0.6484\n",
      "Epoch 296/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.9812 - accuracy: 0.6335 - val_loss: 0.9704 - val_accuracy: 0.6484\n",
      "Epoch 297/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.9794 - accuracy: 0.6329 - val_loss: 0.9758 - val_accuracy: 0.6456\n",
      "Epoch 298/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.9799 - accuracy: 0.6366 - val_loss: 0.9755 - val_accuracy: 0.6425\n",
      "Epoch 299/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.9784 - accuracy: 0.6386 - val_loss: 0.9728 - val_accuracy: 0.6410\n",
      "Epoch 300/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.9818 - accuracy: 0.6363 - val_loss: 0.9706 - val_accuracy: 0.6512\n",
      "Epoch 301/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.9780 - accuracy: 0.6388 - val_loss: 0.9665 - val_accuracy: 0.6478\n",
      "Epoch 302/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.9768 - accuracy: 0.6364 - val_loss: 0.9660 - val_accuracy: 0.6535\n",
      "Epoch 303/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 0.9788 - accuracy: 0.6360 - val_loss: 0.9715 - val_accuracy: 0.6504\n",
      "Epoch 304/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.9774 - accuracy: 0.6350 - val_loss: 0.9669 - val_accuracy: 0.6521\n",
      "Epoch 305/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.9750 - accuracy: 0.6402 - val_loss: 0.9683 - val_accuracy: 0.6456\n",
      "Epoch 306/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.9795 - accuracy: 0.6348 - val_loss: 0.9666 - val_accuracy: 0.6467\n",
      "Epoch 307/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.9766 - accuracy: 0.6367 - val_loss: 0.9727 - val_accuracy: 0.6532\n",
      "Epoch 308/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 0.9714 - accuracy: 0.6413 - val_loss: 0.9628 - val_accuracy: 0.6529\n",
      "Epoch 309/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 0.9731 - accuracy: 0.6394 - val_loss: 0.9647 - val_accuracy: 0.6543\n",
      "Epoch 310/2000\n",
      "14152/14152 [==============================] - 5s 329us/step - loss: 0.9791 - accuracy: 0.6353 - val_loss: 0.9620 - val_accuracy: 0.6532\n",
      "Epoch 311/2000\n",
      "14152/14152 [==============================] - 5s 348us/step - loss: 0.9733 - accuracy: 0.6378 - val_loss: 0.9575 - val_accuracy: 0.6563\n",
      "Epoch 312/2000\n",
      "14152/14152 [==============================] - 5s 345us/step - loss: 0.9724 - accuracy: 0.6395 - val_loss: 0.9595 - val_accuracy: 0.6557\n",
      "Epoch 313/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 0.9697 - accuracy: 0.6387 - val_loss: 0.9622 - val_accuracy: 0.6552\n",
      "Epoch 314/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.9717 - accuracy: 0.6404 - val_loss: 0.9596 - val_accuracy: 0.6529\n",
      "Epoch 315/2000\n",
      "14152/14152 [==============================] - 5s 318us/step - loss: 0.9695 - accuracy: 0.6388 - val_loss: 0.9601 - val_accuracy: 0.6504\n",
      "Epoch 316/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.9680 - accuracy: 0.6415 - val_loss: 0.9561 - val_accuracy: 0.6577\n",
      "Epoch 317/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.9611 - accuracy: 0.6435 - val_loss: 0.9602 - val_accuracy: 0.6515\n",
      "Epoch 318/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.9672 - accuracy: 0.6405 - val_loss: 0.9634 - val_accuracy: 0.6555\n",
      "Epoch 319/2000\n",
      "14152/14152 [==============================] - 5s 331us/step - loss: 0.9662 - accuracy: 0.6408 - val_loss: 0.9595 - val_accuracy: 0.6538\n",
      "Epoch 320/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 0.9705 - accuracy: 0.6407 - val_loss: 0.9589 - val_accuracy: 0.6538\n",
      "Epoch 321/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 0.9702 - accuracy: 0.6371 - val_loss: 0.9527 - val_accuracy: 0.6574\n",
      "Epoch 322/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.9732 - accuracy: 0.6338 - val_loss: 0.9558 - val_accuracy: 0.6538\n",
      "Epoch 323/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.9689 - accuracy: 0.6411 - val_loss: 0.9533 - val_accuracy: 0.6569\n",
      "Epoch 324/2000\n",
      "14152/14152 [==============================] - 5s 376us/step - loss: 0.9675 - accuracy: 0.6415 - val_loss: 0.9534 - val_accuracy: 0.6555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 325/2000\n",
      "14152/14152 [==============================] - 5s 331us/step - loss: 0.9679 - accuracy: 0.6364 - val_loss: 0.9570 - val_accuracy: 0.6507\n",
      "Epoch 326/2000\n",
      "14152/14152 [==============================] - 6s 426us/step - loss: 0.9640 - accuracy: 0.6415 - val_loss: 0.9522 - val_accuracy: 0.6605\n",
      "Epoch 327/2000\n",
      "14152/14152 [==============================] - 6s 402us/step - loss: 0.9629 - accuracy: 0.6429 - val_loss: 0.9558 - val_accuracy: 0.6552\n",
      "Epoch 328/2000\n",
      "14152/14152 [==============================] - 6s 393us/step - loss: 0.9648 - accuracy: 0.6400 - val_loss: 0.9571 - val_accuracy: 0.6523\n",
      "Epoch 329/2000\n",
      "14152/14152 [==============================] - 5s 353us/step - loss: 0.9649 - accuracy: 0.6407 - val_loss: 0.9528 - val_accuracy: 0.6538\n",
      "Epoch 330/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 0.9645 - accuracy: 0.6398 - val_loss: 0.9555 - val_accuracy: 0.6560\n",
      "Epoch 331/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.9630 - accuracy: 0.6417 - val_loss: 0.9485 - val_accuracy: 0.6555\n",
      "Epoch 332/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.9639 - accuracy: 0.6388 - val_loss: 0.9539 - val_accuracy: 0.6572\n",
      "Epoch 333/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.9584 - accuracy: 0.6437 - val_loss: 0.9513 - val_accuracy: 0.6557\n",
      "Epoch 334/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 0.9566 - accuracy: 0.6443 - val_loss: 0.9502 - val_accuracy: 0.6566\n",
      "Epoch 335/2000\n",
      "14152/14152 [==============================] - 5s 318us/step - loss: 0.9562 - accuracy: 0.6431 - val_loss: 0.9453 - val_accuracy: 0.6600\n",
      "Epoch 336/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 0.9601 - accuracy: 0.6411 - val_loss: 0.9438 - val_accuracy: 0.6603\n",
      "Epoch 337/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.9578 - accuracy: 0.6420 - val_loss: 0.9464 - val_accuracy: 0.6583\n",
      "Epoch 338/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.9616 - accuracy: 0.6417 - val_loss: 0.9456 - val_accuracy: 0.6588\n",
      "Epoch 339/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.9630 - accuracy: 0.6425 - val_loss: 0.9485 - val_accuracy: 0.6611\n",
      "Epoch 340/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 0.9576 - accuracy: 0.6437 - val_loss: 0.9429 - val_accuracy: 0.6591\n",
      "Epoch 341/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.9557 - accuracy: 0.6437 - val_loss: 0.9425 - val_accuracy: 0.6611\n",
      "Epoch 342/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.9533 - accuracy: 0.6423 - val_loss: 0.9492 - val_accuracy: 0.6523\n",
      "Epoch 343/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.9562 - accuracy: 0.6425 - val_loss: 0.9446 - val_accuracy: 0.6597\n",
      "Epoch 344/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 0.9536 - accuracy: 0.6446 - val_loss: 0.9385 - val_accuracy: 0.6665\n",
      "Epoch 345/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.9559 - accuracy: 0.6415 - val_loss: 0.9443 - val_accuracy: 0.6620\n",
      "Epoch 346/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.9532 - accuracy: 0.6448 - val_loss: 0.9366 - val_accuracy: 0.6648\n",
      "Epoch 347/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 0.9586 - accuracy: 0.6427 - val_loss: 0.9360 - val_accuracy: 0.6637\n",
      "Epoch 348/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 0.9558 - accuracy: 0.6450 - val_loss: 0.9438 - val_accuracy: 0.6614\n",
      "Epoch 349/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 0.9503 - accuracy: 0.6458 - val_loss: 0.9377 - val_accuracy: 0.6682\n",
      "Epoch 350/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 0.9555 - accuracy: 0.6437 - val_loss: 0.9381 - val_accuracy: 0.6605\n",
      "Epoch 351/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 0.9515 - accuracy: 0.6458 - val_loss: 0.9424 - val_accuracy: 0.6639\n",
      "Epoch 352/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.9500 - accuracy: 0.6451 - val_loss: 0.9333 - val_accuracy: 0.6656\n",
      "Epoch 353/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.9488 - accuracy: 0.6451 - val_loss: 0.9332 - val_accuracy: 0.6656\n",
      "Epoch 354/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.9511 - accuracy: 0.6476 - val_loss: 0.9331 - val_accuracy: 0.6639\n",
      "Epoch 355/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 0.9504 - accuracy: 0.6455 - val_loss: 0.9363 - val_accuracy: 0.6614\n",
      "Epoch 356/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.9454 - accuracy: 0.6485 - val_loss: 0.9343 - val_accuracy: 0.6637\n",
      "Epoch 357/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.9475 - accuracy: 0.6449 - val_loss: 0.9332 - val_accuracy: 0.6651\n",
      "Epoch 358/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.9491 - accuracy: 0.6438 - val_loss: 0.9369 - val_accuracy: 0.6617\n",
      "Epoch 359/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.9441 - accuracy: 0.6458 - val_loss: 0.9327 - val_accuracy: 0.6676\n",
      "Epoch 360/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.9401 - accuracy: 0.6478 - val_loss: 0.9316 - val_accuracy: 0.6651\n",
      "Epoch 361/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.9479 - accuracy: 0.6437 - val_loss: 0.9300 - val_accuracy: 0.6673\n",
      "Epoch 362/2000\n",
      "14152/14152 [==============================] - 5s 339us/step - loss: 0.9441 - accuracy: 0.6492 - val_loss: 0.9292 - val_accuracy: 0.6670\n",
      "Epoch 363/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 0.9470 - accuracy: 0.6451 - val_loss: 0.9276 - val_accuracy: 0.6676\n",
      "Epoch 364/2000\n",
      "14152/14152 [==============================] - 5s 354us/step - loss: 0.9429 - accuracy: 0.6496 - val_loss: 0.9298 - val_accuracy: 0.6685\n",
      "Epoch 365/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 0.9441 - accuracy: 0.6483 - val_loss: 0.9328 - val_accuracy: 0.6617\n",
      "Epoch 366/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.9417 - accuracy: 0.6475 - val_loss: 0.9248 - val_accuracy: 0.6685\n",
      "Epoch 367/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 0.9408 - accuracy: 0.6485 - val_loss: 0.9302 - val_accuracy: 0.6656\n",
      "Epoch 368/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.9403 - accuracy: 0.6497 - val_loss: 0.9245 - val_accuracy: 0.6696\n",
      "Epoch 369/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.9426 - accuracy: 0.6521 - val_loss: 0.9216 - val_accuracy: 0.6685\n",
      "Epoch 370/2000\n",
      "14152/14152 [==============================] - 5s 328us/step - loss: 0.9406 - accuracy: 0.6461 - val_loss: 0.9267 - val_accuracy: 0.6665\n",
      "Epoch 371/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.9395 - accuracy: 0.6464 - val_loss: 0.9248 - val_accuracy: 0.6724\n",
      "Epoch 372/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.9370 - accuracy: 0.6497 - val_loss: 0.9219 - val_accuracy: 0.6687\n",
      "Epoch 373/2000\n",
      "14152/14152 [==============================] - 5s 344us/step - loss: 0.9386 - accuracy: 0.6469 - val_loss: 0.9230 - val_accuracy: 0.6693\n",
      "Epoch 374/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 0.9341 - accuracy: 0.6512 - val_loss: 0.9209 - val_accuracy: 0.6687\n",
      "Epoch 375/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 0.9408 - accuracy: 0.6452 - val_loss: 0.9226 - val_accuracy: 0.6676\n",
      "Epoch 376/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.9424 - accuracy: 0.6498 - val_loss: 0.9224 - val_accuracy: 0.6690\n",
      "Epoch 377/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.9398 - accuracy: 0.6468 - val_loss: 0.9166 - val_accuracy: 0.6727\n",
      "Epoch 378/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.9364 - accuracy: 0.6495 - val_loss: 0.9221 - val_accuracy: 0.6696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 379/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 0.9352 - accuracy: 0.6478 - val_loss: 0.9266 - val_accuracy: 0.6668\n",
      "Epoch 380/2000\n",
      "14152/14152 [==============================] - 5s 331us/step - loss: 0.9339 - accuracy: 0.6502 - val_loss: 0.9173 - val_accuracy: 0.6699\n",
      "Epoch 381/2000\n",
      "14152/14152 [==============================] - 5s 329us/step - loss: 0.9339 - accuracy: 0.6497 - val_loss: 0.9183 - val_accuracy: 0.6702\n",
      "Epoch 382/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.9374 - accuracy: 0.6499 - val_loss: 0.9224 - val_accuracy: 0.6696\n",
      "Epoch 383/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 0.9278 - accuracy: 0.6519 - val_loss: 0.9195 - val_accuracy: 0.6665\n",
      "Epoch 384/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 0.9345 - accuracy: 0.6485 - val_loss: 0.9183 - val_accuracy: 0.6673\n",
      "Epoch 385/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 0.9348 - accuracy: 0.6498 - val_loss: 0.9139 - val_accuracy: 0.6744\n",
      "Epoch 386/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 0.9281 - accuracy: 0.6528 - val_loss: 0.9170 - val_accuracy: 0.6670\n",
      "Epoch 387/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.9316 - accuracy: 0.6521 - val_loss: 0.9195 - val_accuracy: 0.6665\n",
      "Epoch 388/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.9358 - accuracy: 0.6464 - val_loss: 0.9092 - val_accuracy: 0.6727\n",
      "Epoch 389/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.9310 - accuracy: 0.6482 - val_loss: 0.9126 - val_accuracy: 0.6738\n",
      "Epoch 390/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 0.9297 - accuracy: 0.6504 - val_loss: 0.9113 - val_accuracy: 0.6704\n",
      "Epoch 391/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 0.9274 - accuracy: 0.6521 - val_loss: 0.9135 - val_accuracy: 0.6733\n",
      "Epoch 392/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.9337 - accuracy: 0.6487 - val_loss: 0.9133 - val_accuracy: 0.6699\n",
      "Epoch 393/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.9237 - accuracy: 0.6548 - val_loss: 0.9132 - val_accuracy: 0.6710\n",
      "Epoch 394/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.9301 - accuracy: 0.6531 - val_loss: 0.9129 - val_accuracy: 0.6721\n",
      "Epoch 395/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.9268 - accuracy: 0.6503 - val_loss: 0.9080 - val_accuracy: 0.6744\n",
      "Epoch 396/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.9228 - accuracy: 0.6544 - val_loss: 0.9078 - val_accuracy: 0.6735\n",
      "Epoch 397/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.9248 - accuracy: 0.6535 - val_loss: 0.9088 - val_accuracy: 0.6718\n",
      "Epoch 398/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 0.9259 - accuracy: 0.6523 - val_loss: 0.9054 - val_accuracy: 0.6761\n",
      "Epoch 399/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.9266 - accuracy: 0.6502 - val_loss: 0.9066 - val_accuracy: 0.6704\n",
      "Epoch 400/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.9287 - accuracy: 0.6520 - val_loss: 0.9088 - val_accuracy: 0.6724\n",
      "Epoch 401/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.9211 - accuracy: 0.6531 - val_loss: 0.9044 - val_accuracy: 0.6764\n",
      "Epoch 402/2000\n",
      "14152/14152 [==============================] - 5s 318us/step - loss: 0.9185 - accuracy: 0.6528 - val_loss: 0.9027 - val_accuracy: 0.6778\n",
      "Epoch 403/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.9246 - accuracy: 0.6520 - val_loss: 0.9053 - val_accuracy: 0.6724\n",
      "Epoch 404/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.9243 - accuracy: 0.6515 - val_loss: 0.9056 - val_accuracy: 0.6716\n",
      "Epoch 405/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 0.9252 - accuracy: 0.6520 - val_loss: 0.9052 - val_accuracy: 0.6721\n",
      "Epoch 406/2000\n",
      "14152/14152 [==============================] - 5s 362us/step - loss: 0.9205 - accuracy: 0.6557 - val_loss: 0.9033 - val_accuracy: 0.6755\n",
      "Epoch 407/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.9246 - accuracy: 0.6507 - val_loss: 0.9051 - val_accuracy: 0.6747\n",
      "Epoch 408/2000\n",
      "14152/14152 [==============================] - 5s 328us/step - loss: 0.9204 - accuracy: 0.6549 - val_loss: 0.9050 - val_accuracy: 0.6744\n",
      "Epoch 409/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.9204 - accuracy: 0.6520 - val_loss: 0.9017 - val_accuracy: 0.6721\n",
      "Epoch 410/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.9214 - accuracy: 0.6531 - val_loss: 0.9013 - val_accuracy: 0.6750\n",
      "Epoch 411/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.9222 - accuracy: 0.6533 - val_loss: 0.9040 - val_accuracy: 0.6781\n",
      "Epoch 412/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 0.9147 - accuracy: 0.6552 - val_loss: 0.9039 - val_accuracy: 0.6718\n",
      "Epoch 413/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.9219 - accuracy: 0.6540 - val_loss: 0.8961 - val_accuracy: 0.6772\n",
      "Epoch 414/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.9250 - accuracy: 0.6486 - val_loss: 0.9013 - val_accuracy: 0.6769\n",
      "Epoch 415/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.9176 - accuracy: 0.6545 - val_loss: 0.8978 - val_accuracy: 0.6783\n",
      "Epoch 416/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.9204 - accuracy: 0.6508 - val_loss: 0.9011 - val_accuracy: 0.6758\n",
      "Epoch 417/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.9146 - accuracy: 0.6543 - val_loss: 0.8982 - val_accuracy: 0.6727\n",
      "Epoch 418/2000\n",
      "14152/14152 [==============================] - 5s 340us/step - loss: 0.9153 - accuracy: 0.6542 - val_loss: 0.8927 - val_accuracy: 0.6781\n",
      "Epoch 419/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.9126 - accuracy: 0.6552 - val_loss: 0.8991 - val_accuracy: 0.6750\n",
      "Epoch 420/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 0.9127 - accuracy: 0.6536 - val_loss: 0.8947 - val_accuracy: 0.6789\n",
      "Epoch 421/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 0.9156 - accuracy: 0.6581 - val_loss: 0.8898 - val_accuracy: 0.6781\n",
      "Epoch 422/2000\n",
      "14152/14152 [==============================] - 5s 333us/step - loss: 0.9168 - accuracy: 0.6557 - val_loss: 0.8926 - val_accuracy: 0.6817\n",
      "Epoch 423/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 0.9096 - accuracy: 0.6612 - val_loss: 0.8918 - val_accuracy: 0.6783\n",
      "Epoch 424/2000\n",
      "14152/14152 [==============================] - 5s 331us/step - loss: 0.9146 - accuracy: 0.6547 - val_loss: 0.8972 - val_accuracy: 0.6724\n",
      "Epoch 425/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 0.9136 - accuracy: 0.6543 - val_loss: 0.8952 - val_accuracy: 0.6786\n",
      "Epoch 426/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.9080 - accuracy: 0.6584 - val_loss: 0.8916 - val_accuracy: 0.6798\n",
      "Epoch 427/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 0.9092 - accuracy: 0.6564 - val_loss: 0.8916 - val_accuracy: 0.6772\n",
      "Epoch 428/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.9054 - accuracy: 0.6588 - val_loss: 0.8926 - val_accuracy: 0.6817\n",
      "Epoch 429/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.9107 - accuracy: 0.6555 - val_loss: 0.8979 - val_accuracy: 0.6727\n",
      "Epoch 430/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 0.9059 - accuracy: 0.6569 - val_loss: 0.8882 - val_accuracy: 0.6800\n",
      "Epoch 431/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.9081 - accuracy: 0.6570 - val_loss: 0.8910 - val_accuracy: 0.6798\n",
      "Epoch 432/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.9105 - accuracy: 0.6556 - val_loss: 0.8920 - val_accuracy: 0.6789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 433/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.9043 - accuracy: 0.6579 - val_loss: 0.8909 - val_accuracy: 0.6772\n",
      "Epoch 434/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.9080 - accuracy: 0.6565 - val_loss: 0.8874 - val_accuracy: 0.6806\n",
      "Epoch 435/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 0.9085 - accuracy: 0.6552 - val_loss: 0.8856 - val_accuracy: 0.6764\n",
      "Epoch 436/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.9032 - accuracy: 0.6596 - val_loss: 0.8819 - val_accuracy: 0.6837\n",
      "Epoch 437/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.9031 - accuracy: 0.6593 - val_loss: 0.8888 - val_accuracy: 0.6781\n",
      "Epoch 438/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.9087 - accuracy: 0.6572 - val_loss: 0.8872 - val_accuracy: 0.6795\n",
      "Epoch 439/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.9021 - accuracy: 0.6581 - val_loss: 0.8870 - val_accuracy: 0.6815\n",
      "Epoch 440/2000\n",
      "14152/14152 [==============================] - 5s 352us/step - loss: 0.9036 - accuracy: 0.6573 - val_loss: 0.8886 - val_accuracy: 0.6798\n",
      "Epoch 441/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 0.9081 - accuracy: 0.6575 - val_loss: 0.8792 - val_accuracy: 0.6829\n",
      "Epoch 442/2000\n",
      "14152/14152 [==============================] - 5s 328us/step - loss: 0.9058 - accuracy: 0.6554 - val_loss: 0.8771 - val_accuracy: 0.6826\n",
      "Epoch 443/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 0.9060 - accuracy: 0.6582 - val_loss: 0.8819 - val_accuracy: 0.6795\n",
      "Epoch 444/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 0.9045 - accuracy: 0.6571 - val_loss: 0.8827 - val_accuracy: 0.6829\n",
      "Epoch 445/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 0.9002 - accuracy: 0.6605 - val_loss: 0.8798 - val_accuracy: 0.6792\n",
      "Epoch 446/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.9015 - accuracy: 0.6586 - val_loss: 0.8847 - val_accuracy: 0.6812\n",
      "Epoch 447/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.9098 - accuracy: 0.6559 - val_loss: 0.8774 - val_accuracy: 0.6843\n",
      "Epoch 448/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.8958 - accuracy: 0.6595 - val_loss: 0.8839 - val_accuracy: 0.6758\n",
      "Epoch 449/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.9012 - accuracy: 0.6594 - val_loss: 0.8822 - val_accuracy: 0.6803\n",
      "Epoch 450/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.8977 - accuracy: 0.6615 - val_loss: 0.8765 - val_accuracy: 0.6826\n",
      "Epoch 451/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.8959 - accuracy: 0.6609 - val_loss: 0.8802 - val_accuracy: 0.6817\n",
      "Epoch 452/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 0.8975 - accuracy: 0.6574 - val_loss: 0.8771 - val_accuracy: 0.6800\n",
      "Epoch 453/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 0.8962 - accuracy: 0.6606 - val_loss: 0.8776 - val_accuracy: 0.6832\n",
      "Epoch 454/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.8992 - accuracy: 0.6562 - val_loss: 0.8798 - val_accuracy: 0.6812\n",
      "Epoch 455/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.8957 - accuracy: 0.6600 - val_loss: 0.8752 - val_accuracy: 0.6815\n",
      "Epoch 456/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 0.8980 - accuracy: 0.6588 - val_loss: 0.8806 - val_accuracy: 0.6840\n",
      "Epoch 457/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 0.8972 - accuracy: 0.6593 - val_loss: 0.8757 - val_accuracy: 0.6829\n",
      "Epoch 458/2000\n",
      "14152/14152 [==============================] - 5s 365us/step - loss: 0.8941 - accuracy: 0.6626 - val_loss: 0.8766 - val_accuracy: 0.6803\n",
      "Epoch 459/2000\n",
      "14152/14152 [==============================] - 5s 352us/step - loss: 0.8953 - accuracy: 0.6608 - val_loss: 0.8744 - val_accuracy: 0.6826\n",
      "Epoch 460/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 0.8936 - accuracy: 0.6595 - val_loss: 0.8754 - val_accuracy: 0.6829\n",
      "Epoch 461/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 0.8993 - accuracy: 0.6582 - val_loss: 0.8750 - val_accuracy: 0.6854\n",
      "Epoch 462/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.8936 - accuracy: 0.6603 - val_loss: 0.8757 - val_accuracy: 0.6823\n",
      "Epoch 463/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.8934 - accuracy: 0.6612 - val_loss: 0.8706 - val_accuracy: 0.6832\n",
      "Epoch 464/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.8946 - accuracy: 0.6618 - val_loss: 0.8795 - val_accuracy: 0.6783\n",
      "Epoch 465/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.8956 - accuracy: 0.6605 - val_loss: 0.8773 - val_accuracy: 0.6800\n",
      "Epoch 466/2000\n",
      "14152/14152 [==============================] - 5s 331us/step - loss: 0.8950 - accuracy: 0.6594 - val_loss: 0.8728 - val_accuracy: 0.6809\n",
      "Epoch 467/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 0.8894 - accuracy: 0.6635 - val_loss: 0.8696 - val_accuracy: 0.6854\n",
      "Epoch 468/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.8926 - accuracy: 0.6597 - val_loss: 0.8687 - val_accuracy: 0.6854\n",
      "Epoch 469/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 0.8988 - accuracy: 0.6596 - val_loss: 0.8680 - val_accuracy: 0.6860\n",
      "Epoch 470/2000\n",
      "14152/14152 [==============================] - 5s 379us/step - loss: 0.8944 - accuracy: 0.6597 - val_loss: 0.8768 - val_accuracy: 0.6789\n",
      "Epoch 471/2000\n",
      "14152/14152 [==============================] - 5s 346us/step - loss: 0.8884 - accuracy: 0.6625 - val_loss: 0.8692 - val_accuracy: 0.6820\n",
      "Epoch 472/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.8888 - accuracy: 0.6606 - val_loss: 0.8706 - val_accuracy: 0.6834\n",
      "Epoch 473/2000\n",
      "14152/14152 [==============================] - 4s 318us/step - loss: 0.8925 - accuracy: 0.6620 - val_loss: 0.8655 - val_accuracy: 0.6863\n",
      "Epoch 474/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.8902 - accuracy: 0.6600 - val_loss: 0.8742 - val_accuracy: 0.6837\n",
      "Epoch 475/2000\n",
      "14152/14152 [==============================] - 5s 352us/step - loss: 0.8886 - accuracy: 0.6621 - val_loss: 0.8670 - val_accuracy: 0.6849\n",
      "Epoch 476/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.8868 - accuracy: 0.6600 - val_loss: 0.8657 - val_accuracy: 0.6826\n",
      "Epoch 477/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.8874 - accuracy: 0.6612 - val_loss: 0.8727 - val_accuracy: 0.6798\n",
      "Epoch 478/2000\n",
      "14152/14152 [==============================] - 5s 354us/step - loss: 0.8884 - accuracy: 0.6611 - val_loss: 0.8731 - val_accuracy: 0.6803\n",
      "Epoch 479/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 0.8914 - accuracy: 0.6579 - val_loss: 0.8628 - val_accuracy: 0.6854\n",
      "Epoch 480/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 0.8836 - accuracy: 0.6599 - val_loss: 0.8644 - val_accuracy: 0.6854\n",
      "Epoch 481/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.8839 - accuracy: 0.6623 - val_loss: 0.8694 - val_accuracy: 0.6832\n",
      "Epoch 482/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.8873 - accuracy: 0.6612 - val_loss: 0.8697 - val_accuracy: 0.6840\n",
      "Epoch 483/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.8824 - accuracy: 0.6619 - val_loss: 0.8607 - val_accuracy: 0.6860\n",
      "Epoch 484/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.8860 - accuracy: 0.6615 - val_loss: 0.8673 - val_accuracy: 0.6860\n",
      "Epoch 485/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.8848 - accuracy: 0.6629 - val_loss: 0.8619 - val_accuracy: 0.6865\n",
      "Epoch 486/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 0.8894 - accuracy: 0.6581 - val_loss: 0.8605 - val_accuracy: 0.6857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 487/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.8798 - accuracy: 0.6649 - val_loss: 0.8637 - val_accuracy: 0.6863\n",
      "Epoch 488/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.8856 - accuracy: 0.6592 - val_loss: 0.8607 - val_accuracy: 0.6891\n",
      "Epoch 489/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.8839 - accuracy: 0.6598 - val_loss: 0.8623 - val_accuracy: 0.6874\n",
      "Epoch 490/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.8793 - accuracy: 0.6618 - val_loss: 0.8540 - val_accuracy: 0.6899\n",
      "Epoch 491/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.8821 - accuracy: 0.6620 - val_loss: 0.8597 - val_accuracy: 0.6885\n",
      "Epoch 492/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.8851 - accuracy: 0.6615 - val_loss: 0.8595 - val_accuracy: 0.6857\n",
      "Epoch 493/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.8791 - accuracy: 0.6629 - val_loss: 0.8606 - val_accuracy: 0.6854\n",
      "Epoch 494/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.8808 - accuracy: 0.6622 - val_loss: 0.8606 - val_accuracy: 0.6860\n",
      "Epoch 495/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.8802 - accuracy: 0.6618 - val_loss: 0.8528 - val_accuracy: 0.6871\n",
      "Epoch 496/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 0.8822 - accuracy: 0.6639 - val_loss: 0.8614 - val_accuracy: 0.6812\n",
      "Epoch 497/2000\n",
      "14152/14152 [==============================] - 5s 342us/step - loss: 0.8834 - accuracy: 0.6617 - val_loss: 0.8605 - val_accuracy: 0.6860\n",
      "Epoch 498/2000\n",
      "14152/14152 [==============================] - 5s 333us/step - loss: 0.8731 - accuracy: 0.6637 - val_loss: 0.8605 - val_accuracy: 0.6854\n",
      "Epoch 499/2000\n",
      "14152/14152 [==============================] - 5s 335us/step - loss: 0.8758 - accuracy: 0.6656 - val_loss: 0.8535 - val_accuracy: 0.6891\n",
      "Epoch 500/2000\n",
      "14152/14152 [==============================] - 5s 342us/step - loss: 0.8764 - accuracy: 0.6636 - val_loss: 0.8594 - val_accuracy: 0.6846\n",
      "Epoch 501/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.8770 - accuracy: 0.6649 - val_loss: 0.8497 - val_accuracy: 0.6897\n",
      "Epoch 502/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.8801 - accuracy: 0.6630 - val_loss: 0.8557 - val_accuracy: 0.6877\n",
      "Epoch 503/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.8790 - accuracy: 0.6642 - val_loss: 0.8583 - val_accuracy: 0.6863\n",
      "Epoch 504/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.8735 - accuracy: 0.6621 - val_loss: 0.8551 - val_accuracy: 0.6888\n",
      "Epoch 505/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.8724 - accuracy: 0.6670 - val_loss: 0.8505 - val_accuracy: 0.6905\n",
      "Epoch 506/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.8772 - accuracy: 0.6641 - val_loss: 0.8486 - val_accuracy: 0.6919\n",
      "Epoch 507/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 0.8641 - accuracy: 0.6700 - val_loss: 0.8529 - val_accuracy: 0.6885\n",
      "Epoch 508/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.8702 - accuracy: 0.6658 - val_loss: 0.8419 - val_accuracy: 0.6902\n",
      "Epoch 509/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.8708 - accuracy: 0.6660 - val_loss: 0.8498 - val_accuracy: 0.6908\n",
      "Epoch 510/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.8705 - accuracy: 0.6658 - val_loss: 0.8553 - val_accuracy: 0.6897\n",
      "Epoch 511/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.8738 - accuracy: 0.6650 - val_loss: 0.8532 - val_accuracy: 0.6860\n",
      "Epoch 512/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.8681 - accuracy: 0.6683 - val_loss: 0.8494 - val_accuracy: 0.6899\n",
      "Epoch 513/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.8708 - accuracy: 0.6644 - val_loss: 0.8472 - val_accuracy: 0.6919\n",
      "Epoch 514/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.8714 - accuracy: 0.6660 - val_loss: 0.8473 - val_accuracy: 0.6894\n",
      "Epoch 515/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 0.8700 - accuracy: 0.6661 - val_loss: 0.8475 - val_accuracy: 0.6914\n",
      "Epoch 516/2000\n",
      "14152/14152 [==============================] - 5s 382us/step - loss: 0.8696 - accuracy: 0.6665 - val_loss: 0.8482 - val_accuracy: 0.6865\n",
      "Epoch 517/2000\n",
      "14152/14152 [==============================] - 5s 344us/step - loss: 0.8700 - accuracy: 0.6629 - val_loss: 0.8482 - val_accuracy: 0.6880\n",
      "Epoch 518/2000\n",
      "14152/14152 [==============================] - 5s 354us/step - loss: 0.8675 - accuracy: 0.6656 - val_loss: 0.8466 - val_accuracy: 0.6888\n",
      "Epoch 519/2000\n",
      "14152/14152 [==============================] - 5s 344us/step - loss: 0.8689 - accuracy: 0.6648 - val_loss: 0.8489 - val_accuracy: 0.6897\n",
      "Epoch 520/2000\n",
      "14152/14152 [==============================] - 6s 396us/step - loss: 0.8723 - accuracy: 0.6641 - val_loss: 0.8428 - val_accuracy: 0.6905\n",
      "Epoch 521/2000\n",
      "14152/14152 [==============================] - 5s 355us/step - loss: 0.8563 - accuracy: 0.6687 - val_loss: 0.8433 - val_accuracy: 0.6916\n",
      "Epoch 522/2000\n",
      "14152/14152 [==============================] - 5s 358us/step - loss: 0.8668 - accuracy: 0.6666 - val_loss: 0.8493 - val_accuracy: 0.6868\n",
      "Epoch 523/2000\n",
      "14152/14152 [==============================] - 5s 339us/step - loss: 0.8704 - accuracy: 0.6653 - val_loss: 0.8463 - val_accuracy: 0.6916\n",
      "Epoch 524/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.8644 - accuracy: 0.6667 - val_loss: 0.8425 - val_accuracy: 0.6911\n",
      "Epoch 525/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 0.8639 - accuracy: 0.6680 - val_loss: 0.8411 - val_accuracy: 0.6911\n",
      "Epoch 526/2000\n",
      "14152/14152 [==============================] - 5s 339us/step - loss: 0.8656 - accuracy: 0.6665 - val_loss: 0.8467 - val_accuracy: 0.6877\n",
      "Epoch 527/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.8695 - accuracy: 0.6636 - val_loss: 0.8456 - val_accuracy: 0.6894\n",
      "Epoch 528/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 0.8627 - accuracy: 0.6675 - val_loss: 0.8408 - val_accuracy: 0.6914\n",
      "Epoch 529/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.8640 - accuracy: 0.6671 - val_loss: 0.8390 - val_accuracy: 0.6908\n",
      "Epoch 530/2000\n",
      "14152/14152 [==============================] - 4s 318us/step - loss: 0.8638 - accuracy: 0.6694 - val_loss: 0.8459 - val_accuracy: 0.6899\n",
      "Epoch 531/2000\n",
      "14152/14152 [==============================] - 5s 318us/step - loss: 0.8674 - accuracy: 0.6662 - val_loss: 0.8473 - val_accuracy: 0.6863\n",
      "Epoch 532/2000\n",
      "14152/14152 [==============================] - 5s 361us/step - loss: 0.8649 - accuracy: 0.6664 - val_loss: 0.8528 - val_accuracy: 0.6854\n",
      "Epoch 533/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 0.8610 - accuracy: 0.6690 - val_loss: 0.8437 - val_accuracy: 0.6891\n",
      "Epoch 534/2000\n",
      "14152/14152 [==============================] - 5s 318us/step - loss: 0.8603 - accuracy: 0.6668 - val_loss: 0.8369 - val_accuracy: 0.6911\n",
      "Epoch 535/2000\n",
      "14152/14152 [==============================] - 5s 353us/step - loss: 0.8585 - accuracy: 0.6673 - val_loss: 0.8455 - val_accuracy: 0.6880\n",
      "Epoch 536/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 0.8599 - accuracy: 0.6682 - val_loss: 0.8422 - val_accuracy: 0.6882\n",
      "Epoch 537/2000\n",
      "14152/14152 [==============================] - 5s 328us/step - loss: 0.8586 - accuracy: 0.6684 - val_loss: 0.8367 - val_accuracy: 0.6905\n",
      "Epoch 538/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 0.8591 - accuracy: 0.6703 - val_loss: 0.8405 - val_accuracy: 0.6891\n",
      "Epoch 539/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.8582 - accuracy: 0.6678 - val_loss: 0.8411 - val_accuracy: 0.6902\n",
      "Epoch 540/2000\n",
      "14152/14152 [==============================] - 5s 370us/step - loss: 0.8623 - accuracy: 0.6682 - val_loss: 0.8342 - val_accuracy: 0.6930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 541/2000\n",
      "14152/14152 [==============================] - 5s 358us/step - loss: 0.8539 - accuracy: 0.6687 - val_loss: 0.8367 - val_accuracy: 0.6914\n",
      "Epoch 542/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 0.8596 - accuracy: 0.6661 - val_loss: 0.8409 - val_accuracy: 0.6899\n",
      "Epoch 543/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.8550 - accuracy: 0.6704 - val_loss: 0.8372 - val_accuracy: 0.6914\n",
      "Epoch 544/2000\n",
      "14152/14152 [==============================] - 5s 335us/step - loss: 0.8654 - accuracy: 0.6663 - val_loss: 0.8373 - val_accuracy: 0.6908\n",
      "Epoch 545/2000\n",
      "14152/14152 [==============================] - 5s 329us/step - loss: 0.8577 - accuracy: 0.6684 - val_loss: 0.8307 - val_accuracy: 0.6922\n",
      "Epoch 546/2000\n",
      "14152/14152 [==============================] - 5s 335us/step - loss: 0.8579 - accuracy: 0.6689 - val_loss: 0.8358 - val_accuracy: 0.6933\n",
      "Epoch 547/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 0.8541 - accuracy: 0.6715 - val_loss: 0.8304 - val_accuracy: 0.6942\n",
      "Epoch 548/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.8578 - accuracy: 0.6696 - val_loss: 0.8365 - val_accuracy: 0.6899\n",
      "Epoch 549/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 0.8555 - accuracy: 0.6705 - val_loss: 0.8410 - val_accuracy: 0.6891\n",
      "Epoch 550/2000\n",
      "14152/14152 [==============================] - 5s 381us/step - loss: 0.8581 - accuracy: 0.6719 - val_loss: 0.8393 - val_accuracy: 0.6902\n",
      "Epoch 551/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.8611 - accuracy: 0.6690 - val_loss: 0.8343 - val_accuracy: 0.6914\n",
      "Epoch 552/2000\n",
      "14152/14152 [==============================] - 6s 389us/step - loss: 0.8566 - accuracy: 0.6678 - val_loss: 0.8425 - val_accuracy: 0.6874\n",
      "Epoch 553/2000\n",
      "14152/14152 [==============================] - 5s 376us/step - loss: 0.8575 - accuracy: 0.6698 - val_loss: 0.8323 - val_accuracy: 0.6902\n",
      "Epoch 554/2000\n",
      "14152/14152 [==============================] - 5s 371us/step - loss: 0.8537 - accuracy: 0.6694 - val_loss: 0.8239 - val_accuracy: 0.6956\n",
      "Epoch 555/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 0.8438 - accuracy: 0.6732 - val_loss: 0.8319 - val_accuracy: 0.6916\n",
      "Epoch 556/2000\n",
      "14152/14152 [==============================] - 5s 328us/step - loss: 0.8534 - accuracy: 0.6690 - val_loss: 0.8326 - val_accuracy: 0.6905\n",
      "Epoch 557/2000\n",
      "14152/14152 [==============================] - 4s 318us/step - loss: 0.8498 - accuracy: 0.6688 - val_loss: 0.8336 - val_accuracy: 0.6891\n",
      "Epoch 558/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 0.8528 - accuracy: 0.6682 - val_loss: 0.8322 - val_accuracy: 0.6899\n",
      "Epoch 559/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 0.8547 - accuracy: 0.6700 - val_loss: 0.8277 - val_accuracy: 0.6914\n",
      "Epoch 560/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.8513 - accuracy: 0.6694 - val_loss: 0.8260 - val_accuracy: 0.6928\n",
      "Epoch 561/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.8522 - accuracy: 0.6691 - val_loss: 0.8256 - val_accuracy: 0.6939\n",
      "Epoch 562/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.8513 - accuracy: 0.6695 - val_loss: 0.8292 - val_accuracy: 0.6925\n",
      "Epoch 563/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.8453 - accuracy: 0.6699 - val_loss: 0.8267 - val_accuracy: 0.6936\n",
      "Epoch 564/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.8487 - accuracy: 0.6721 - val_loss: 0.8349 - val_accuracy: 0.6897\n",
      "Epoch 565/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.8496 - accuracy: 0.6681 - val_loss: 0.8325 - val_accuracy: 0.6894\n",
      "Epoch 566/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.8485 - accuracy: 0.6711 - val_loss: 0.8261 - val_accuracy: 0.6905\n",
      "Epoch 567/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.8472 - accuracy: 0.6718 - val_loss: 0.8179 - val_accuracy: 0.6953\n",
      "Epoch 568/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.8456 - accuracy: 0.6709 - val_loss: 0.8295 - val_accuracy: 0.6925\n",
      "Epoch 569/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.8496 - accuracy: 0.6690 - val_loss: 0.8297 - val_accuracy: 0.6902\n",
      "Epoch 570/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.8515 - accuracy: 0.6675 - val_loss: 0.8314 - val_accuracy: 0.6908\n",
      "Epoch 571/2000\n",
      "14152/14152 [==============================] - 5s 344us/step - loss: 0.8471 - accuracy: 0.6699 - val_loss: 0.8208 - val_accuracy: 0.6945\n",
      "Epoch 572/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 0.8481 - accuracy: 0.6701 - val_loss: 0.8254 - val_accuracy: 0.6925\n",
      "Epoch 573/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 0.8491 - accuracy: 0.6706 - val_loss: 0.8225 - val_accuracy: 0.6959\n",
      "Epoch 574/2000\n",
      "14152/14152 [==============================] - 5s 342us/step - loss: 0.8526 - accuracy: 0.6713 - val_loss: 0.8289 - val_accuracy: 0.6919\n",
      "Epoch 575/2000\n",
      "14152/14152 [==============================] - 5s 339us/step - loss: 0.8516 - accuracy: 0.6696 - val_loss: 0.8283 - val_accuracy: 0.6914\n",
      "Epoch 576/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 0.8432 - accuracy: 0.6735 - val_loss: 0.8221 - val_accuracy: 0.6939\n",
      "Epoch 577/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 0.8433 - accuracy: 0.6728 - val_loss: 0.8243 - val_accuracy: 0.6922\n",
      "Epoch 578/2000\n",
      "14152/14152 [==============================] - 5s 351us/step - loss: 0.8485 - accuracy: 0.6678 - val_loss: 0.8200 - val_accuracy: 0.6947\n",
      "Epoch 579/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 0.8374 - accuracy: 0.6751 - val_loss: 0.8167 - val_accuracy: 0.6953\n",
      "Epoch 580/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 0.8444 - accuracy: 0.6711 - val_loss: 0.8188 - val_accuracy: 0.6953\n",
      "Epoch 581/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.8397 - accuracy: 0.6742 - val_loss: 0.8260 - val_accuracy: 0.6914\n",
      "Epoch 582/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 0.8403 - accuracy: 0.6721 - val_loss: 0.8238 - val_accuracy: 0.6939\n",
      "Epoch 583/2000\n",
      "14152/14152 [==============================] - 5s 329us/step - loss: 0.8481 - accuracy: 0.6708 - val_loss: 0.8186 - val_accuracy: 0.6959\n",
      "Epoch 584/2000\n",
      "14152/14152 [==============================] - 5s 348us/step - loss: 0.8388 - accuracy: 0.6729 - val_loss: 0.8169 - val_accuracy: 0.6942\n",
      "Epoch 585/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 0.8431 - accuracy: 0.6732 - val_loss: 0.8210 - val_accuracy: 0.6945\n",
      "Epoch 586/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 0.8417 - accuracy: 0.6742 - val_loss: 0.8177 - val_accuracy: 0.6947\n",
      "Epoch 587/2000\n",
      "14152/14152 [==============================] - 5s 330us/step - loss: 0.8438 - accuracy: 0.6738 - val_loss: 0.8133 - val_accuracy: 0.6956\n",
      "Epoch 588/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.8435 - accuracy: 0.6741 - val_loss: 0.8179 - val_accuracy: 0.6950\n",
      "Epoch 589/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.8309 - accuracy: 0.6759 - val_loss: 0.8181 - val_accuracy: 0.6947\n",
      "Epoch 590/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.8370 - accuracy: 0.6724 - val_loss: 0.8148 - val_accuracy: 0.6956\n",
      "Epoch 591/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.8369 - accuracy: 0.6746 - val_loss: 0.8138 - val_accuracy: 0.6964\n",
      "Epoch 592/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.8328 - accuracy: 0.6745 - val_loss: 0.8155 - val_accuracy: 0.6959\n",
      "Epoch 593/2000\n",
      "14152/14152 [==============================] - 5s 333us/step - loss: 0.8349 - accuracy: 0.6756 - val_loss: 0.8132 - val_accuracy: 0.6947\n",
      "Epoch 594/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.8357 - accuracy: 0.6744 - val_loss: 0.8200 - val_accuracy: 0.6928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 595/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.8402 - accuracy: 0.6716 - val_loss: 0.8131 - val_accuracy: 0.6962\n",
      "Epoch 596/2000\n",
      "14152/14152 [==============================] - 4s 318us/step - loss: 0.8359 - accuracy: 0.6740 - val_loss: 0.8208 - val_accuracy: 0.6933\n",
      "Epoch 597/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.8317 - accuracy: 0.6757 - val_loss: 0.8159 - val_accuracy: 0.6962\n",
      "Epoch 598/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.8332 - accuracy: 0.6749 - val_loss: 0.8139 - val_accuracy: 0.6959\n",
      "Epoch 599/2000\n",
      "14152/14152 [==============================] - 5s 370us/step - loss: 0.8327 - accuracy: 0.6778 - val_loss: 0.8173 - val_accuracy: 0.6956\n",
      "Epoch 600/2000\n",
      "14152/14152 [==============================] - 5s 376us/step - loss: 0.8325 - accuracy: 0.6751 - val_loss: 0.8201 - val_accuracy: 0.6936\n",
      "Epoch 601/2000\n",
      "14152/14152 [==============================] - 5s 347us/step - loss: 0.8304 - accuracy: 0.6760 - val_loss: 0.8114 - val_accuracy: 0.6956\n",
      "Epoch 602/2000\n",
      "14152/14152 [==============================] - 5s 376us/step - loss: 0.8357 - accuracy: 0.6733 - val_loss: 0.8123 - val_accuracy: 0.6973\n",
      "Epoch 603/2000\n",
      "14152/14152 [==============================] - 5s 383us/step - loss: 0.8280 - accuracy: 0.6757 - val_loss: 0.8142 - val_accuracy: 0.6970\n",
      "Epoch 604/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 0.8308 - accuracy: 0.6762 - val_loss: 0.8146 - val_accuracy: 0.6953\n",
      "Epoch 605/2000\n",
      "14152/14152 [==============================] - 5s 330us/step - loss: 0.8332 - accuracy: 0.6745 - val_loss: 0.8148 - val_accuracy: 0.6956\n",
      "Epoch 606/2000\n",
      "14152/14152 [==============================] - 5s 328us/step - loss: 0.8306 - accuracy: 0.6769 - val_loss: 0.8029 - val_accuracy: 0.6987\n",
      "Epoch 607/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 0.8323 - accuracy: 0.6747 - val_loss: 0.8102 - val_accuracy: 0.6973\n",
      "Epoch 608/2000\n",
      "14152/14152 [==============================] - 5s 328us/step - loss: 0.8314 - accuracy: 0.6757 - val_loss: 0.8145 - val_accuracy: 0.6970\n",
      "Epoch 609/2000\n",
      "14152/14152 [==============================] - 4s 318us/step - loss: 0.8333 - accuracy: 0.6758 - val_loss: 0.8103 - val_accuracy: 0.6995\n",
      "Epoch 610/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 0.8376 - accuracy: 0.6740 - val_loss: 0.8144 - val_accuracy: 0.6962\n",
      "Epoch 611/2000\n",
      "14152/14152 [==============================] - 5s 343us/step - loss: 0.8287 - accuracy: 0.6776 - val_loss: 0.8107 - val_accuracy: 0.6970\n",
      "Epoch 612/2000\n",
      "14152/14152 [==============================] - 5s 341us/step - loss: 0.8313 - accuracy: 0.6760 - val_loss: 0.8047 - val_accuracy: 0.7001\n",
      "Epoch 613/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 0.8235 - accuracy: 0.6786 - val_loss: 0.8124 - val_accuracy: 0.6959\n",
      "Epoch 614/2000\n",
      "14152/14152 [==============================] - 5s 330us/step - loss: 0.8287 - accuracy: 0.6769 - val_loss: 0.8064 - val_accuracy: 0.6979\n",
      "Epoch 615/2000\n",
      "14152/14152 [==============================] - 5s 343us/step - loss: 0.8223 - accuracy: 0.6788 - val_loss: 0.8057 - val_accuracy: 0.6993\n",
      "Epoch 616/2000\n",
      "14152/14152 [==============================] - 5s 346us/step - loss: 0.8362 - accuracy: 0.6721 - val_loss: 0.8016 - val_accuracy: 0.6995\n",
      "Epoch 617/2000\n",
      "14152/14152 [==============================] - 5s 349us/step - loss: 0.8287 - accuracy: 0.6762 - val_loss: 0.8062 - val_accuracy: 0.6981\n",
      "Epoch 618/2000\n",
      "14152/14152 [==============================] - 5s 331us/step - loss: 0.8293 - accuracy: 0.6740 - val_loss: 0.8099 - val_accuracy: 0.6959\n",
      "Epoch 619/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.8261 - accuracy: 0.6768 - val_loss: 0.8075 - val_accuracy: 0.6970\n",
      "Epoch 620/2000\n",
      "14152/14152 [==============================] - 5s 355us/step - loss: 0.8251 - accuracy: 0.6771 - val_loss: 0.8066 - val_accuracy: 0.6981\n",
      "Epoch 621/2000\n",
      "14152/14152 [==============================] - 6s 396us/step - loss: 0.8194 - accuracy: 0.6786 - val_loss: 0.8057 - val_accuracy: 0.6993\n",
      "Epoch 622/2000\n",
      "14152/14152 [==============================] - 5s 366us/step - loss: 0.8236 - accuracy: 0.6781 - val_loss: 0.8031 - val_accuracy: 0.6979\n",
      "Epoch 623/2000\n",
      "14152/14152 [==============================] - 5s 355us/step - loss: 0.8258 - accuracy: 0.6769 - val_loss: 0.8072 - val_accuracy: 0.6987\n",
      "Epoch 624/2000\n",
      "14152/14152 [==============================] - 5s 342us/step - loss: 0.8278 - accuracy: 0.6793 - val_loss: 0.8045 - val_accuracy: 0.6987\n",
      "Epoch 625/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.8215 - accuracy: 0.6779 - val_loss: 0.8043 - val_accuracy: 0.6979\n",
      "Epoch 626/2000\n",
      "14152/14152 [==============================] - 5s 331us/step - loss: 0.8243 - accuracy: 0.6776 - val_loss: 0.8027 - val_accuracy: 0.6995\n",
      "Epoch 627/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.8266 - accuracy: 0.6776 - val_loss: 0.8064 - val_accuracy: 0.6987\n",
      "Epoch 628/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 0.8271 - accuracy: 0.6736 - val_loss: 0.8010 - val_accuracy: 0.6987\n",
      "Epoch 629/2000\n",
      "14152/14152 [==============================] - 5s 342us/step - loss: 0.8213 - accuracy: 0.6777 - val_loss: 0.8008 - val_accuracy: 0.6987\n",
      "Epoch 630/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.8221 - accuracy: 0.6771 - val_loss: 0.7961 - val_accuracy: 0.7018\n",
      "Epoch 631/2000\n",
      "14152/14152 [==============================] - 4s 318us/step - loss: 0.8165 - accuracy: 0.6799 - val_loss: 0.8035 - val_accuracy: 0.6995\n",
      "Epoch 632/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.8231 - accuracy: 0.6779 - val_loss: 0.7969 - val_accuracy: 0.7015\n",
      "Epoch 633/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.8203 - accuracy: 0.6777 - val_loss: 0.8003 - val_accuracy: 0.6987\n",
      "Epoch 634/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 0.8143 - accuracy: 0.6802 - val_loss: 0.8024 - val_accuracy: 0.6984\n",
      "Epoch 635/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.8176 - accuracy: 0.6817 - val_loss: 0.8015 - val_accuracy: 0.6981\n",
      "Epoch 636/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.8218 - accuracy: 0.6781 - val_loss: 0.8058 - val_accuracy: 0.6973\n",
      "Epoch 637/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.8239 - accuracy: 0.6762 - val_loss: 0.7964 - val_accuracy: 0.7007\n",
      "Epoch 638/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.8252 - accuracy: 0.6772 - val_loss: 0.7973 - val_accuracy: 0.7007\n",
      "Epoch 639/2000\n",
      "14152/14152 [==============================] - 5s 339us/step - loss: 0.8179 - accuracy: 0.6794 - val_loss: 0.8068 - val_accuracy: 0.6953\n",
      "Epoch 640/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.8203 - accuracy: 0.6786 - val_loss: 0.7975 - val_accuracy: 0.7012\n",
      "Epoch 641/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 0.8141 - accuracy: 0.6805 - val_loss: 0.8004 - val_accuracy: 0.6981\n",
      "Epoch 642/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 0.8232 - accuracy: 0.6779 - val_loss: 0.8023 - val_accuracy: 0.6976\n",
      "Epoch 643/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 0.8162 - accuracy: 0.6820 - val_loss: 0.7983 - val_accuracy: 0.6987\n",
      "Epoch 644/2000\n",
      "14152/14152 [==============================] - 5s 328us/step - loss: 0.8146 - accuracy: 0.6793 - val_loss: 0.7977 - val_accuracy: 0.7015\n",
      "Epoch 645/2000\n",
      "14152/14152 [==============================] - 5s 331us/step - loss: 0.8167 - accuracy: 0.6801 - val_loss: 0.7951 - val_accuracy: 0.7015\n",
      "Epoch 646/2000\n",
      "14152/14152 [==============================] - 5s 364us/step - loss: 0.8170 - accuracy: 0.6808 - val_loss: 0.7925 - val_accuracy: 0.7007\n",
      "Epoch 647/2000\n",
      "14152/14152 [==============================] - 5s 355us/step - loss: 0.8141 - accuracy: 0.6789 - val_loss: 0.7998 - val_accuracy: 0.7001\n",
      "Epoch 648/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 0.8215 - accuracy: 0.6783 - val_loss: 0.7910 - val_accuracy: 0.7015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 649/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 0.8115 - accuracy: 0.6791 - val_loss: 0.7948 - val_accuracy: 0.7010\n",
      "Epoch 650/2000\n",
      "14152/14152 [==============================] - 5s 341us/step - loss: 0.8174 - accuracy: 0.6803 - val_loss: 0.7948 - val_accuracy: 0.7010\n",
      "Epoch 651/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.8152 - accuracy: 0.6800 - val_loss: 0.8004 - val_accuracy: 0.6995\n",
      "Epoch 652/2000\n",
      "14152/14152 [==============================] - 5s 346us/step - loss: 0.8151 - accuracy: 0.6787 - val_loss: 0.7962 - val_accuracy: 0.6998\n",
      "Epoch 653/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 0.8162 - accuracy: 0.6767 - val_loss: 0.7929 - val_accuracy: 0.7007\n",
      "Epoch 654/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.8188 - accuracy: 0.6794 - val_loss: 0.8053 - val_accuracy: 0.6945\n",
      "Epoch 655/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.8084 - accuracy: 0.6826 - val_loss: 0.7887 - val_accuracy: 0.7021\n",
      "Epoch 656/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.8206 - accuracy: 0.6773 - val_loss: 0.7924 - val_accuracy: 0.6995\n",
      "Epoch 657/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.8063 - accuracy: 0.6828 - val_loss: 0.7915 - val_accuracy: 0.7015\n",
      "Epoch 658/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 0.8100 - accuracy: 0.6821 - val_loss: 0.7996 - val_accuracy: 0.6995\n",
      "Epoch 659/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.8094 - accuracy: 0.6819 - val_loss: 0.7960 - val_accuracy: 0.6993\n",
      "Epoch 660/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.8133 - accuracy: 0.6819 - val_loss: 0.7899 - val_accuracy: 0.7015\n",
      "Epoch 661/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 0.8120 - accuracy: 0.6816 - val_loss: 0.7919 - val_accuracy: 0.7024\n",
      "Epoch 662/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.8111 - accuracy: 0.6825 - val_loss: 0.7978 - val_accuracy: 0.7010\n",
      "Epoch 663/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.8177 - accuracy: 0.6781 - val_loss: 0.7924 - val_accuracy: 0.7004\n",
      "Epoch 664/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 0.8104 - accuracy: 0.6843 - val_loss: 0.7959 - val_accuracy: 0.7015\n",
      "Epoch 665/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 0.8140 - accuracy: 0.6800 - val_loss: 0.7919 - val_accuracy: 0.7024\n",
      "Epoch 666/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.8130 - accuracy: 0.6791 - val_loss: 0.7932 - val_accuracy: 0.7004\n",
      "Epoch 667/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 0.8045 - accuracy: 0.6841 - val_loss: 0.7876 - val_accuracy: 0.7015\n",
      "Epoch 668/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 0.8070 - accuracy: 0.6846 - val_loss: 0.7895 - val_accuracy: 0.7007\n",
      "Epoch 669/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.8048 - accuracy: 0.6830 - val_loss: 0.7902 - val_accuracy: 0.7027\n",
      "Epoch 670/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.8024 - accuracy: 0.6845 - val_loss: 0.7936 - val_accuracy: 0.7007\n",
      "Epoch 671/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.8043 - accuracy: 0.6851 - val_loss: 0.7911 - val_accuracy: 0.7027\n",
      "Epoch 672/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.8052 - accuracy: 0.6846 - val_loss: 0.7829 - val_accuracy: 0.7032\n",
      "Epoch 673/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.8092 - accuracy: 0.6828 - val_loss: 0.7846 - val_accuracy: 0.7049\n",
      "Epoch 674/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.8002 - accuracy: 0.6861 - val_loss: 0.7815 - val_accuracy: 0.7029\n",
      "Epoch 675/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.8119 - accuracy: 0.6813 - val_loss: 0.7920 - val_accuracy: 0.7021\n",
      "Epoch 676/2000\n",
      "14152/14152 [==============================] - 5s 335us/step - loss: 0.8057 - accuracy: 0.6820 - val_loss: 0.7886 - val_accuracy: 0.7024\n",
      "Epoch 677/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 0.8065 - accuracy: 0.6827 - val_loss: 0.7952 - val_accuracy: 0.6990\n",
      "Epoch 678/2000\n",
      "14152/14152 [==============================] - 5s 335us/step - loss: 0.8125 - accuracy: 0.6846 - val_loss: 0.7801 - val_accuracy: 0.7069\n",
      "Epoch 679/2000\n",
      "14152/14152 [==============================] - 5s 345us/step - loss: 0.8041 - accuracy: 0.6822 - val_loss: 0.7858 - val_accuracy: 0.7018\n",
      "Epoch 680/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.7987 - accuracy: 0.6873 - val_loss: 0.7856 - val_accuracy: 0.7038\n",
      "Epoch 681/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7998 - accuracy: 0.6865 - val_loss: 0.7946 - val_accuracy: 0.7012\n",
      "Epoch 682/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.7944 - accuracy: 0.6868 - val_loss: 0.7876 - val_accuracy: 0.7024\n",
      "Epoch 683/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 0.8015 - accuracy: 0.6859 - val_loss: 0.7758 - val_accuracy: 0.7052\n",
      "Epoch 684/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 0.8015 - accuracy: 0.6832 - val_loss: 0.7851 - val_accuracy: 0.7032\n",
      "Epoch 685/2000\n",
      "14152/14152 [==============================] - 6s 390us/step - loss: 0.8036 - accuracy: 0.6852 - val_loss: 0.7806 - val_accuracy: 0.7055\n",
      "Epoch 686/2000\n",
      "14152/14152 [==============================] - 6s 422us/step - loss: 0.8001 - accuracy: 0.6854 - val_loss: 0.7822 - val_accuracy: 0.7052\n",
      "Epoch 687/2000\n",
      "14152/14152 [==============================] - 5s 367us/step - loss: 0.8000 - accuracy: 0.6836 - val_loss: 0.7845 - val_accuracy: 0.7049\n",
      "Epoch 688/2000\n",
      "14152/14152 [==============================] - 5s 366us/step - loss: 0.7943 - accuracy: 0.6906 - val_loss: 0.7792 - val_accuracy: 0.7060\n",
      "Epoch 689/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 0.7930 - accuracy: 0.6853 - val_loss: 0.7719 - val_accuracy: 0.7075\n",
      "Epoch 690/2000\n",
      "14152/14152 [==============================] - 5s 356us/step - loss: 0.7993 - accuracy: 0.6861 - val_loss: 0.7782 - val_accuracy: 0.7055\n",
      "Epoch 691/2000\n",
      "14152/14152 [==============================] - 5s 335us/step - loss: 0.7967 - accuracy: 0.6868 - val_loss: 0.7865 - val_accuracy: 0.7012\n",
      "Epoch 692/2000\n",
      "14152/14152 [==============================] - 5s 365us/step - loss: 0.8044 - accuracy: 0.6821 - val_loss: 0.7792 - val_accuracy: 0.7055\n",
      "Epoch 693/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 0.8019 - accuracy: 0.6839 - val_loss: 0.7814 - val_accuracy: 0.7049\n",
      "Epoch 694/2000\n",
      "14152/14152 [==============================] - 5s 354us/step - loss: 0.7998 - accuracy: 0.6849 - val_loss: 0.7845 - val_accuracy: 0.7041\n",
      "Epoch 695/2000\n",
      "14152/14152 [==============================] - 5s 339us/step - loss: 0.7993 - accuracy: 0.6878 - val_loss: 0.7869 - val_accuracy: 0.7032\n",
      "Epoch 696/2000\n",
      "14152/14152 [==============================] - 5s 376us/step - loss: 0.7991 - accuracy: 0.6843 - val_loss: 0.7793 - val_accuracy: 0.7072\n",
      "Epoch 697/2000\n",
      "14152/14152 [==============================] - 6s 406us/step - loss: 0.7971 - accuracy: 0.6856 - val_loss: 0.7823 - val_accuracy: 0.7032\n",
      "Epoch 698/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 0.8064 - accuracy: 0.6858 - val_loss: 0.7755 - val_accuracy: 0.7060\n",
      "Epoch 699/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 0.7951 - accuracy: 0.6850 - val_loss: 0.7825 - val_accuracy: 0.7044\n",
      "Epoch 700/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 0.7990 - accuracy: 0.6854 - val_loss: 0.7756 - val_accuracy: 0.7111\n",
      "Epoch 701/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 0.7983 - accuracy: 0.6870 - val_loss: 0.7791 - val_accuracy: 0.7035\n",
      "Epoch 702/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 0.7990 - accuracy: 0.6865 - val_loss: 0.7787 - val_accuracy: 0.7063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 703/2000\n",
      "14152/14152 [==============================] - 5s 349us/step - loss: 0.7987 - accuracy: 0.6870 - val_loss: 0.7863 - val_accuracy: 0.7044\n",
      "Epoch 704/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 0.7976 - accuracy: 0.6860 - val_loss: 0.7795 - val_accuracy: 0.7058\n",
      "Epoch 705/2000\n",
      "14152/14152 [==============================] - 5s 339us/step - loss: 0.7968 - accuracy: 0.6856 - val_loss: 0.7773 - val_accuracy: 0.7080\n",
      "Epoch 706/2000\n",
      "14152/14152 [==============================] - 5s 377us/step - loss: 0.8017 - accuracy: 0.6859 - val_loss: 0.7737 - val_accuracy: 0.7086\n",
      "Epoch 707/2000\n",
      "14152/14152 [==============================] - 5s 354us/step - loss: 0.7910 - accuracy: 0.6886 - val_loss: 0.7801 - val_accuracy: 0.7097\n",
      "Epoch 708/2000\n",
      "14152/14152 [==============================] - 6s 400us/step - loss: 0.7910 - accuracy: 0.6873 - val_loss: 0.7780 - val_accuracy: 0.7055\n",
      "Epoch 709/2000\n",
      "14152/14152 [==============================] - 5s 330us/step - loss: 0.7955 - accuracy: 0.6889 - val_loss: 0.7735 - val_accuracy: 0.7069\n",
      "Epoch 710/2000\n",
      "14152/14152 [==============================] - 5s 318us/step - loss: 0.7951 - accuracy: 0.6873 - val_loss: 0.7742 - val_accuracy: 0.7086\n",
      "Epoch 711/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.7920 - accuracy: 0.6870 - val_loss: 0.7703 - val_accuracy: 0.7080\n",
      "Epoch 712/2000\n",
      "14152/14152 [==============================] - 5s 346us/step - loss: 0.7902 - accuracy: 0.6892 - val_loss: 0.7769 - val_accuracy: 0.7089\n",
      "Epoch 713/2000\n",
      "14152/14152 [==============================] - 5s 358us/step - loss: 0.7893 - accuracy: 0.6880 - val_loss: 0.7760 - val_accuracy: 0.7075\n",
      "Epoch 714/2000\n",
      "14152/14152 [==============================] - 5s 353us/step - loss: 0.7888 - accuracy: 0.6907 - val_loss: 0.7783 - val_accuracy: 0.7058\n",
      "Epoch 715/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.7909 - accuracy: 0.6874 - val_loss: 0.7770 - val_accuracy: 0.7063\n",
      "Epoch 716/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.7935 - accuracy: 0.6885 - val_loss: 0.7769 - val_accuracy: 0.7052\n",
      "Epoch 717/2000\n",
      "14152/14152 [==============================] - 5s 329us/step - loss: 0.7955 - accuracy: 0.6861 - val_loss: 0.7719 - val_accuracy: 0.7089\n",
      "Epoch 718/2000\n",
      "14152/14152 [==============================] - 5s 350us/step - loss: 0.7937 - accuracy: 0.6849 - val_loss: 0.7761 - val_accuracy: 0.7077\n",
      "Epoch 719/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 0.7844 - accuracy: 0.6904 - val_loss: 0.7689 - val_accuracy: 0.7123\n",
      "Epoch 720/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 0.7868 - accuracy: 0.6904 - val_loss: 0.7756 - val_accuracy: 0.7080\n",
      "Epoch 721/2000\n",
      "14152/14152 [==============================] - 5s 348us/step - loss: 0.7903 - accuracy: 0.6883 - val_loss: 0.7710 - val_accuracy: 0.7092\n",
      "Epoch 722/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.7830 - accuracy: 0.6898 - val_loss: 0.7736 - val_accuracy: 0.7077\n",
      "Epoch 723/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 0.7888 - accuracy: 0.6909 - val_loss: 0.7711 - val_accuracy: 0.7066\n",
      "Epoch 724/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 0.7909 - accuracy: 0.6895 - val_loss: 0.7777 - val_accuracy: 0.7058\n",
      "Epoch 725/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.7864 - accuracy: 0.6899 - val_loss: 0.7760 - val_accuracy: 0.7058\n",
      "Epoch 726/2000\n",
      "14152/14152 [==============================] - 5s 341us/step - loss: 0.7912 - accuracy: 0.6871 - val_loss: 0.7660 - val_accuracy: 0.7123\n",
      "Epoch 727/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 0.7899 - accuracy: 0.6880 - val_loss: 0.7711 - val_accuracy: 0.7077\n",
      "Epoch 728/2000\n",
      "14152/14152 [==============================] - 4s 318us/step - loss: 0.7842 - accuracy: 0.6929 - val_loss: 0.7678 - val_accuracy: 0.7092\n",
      "Epoch 729/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.7877 - accuracy: 0.6870 - val_loss: 0.7778 - val_accuracy: 0.7052\n",
      "Epoch 730/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 0.7865 - accuracy: 0.6888 - val_loss: 0.7594 - val_accuracy: 0.7140\n",
      "Epoch 731/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 0.7916 - accuracy: 0.6873 - val_loss: 0.7686 - val_accuracy: 0.7086\n",
      "Epoch 732/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 0.7832 - accuracy: 0.6891 - val_loss: 0.7734 - val_accuracy: 0.7100\n",
      "Epoch 733/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 0.7798 - accuracy: 0.6919 - val_loss: 0.7682 - val_accuracy: 0.7117\n",
      "Epoch 734/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 0.7848 - accuracy: 0.6915 - val_loss: 0.7654 - val_accuracy: 0.7114\n",
      "Epoch 735/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7865 - accuracy: 0.6907 - val_loss: 0.7728 - val_accuracy: 0.7083\n",
      "Epoch 736/2000\n",
      "14152/14152 [==============================] - 5s 339us/step - loss: 0.7843 - accuracy: 0.6918 - val_loss: 0.7641 - val_accuracy: 0.7117\n",
      "Epoch 737/2000\n",
      "14152/14152 [==============================] - 5s 342us/step - loss: 0.7871 - accuracy: 0.6903 - val_loss: 0.7673 - val_accuracy: 0.7117\n",
      "Epoch 738/2000\n",
      "14152/14152 [==============================] - 5s 361us/step - loss: 0.7800 - accuracy: 0.6899 - val_loss: 0.7665 - val_accuracy: 0.7075\n",
      "Epoch 739/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 0.7779 - accuracy: 0.6915 - val_loss: 0.7690 - val_accuracy: 0.7089\n",
      "Epoch 740/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.7854 - accuracy: 0.6889 - val_loss: 0.7608 - val_accuracy: 0.7137\n",
      "Epoch 741/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 0.7917 - accuracy: 0.6877 - val_loss: 0.7687 - val_accuracy: 0.7083\n",
      "Epoch 742/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 0.7798 - accuracy: 0.6914 - val_loss: 0.7665 - val_accuracy: 0.7120\n",
      "Epoch 743/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.7795 - accuracy: 0.6921 - val_loss: 0.7661 - val_accuracy: 0.7103\n",
      "Epoch 744/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.7808 - accuracy: 0.6916 - val_loss: 0.7627 - val_accuracy: 0.7120\n",
      "Epoch 745/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7788 - accuracy: 0.6894 - val_loss: 0.7596 - val_accuracy: 0.7134\n",
      "Epoch 746/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7828 - accuracy: 0.6910 - val_loss: 0.7619 - val_accuracy: 0.7114\n",
      "Epoch 747/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.7843 - accuracy: 0.6889 - val_loss: 0.7702 - val_accuracy: 0.7080\n",
      "Epoch 748/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.7812 - accuracy: 0.6938 - val_loss: 0.7731 - val_accuracy: 0.7072\n",
      "Epoch 749/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7789 - accuracy: 0.6918 - val_loss: 0.7634 - val_accuracy: 0.7117\n",
      "Epoch 750/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7829 - accuracy: 0.6906 - val_loss: 0.7610 - val_accuracy: 0.7086\n",
      "Epoch 751/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7795 - accuracy: 0.6898 - val_loss: 0.7559 - val_accuracy: 0.7137\n",
      "Epoch 752/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.7826 - accuracy: 0.6920 - val_loss: 0.7691 - val_accuracy: 0.7111\n",
      "Epoch 753/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.7751 - accuracy: 0.6933 - val_loss: 0.7608 - val_accuracy: 0.7134\n",
      "Epoch 754/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.7843 - accuracy: 0.6916 - val_loss: 0.7596 - val_accuracy: 0.7137\n",
      "Epoch 755/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7771 - accuracy: 0.6928 - val_loss: 0.7617 - val_accuracy: 0.7117\n",
      "Epoch 756/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7793 - accuracy: 0.6911 - val_loss: 0.7571 - val_accuracy: 0.7111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 757/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7795 - accuracy: 0.6906 - val_loss: 0.7612 - val_accuracy: 0.7120\n",
      "Epoch 758/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7814 - accuracy: 0.6901 - val_loss: 0.7553 - val_accuracy: 0.7114\n",
      "Epoch 759/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7748 - accuracy: 0.6943 - val_loss: 0.7563 - val_accuracy: 0.7134\n",
      "Epoch 760/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7807 - accuracy: 0.6912 - val_loss: 0.7602 - val_accuracy: 0.7142\n",
      "Epoch 761/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7747 - accuracy: 0.6926 - val_loss: 0.7561 - val_accuracy: 0.7157\n",
      "Epoch 762/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7746 - accuracy: 0.6915 - val_loss: 0.7604 - val_accuracy: 0.7109\n",
      "Epoch 763/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7821 - accuracy: 0.6897 - val_loss: 0.7564 - val_accuracy: 0.7114\n",
      "Epoch 764/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7786 - accuracy: 0.6899 - val_loss: 0.7639 - val_accuracy: 0.7120\n",
      "Epoch 765/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7738 - accuracy: 0.6947 - val_loss: 0.7585 - val_accuracy: 0.7109\n",
      "Epoch 766/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7781 - accuracy: 0.6928 - val_loss: 0.7586 - val_accuracy: 0.7128\n",
      "Epoch 767/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.7763 - accuracy: 0.6923 - val_loss: 0.7562 - val_accuracy: 0.7131\n",
      "Epoch 768/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7768 - accuracy: 0.6923 - val_loss: 0.7554 - val_accuracy: 0.7125\n",
      "Epoch 769/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7748 - accuracy: 0.6928 - val_loss: 0.7550 - val_accuracy: 0.7148\n",
      "Epoch 770/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7824 - accuracy: 0.6903 - val_loss: 0.7551 - val_accuracy: 0.7128\n",
      "Epoch 771/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7741 - accuracy: 0.6947 - val_loss: 0.7551 - val_accuracy: 0.7117\n",
      "Epoch 772/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7756 - accuracy: 0.6927 - val_loss: 0.7617 - val_accuracy: 0.7145\n",
      "Epoch 773/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.7792 - accuracy: 0.6940 - val_loss: 0.7548 - val_accuracy: 0.7148\n",
      "Epoch 774/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7753 - accuracy: 0.6927 - val_loss: 0.7513 - val_accuracy: 0.7142\n",
      "Epoch 775/2000\n",
      "14152/14152 [==============================] - 4s 318us/step - loss: 0.7745 - accuracy: 0.6954 - val_loss: 0.7639 - val_accuracy: 0.7092\n",
      "Epoch 776/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7654 - accuracy: 0.6958 - val_loss: 0.7579 - val_accuracy: 0.7106\n",
      "Epoch 777/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7792 - accuracy: 0.6941 - val_loss: 0.7553 - val_accuracy: 0.7117\n",
      "Epoch 778/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7701 - accuracy: 0.6974 - val_loss: 0.7539 - val_accuracy: 0.7142\n",
      "Epoch 779/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7732 - accuracy: 0.6954 - val_loss: 0.7549 - val_accuracy: 0.7123\n",
      "Epoch 780/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.7706 - accuracy: 0.6967 - val_loss: 0.7557 - val_accuracy: 0.7134\n",
      "Epoch 781/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7733 - accuracy: 0.6939 - val_loss: 0.7611 - val_accuracy: 0.7131\n",
      "Epoch 782/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7665 - accuracy: 0.6938 - val_loss: 0.7555 - val_accuracy: 0.7154\n",
      "Epoch 783/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7690 - accuracy: 0.6971 - val_loss: 0.7507 - val_accuracy: 0.7145\n",
      "Epoch 784/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7673 - accuracy: 0.6963 - val_loss: 0.7492 - val_accuracy: 0.7145\n",
      "Epoch 785/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7726 - accuracy: 0.6954 - val_loss: 0.7598 - val_accuracy: 0.7140\n",
      "Epoch 786/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7672 - accuracy: 0.6948 - val_loss: 0.7523 - val_accuracy: 0.7142\n",
      "Epoch 787/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7764 - accuracy: 0.6905 - val_loss: 0.7516 - val_accuracy: 0.7145\n",
      "Epoch 788/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.7683 - accuracy: 0.6955 - val_loss: 0.7484 - val_accuracy: 0.7159\n",
      "Epoch 789/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.7631 - accuracy: 0.6963 - val_loss: 0.7461 - val_accuracy: 0.7165\n",
      "Epoch 790/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7620 - accuracy: 0.6988 - val_loss: 0.7542 - val_accuracy: 0.7125\n",
      "Epoch 791/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7644 - accuracy: 0.6953 - val_loss: 0.7506 - val_accuracy: 0.7151\n",
      "Epoch 792/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.7637 - accuracy: 0.6948 - val_loss: 0.7538 - val_accuracy: 0.7134\n",
      "Epoch 793/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7620 - accuracy: 0.6992 - val_loss: 0.7561 - val_accuracy: 0.7123\n",
      "Epoch 794/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.7699 - accuracy: 0.6939 - val_loss: 0.7523 - val_accuracy: 0.7154\n",
      "Epoch 795/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7662 - accuracy: 0.6947 - val_loss: 0.7515 - val_accuracy: 0.7151\n",
      "Epoch 796/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7623 - accuracy: 0.6991 - val_loss: 0.7485 - val_accuracy: 0.7162\n",
      "Epoch 797/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7672 - accuracy: 0.6947 - val_loss: 0.7481 - val_accuracy: 0.7154\n",
      "Epoch 798/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7650 - accuracy: 0.6969 - val_loss: 0.7499 - val_accuracy: 0.7151\n",
      "Epoch 799/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7689 - accuracy: 0.6950 - val_loss: 0.7513 - val_accuracy: 0.7137\n",
      "Epoch 800/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7601 - accuracy: 0.6987 - val_loss: 0.7536 - val_accuracy: 0.7123\n",
      "Epoch 801/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7604 - accuracy: 0.6986 - val_loss: 0.7578 - val_accuracy: 0.7120\n",
      "Epoch 802/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7615 - accuracy: 0.6986 - val_loss: 0.7497 - val_accuracy: 0.7140\n",
      "Epoch 803/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7673 - accuracy: 0.6957 - val_loss: 0.7437 - val_accuracy: 0.7162\n",
      "Epoch 804/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7610 - accuracy: 0.6976 - val_loss: 0.7492 - val_accuracy: 0.7140\n",
      "Epoch 805/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.7600 - accuracy: 0.6993 - val_loss: 0.7446 - val_accuracy: 0.7179\n",
      "Epoch 806/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7622 - accuracy: 0.6963 - val_loss: 0.7500 - val_accuracy: 0.7148\n",
      "Epoch 807/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.7603 - accuracy: 0.6952 - val_loss: 0.7616 - val_accuracy: 0.7134\n",
      "Epoch 808/2000\n",
      "14152/14152 [==============================] - 5s 330us/step - loss: 0.7590 - accuracy: 0.6999 - val_loss: 0.7413 - val_accuracy: 0.7176\n",
      "Epoch 809/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7623 - accuracy: 0.6957 - val_loss: 0.7482 - val_accuracy: 0.7157\n",
      "Epoch 810/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7724 - accuracy: 0.6961 - val_loss: 0.7478 - val_accuracy: 0.7148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 811/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7553 - accuracy: 0.7003 - val_loss: 0.7489 - val_accuracy: 0.7148\n",
      "Epoch 812/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7586 - accuracy: 0.6994 - val_loss: 0.7452 - val_accuracy: 0.7140\n",
      "Epoch 813/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7615 - accuracy: 0.6972 - val_loss: 0.7441 - val_accuracy: 0.7165\n",
      "Epoch 814/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7571 - accuracy: 0.6983 - val_loss: 0.7425 - val_accuracy: 0.7210\n",
      "Epoch 815/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7614 - accuracy: 0.6988 - val_loss: 0.7439 - val_accuracy: 0.7162\n",
      "Epoch 816/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7542 - accuracy: 0.6994 - val_loss: 0.7364 - val_accuracy: 0.7193\n",
      "Epoch 817/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7698 - accuracy: 0.6932 - val_loss: 0.7460 - val_accuracy: 0.7151\n",
      "Epoch 818/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7623 - accuracy: 0.6977 - val_loss: 0.7408 - val_accuracy: 0.7182\n",
      "Epoch 819/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7591 - accuracy: 0.6956 - val_loss: 0.7493 - val_accuracy: 0.7123\n",
      "Epoch 820/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7604 - accuracy: 0.6972 - val_loss: 0.7471 - val_accuracy: 0.7140\n",
      "Epoch 821/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7654 - accuracy: 0.7003 - val_loss: 0.7416 - val_accuracy: 0.7168\n",
      "Epoch 822/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7545 - accuracy: 0.7001 - val_loss: 0.7373 - val_accuracy: 0.7191\n",
      "Epoch 823/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7606 - accuracy: 0.6972 - val_loss: 0.7446 - val_accuracy: 0.7165\n",
      "Epoch 824/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7555 - accuracy: 0.6994 - val_loss: 0.7409 - val_accuracy: 0.7171\n",
      "Epoch 825/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7574 - accuracy: 0.6980 - val_loss: 0.7343 - val_accuracy: 0.7179\n",
      "Epoch 826/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7532 - accuracy: 0.6967 - val_loss: 0.7449 - val_accuracy: 0.7165\n",
      "Epoch 827/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7623 - accuracy: 0.6950 - val_loss: 0.7418 - val_accuracy: 0.7134\n",
      "Epoch 828/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7522 - accuracy: 0.6995 - val_loss: 0.7350 - val_accuracy: 0.7199\n",
      "Epoch 829/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7580 - accuracy: 0.6954 - val_loss: 0.7460 - val_accuracy: 0.7179\n",
      "Epoch 830/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.7550 - accuracy: 0.6988 - val_loss: 0.7377 - val_accuracy: 0.7193\n",
      "Epoch 831/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7528 - accuracy: 0.6998 - val_loss: 0.7414 - val_accuracy: 0.7182\n",
      "Epoch 832/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.7554 - accuracy: 0.6981 - val_loss: 0.7482 - val_accuracy: 0.7159\n",
      "Epoch 833/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7533 - accuracy: 0.6996 - val_loss: 0.7386 - val_accuracy: 0.7207\n",
      "Epoch 834/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7528 - accuracy: 0.6979 - val_loss: 0.7421 - val_accuracy: 0.7182\n",
      "Epoch 835/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.7576 - accuracy: 0.6990 - val_loss: 0.7417 - val_accuracy: 0.7148\n",
      "Epoch 836/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7534 - accuracy: 0.7007 - val_loss: 0.7345 - val_accuracy: 0.7191\n",
      "Epoch 837/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7519 - accuracy: 0.7007 - val_loss: 0.7366 - val_accuracy: 0.7196\n",
      "Epoch 838/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7493 - accuracy: 0.7014 - val_loss: 0.7362 - val_accuracy: 0.7176\n",
      "Epoch 839/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7572 - accuracy: 0.6995 - val_loss: 0.7337 - val_accuracy: 0.7213\n",
      "Epoch 840/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.7578 - accuracy: 0.6981 - val_loss: 0.7383 - val_accuracy: 0.7176\n",
      "Epoch 841/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7503 - accuracy: 0.7000 - val_loss: 0.7328 - val_accuracy: 0.7236\n",
      "Epoch 842/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.7500 - accuracy: 0.7022 - val_loss: 0.7384 - val_accuracy: 0.7174\n",
      "Epoch 843/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.7527 - accuracy: 0.7010 - val_loss: 0.7379 - val_accuracy: 0.7159\n",
      "Epoch 844/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7540 - accuracy: 0.6982 - val_loss: 0.7338 - val_accuracy: 0.7185\n",
      "Epoch 845/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7563 - accuracy: 0.7012 - val_loss: 0.7332 - val_accuracy: 0.7196\n",
      "Epoch 846/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7455 - accuracy: 0.7012 - val_loss: 0.7323 - val_accuracy: 0.7193\n",
      "Epoch 847/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7512 - accuracy: 0.7010 - val_loss: 0.7355 - val_accuracy: 0.7193\n",
      "Epoch 848/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7447 - accuracy: 0.6995 - val_loss: 0.7419 - val_accuracy: 0.7176\n",
      "Epoch 849/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7497 - accuracy: 0.7008 - val_loss: 0.7334 - val_accuracy: 0.7219\n",
      "Epoch 850/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7516 - accuracy: 0.6979 - val_loss: 0.7421 - val_accuracy: 0.7128\n",
      "Epoch 851/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7537 - accuracy: 0.6996 - val_loss: 0.7326 - val_accuracy: 0.7227\n",
      "Epoch 852/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7492 - accuracy: 0.7000 - val_loss: 0.7353 - val_accuracy: 0.7182\n",
      "Epoch 853/2000\n",
      "14152/14152 [==============================] - 4s 305us/step - loss: 0.7528 - accuracy: 0.6992 - val_loss: 0.7406 - val_accuracy: 0.7151\n",
      "Epoch 854/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7527 - accuracy: 0.7009 - val_loss: 0.7346 - val_accuracy: 0.7193\n",
      "Epoch 855/2000\n",
      "14152/14152 [==============================] - 4s 305us/step - loss: 0.7464 - accuracy: 0.7010 - val_loss: 0.7307 - val_accuracy: 0.7185\n",
      "Epoch 856/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.7536 - accuracy: 0.7003 - val_loss: 0.7385 - val_accuracy: 0.7151\n",
      "Epoch 857/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7483 - accuracy: 0.7014 - val_loss: 0.7367 - val_accuracy: 0.7193\n",
      "Epoch 858/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7469 - accuracy: 0.7012 - val_loss: 0.7309 - val_accuracy: 0.7216\n",
      "Epoch 859/2000\n",
      "14152/14152 [==============================] - 4s 305us/step - loss: 0.7571 - accuracy: 0.6973 - val_loss: 0.7333 - val_accuracy: 0.7202\n",
      "Epoch 860/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 0.7484 - accuracy: 0.7022 - val_loss: 0.7400 - val_accuracy: 0.7179\n",
      "Epoch 861/2000\n",
      "14152/14152 [==============================] - 5s 341us/step - loss: 0.7467 - accuracy: 0.7023 - val_loss: 0.7290 - val_accuracy: 0.7250\n",
      "Epoch 862/2000\n",
      "14152/14152 [==============================] - 5s 344us/step - loss: 0.7449 - accuracy: 0.7010 - val_loss: 0.7334 - val_accuracy: 0.7199\n",
      "Epoch 863/2000\n",
      "14152/14152 [==============================] - 6s 442us/step - loss: 0.7500 - accuracy: 0.6994 - val_loss: 0.7262 - val_accuracy: 0.7205\n",
      "Epoch 864/2000\n",
      "14152/14152 [==============================] - 6s 391us/step - loss: 0.7548 - accuracy: 0.6996 - val_loss: 0.7307 - val_accuracy: 0.7182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 865/2000\n",
      "14152/14152 [==============================] - 5s 352us/step - loss: 0.7439 - accuracy: 0.7039 - val_loss: 0.7312 - val_accuracy: 0.7182\n",
      "Epoch 866/2000\n",
      "14152/14152 [==============================] - 5s 354us/step - loss: 0.7448 - accuracy: 0.7049 - val_loss: 0.7270 - val_accuracy: 0.7213\n",
      "Epoch 867/2000\n",
      "14152/14152 [==============================] - 5s 359us/step - loss: 0.7404 - accuracy: 0.7000 - val_loss: 0.7328 - val_accuracy: 0.7159\n",
      "Epoch 868/2000\n",
      "14152/14152 [==============================] - 5s 366us/step - loss: 0.7479 - accuracy: 0.7047 - val_loss: 0.7297 - val_accuracy: 0.7222\n",
      "Epoch 869/2000\n",
      "14152/14152 [==============================] - 5s 350us/step - loss: 0.7475 - accuracy: 0.7029 - val_loss: 0.7296 - val_accuracy: 0.7196\n",
      "Epoch 870/2000\n",
      "14152/14152 [==============================] - 5s 343us/step - loss: 0.7515 - accuracy: 0.6974 - val_loss: 0.7296 - val_accuracy: 0.7168\n",
      "Epoch 871/2000\n",
      "14152/14152 [==============================] - 5s 343us/step - loss: 0.7511 - accuracy: 0.7009 - val_loss: 0.7205 - val_accuracy: 0.7236\n",
      "Epoch 872/2000\n",
      "14152/14152 [==============================] - 5s 345us/step - loss: 0.7486 - accuracy: 0.6988 - val_loss: 0.7298 - val_accuracy: 0.7162\n",
      "Epoch 873/2000\n",
      "14152/14152 [==============================] - 5s 370us/step - loss: 0.7455 - accuracy: 0.7020 - val_loss: 0.7318 - val_accuracy: 0.7179\n",
      "Epoch 874/2000\n",
      "14152/14152 [==============================] - 5s 360us/step - loss: 0.7428 - accuracy: 0.7053 - val_loss: 0.7262 - val_accuracy: 0.7182\n",
      "Epoch 875/2000\n",
      "14152/14152 [==============================] - 5s 318us/step - loss: 0.7493 - accuracy: 0.6970 - val_loss: 0.7284 - val_accuracy: 0.7210\n",
      "Epoch 876/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 0.7482 - accuracy: 0.7021 - val_loss: 0.7321 - val_accuracy: 0.7213\n",
      "Epoch 877/2000\n",
      "14152/14152 [==============================] - 4s 318us/step - loss: 0.7456 - accuracy: 0.7020 - val_loss: 0.7313 - val_accuracy: 0.7159\n",
      "Epoch 878/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.7469 - accuracy: 0.7004 - val_loss: 0.7232 - val_accuracy: 0.7202\n",
      "Epoch 879/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7452 - accuracy: 0.6992 - val_loss: 0.7283 - val_accuracy: 0.7188\n",
      "Epoch 880/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.7411 - accuracy: 0.7014 - val_loss: 0.7163 - val_accuracy: 0.7256\n",
      "Epoch 881/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 0.7397 - accuracy: 0.7029 - val_loss: 0.7271 - val_accuracy: 0.7196\n",
      "Epoch 882/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 0.7383 - accuracy: 0.7031 - val_loss: 0.7277 - val_accuracy: 0.7191\n",
      "Epoch 883/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 0.7403 - accuracy: 0.7031 - val_loss: 0.7259 - val_accuracy: 0.7199\n",
      "Epoch 884/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 0.7457 - accuracy: 0.7026 - val_loss: 0.7220 - val_accuracy: 0.7202\n",
      "Epoch 885/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.7350 - accuracy: 0.7047 - val_loss: 0.7239 - val_accuracy: 0.7205\n",
      "Epoch 886/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.7481 - accuracy: 0.7021 - val_loss: 0.7261 - val_accuracy: 0.7241\n",
      "Epoch 887/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.7458 - accuracy: 0.7027 - val_loss: 0.7273 - val_accuracy: 0.7224\n",
      "Epoch 888/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.7370 - accuracy: 0.7039 - val_loss: 0.7245 - val_accuracy: 0.7193\n",
      "Epoch 889/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.7378 - accuracy: 0.7053 - val_loss: 0.7227 - val_accuracy: 0.7213\n",
      "Epoch 890/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.7473 - accuracy: 0.7010 - val_loss: 0.7221 - val_accuracy: 0.7222\n",
      "Epoch 891/2000\n",
      "14152/14152 [==============================] - 5s 330us/step - loss: 0.7362 - accuracy: 0.7036 - val_loss: 0.7226 - val_accuracy: 0.7207\n",
      "Epoch 892/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 0.7438 - accuracy: 0.7029 - val_loss: 0.7217 - val_accuracy: 0.7227\n",
      "Epoch 893/2000\n",
      "14152/14152 [==============================] - 5s 328us/step - loss: 0.7477 - accuracy: 0.6991 - val_loss: 0.7200 - val_accuracy: 0.7241\n",
      "Epoch 894/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 0.7403 - accuracy: 0.7024 - val_loss: 0.7254 - val_accuracy: 0.7216\n",
      "Epoch 895/2000\n",
      "14152/14152 [==============================] - 5s 329us/step - loss: 0.7433 - accuracy: 0.7012 - val_loss: 0.7310 - val_accuracy: 0.7145\n",
      "Epoch 896/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 0.7428 - accuracy: 0.7024 - val_loss: 0.7220 - val_accuracy: 0.7205\n",
      "Epoch 897/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 0.7384 - accuracy: 0.7051 - val_loss: 0.7256 - val_accuracy: 0.7216\n",
      "Epoch 898/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 0.7384 - accuracy: 0.7011 - val_loss: 0.7251 - val_accuracy: 0.7210\n",
      "Epoch 899/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 0.7368 - accuracy: 0.7055 - val_loss: 0.7237 - val_accuracy: 0.7191\n",
      "Epoch 900/2000\n",
      "14152/14152 [==============================] - 5s 339us/step - loss: 0.7370 - accuracy: 0.7056 - val_loss: 0.7229 - val_accuracy: 0.7210\n",
      "Epoch 901/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.7369 - accuracy: 0.7029 - val_loss: 0.7246 - val_accuracy: 0.7207\n",
      "Epoch 902/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.7363 - accuracy: 0.7065 - val_loss: 0.7268 - val_accuracy: 0.7210\n",
      "Epoch 903/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.7418 - accuracy: 0.7018 - val_loss: 0.7267 - val_accuracy: 0.7227\n",
      "Epoch 904/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7352 - accuracy: 0.7049 - val_loss: 0.7191 - val_accuracy: 0.7233\n",
      "Epoch 905/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7299 - accuracy: 0.7085 - val_loss: 0.7112 - val_accuracy: 0.7247\n",
      "Epoch 906/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7340 - accuracy: 0.7065 - val_loss: 0.7261 - val_accuracy: 0.7188\n",
      "Epoch 907/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.7411 - accuracy: 0.7040 - val_loss: 0.7187 - val_accuracy: 0.7216\n",
      "Epoch 908/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.7402 - accuracy: 0.7038 - val_loss: 0.7274 - val_accuracy: 0.7193\n",
      "Epoch 909/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.7437 - accuracy: 0.7008 - val_loss: 0.7183 - val_accuracy: 0.7233\n",
      "Epoch 910/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7344 - accuracy: 0.7008 - val_loss: 0.7198 - val_accuracy: 0.7239\n",
      "Epoch 911/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.7354 - accuracy: 0.7038 - val_loss: 0.7221 - val_accuracy: 0.7199\n",
      "Epoch 912/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7342 - accuracy: 0.7035 - val_loss: 0.7114 - val_accuracy: 0.7227\n",
      "Epoch 913/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7332 - accuracy: 0.7054 - val_loss: 0.7186 - val_accuracy: 0.7241\n",
      "Epoch 914/2000\n",
      "14152/14152 [==============================] - 5s 331us/step - loss: 0.7317 - accuracy: 0.7080 - val_loss: 0.7222 - val_accuracy: 0.7216\n",
      "Epoch 915/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.7296 - accuracy: 0.7061 - val_loss: 0.7143 - val_accuracy: 0.7227\n",
      "Epoch 916/2000\n",
      "14152/14152 [==============================] - 5s 350us/step - loss: 0.7354 - accuracy: 0.7041 - val_loss: 0.7188 - val_accuracy: 0.7227\n",
      "Epoch 917/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7259 - accuracy: 0.7075 - val_loss: 0.7192 - val_accuracy: 0.7230\n",
      "Epoch 918/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7315 - accuracy: 0.7055 - val_loss: 0.7134 - val_accuracy: 0.7230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 919/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7308 - accuracy: 0.7083 - val_loss: 0.7151 - val_accuracy: 0.7236\n",
      "Epoch 920/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7291 - accuracy: 0.7088 - val_loss: 0.7213 - val_accuracy: 0.7196\n",
      "Epoch 921/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7384 - accuracy: 0.7057 - val_loss: 0.7179 - val_accuracy: 0.7233\n",
      "Epoch 922/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7279 - accuracy: 0.7058 - val_loss: 0.7174 - val_accuracy: 0.7230\n",
      "Epoch 923/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7357 - accuracy: 0.7061 - val_loss: 0.7189 - val_accuracy: 0.7216\n",
      "Epoch 924/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7366 - accuracy: 0.7049 - val_loss: 0.7181 - val_accuracy: 0.7210\n",
      "Epoch 925/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.7317 - accuracy: 0.7056 - val_loss: 0.7145 - val_accuracy: 0.7222\n",
      "Epoch 926/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.7280 - accuracy: 0.7080 - val_loss: 0.7252 - val_accuracy: 0.7216\n",
      "Epoch 927/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7344 - accuracy: 0.7041 - val_loss: 0.7142 - val_accuracy: 0.7224\n",
      "Epoch 928/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7349 - accuracy: 0.7044 - val_loss: 0.7166 - val_accuracy: 0.7219\n",
      "Epoch 929/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7295 - accuracy: 0.7082 - val_loss: 0.7114 - val_accuracy: 0.7258\n",
      "Epoch 930/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7321 - accuracy: 0.7041 - val_loss: 0.7195 - val_accuracy: 0.7207\n",
      "Epoch 931/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7322 - accuracy: 0.7047 - val_loss: 0.7167 - val_accuracy: 0.7216\n",
      "Epoch 932/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7308 - accuracy: 0.7064 - val_loss: 0.7221 - val_accuracy: 0.7205\n",
      "Epoch 933/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7344 - accuracy: 0.7039 - val_loss: 0.7174 - val_accuracy: 0.7224\n",
      "Epoch 934/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7294 - accuracy: 0.7046 - val_loss: 0.7156 - val_accuracy: 0.7230\n",
      "Epoch 935/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7268 - accuracy: 0.7084 - val_loss: 0.7202 - val_accuracy: 0.7227\n",
      "Epoch 936/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.7305 - accuracy: 0.7067 - val_loss: 0.7110 - val_accuracy: 0.7233\n",
      "Epoch 937/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7289 - accuracy: 0.7066 - val_loss: 0.7174 - val_accuracy: 0.7202\n",
      "Epoch 938/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.7325 - accuracy: 0.7053 - val_loss: 0.7122 - val_accuracy: 0.7227\n",
      "Epoch 939/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7379 - accuracy: 0.7052 - val_loss: 0.7123 - val_accuracy: 0.7267\n",
      "Epoch 940/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.7269 - accuracy: 0.7058 - val_loss: 0.7149 - val_accuracy: 0.7224\n",
      "Epoch 941/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7371 - accuracy: 0.7034 - val_loss: 0.7138 - val_accuracy: 0.7227\n",
      "Epoch 942/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7294 - accuracy: 0.7088 - val_loss: 0.7054 - val_accuracy: 0.7258\n",
      "Epoch 943/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7296 - accuracy: 0.7063 - val_loss: 0.7135 - val_accuracy: 0.7239\n",
      "Epoch 944/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7253 - accuracy: 0.7080 - val_loss: 0.7151 - val_accuracy: 0.7236\n",
      "Epoch 945/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7281 - accuracy: 0.7058 - val_loss: 0.7127 - val_accuracy: 0.7227\n",
      "Epoch 946/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7263 - accuracy: 0.7061 - val_loss: 0.7033 - val_accuracy: 0.7250\n",
      "Epoch 947/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7290 - accuracy: 0.7075 - val_loss: 0.7109 - val_accuracy: 0.7267\n",
      "Epoch 948/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7302 - accuracy: 0.7062 - val_loss: 0.7192 - val_accuracy: 0.7202\n",
      "Epoch 949/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7280 - accuracy: 0.7040 - val_loss: 0.7200 - val_accuracy: 0.7219\n",
      "Epoch 950/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7248 - accuracy: 0.7089 - val_loss: 0.7081 - val_accuracy: 0.7250\n",
      "Epoch 951/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7217 - accuracy: 0.7041 - val_loss: 0.7000 - val_accuracy: 0.7278\n",
      "Epoch 952/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.7318 - accuracy: 0.7024 - val_loss: 0.7198 - val_accuracy: 0.7210\n",
      "Epoch 953/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7203 - accuracy: 0.7113 - val_loss: 0.7133 - val_accuracy: 0.7207\n",
      "Epoch 954/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7167 - accuracy: 0.7106 - val_loss: 0.7092 - val_accuracy: 0.7258\n",
      "Epoch 955/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7230 - accuracy: 0.7068 - val_loss: 0.7023 - val_accuracy: 0.7278\n",
      "Epoch 956/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7284 - accuracy: 0.7089 - val_loss: 0.7092 - val_accuracy: 0.7236\n",
      "Epoch 957/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7243 - accuracy: 0.7072 - val_loss: 0.7131 - val_accuracy: 0.7250\n",
      "Epoch 958/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7305 - accuracy: 0.7053 - val_loss: 0.7120 - val_accuracy: 0.7244\n",
      "Epoch 959/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7235 - accuracy: 0.7086 - val_loss: 0.7038 - val_accuracy: 0.7275\n",
      "Epoch 960/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7258 - accuracy: 0.7078 - val_loss: 0.7123 - val_accuracy: 0.7236\n",
      "Epoch 961/2000\n",
      "14152/14152 [==============================] - 4s 305us/step - loss: 0.7245 - accuracy: 0.7070 - val_loss: 0.7084 - val_accuracy: 0.7236\n",
      "Epoch 962/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7226 - accuracy: 0.7117 - val_loss: 0.7114 - val_accuracy: 0.7258\n",
      "Epoch 963/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7226 - accuracy: 0.7092 - val_loss: 0.7122 - val_accuracy: 0.7227\n",
      "Epoch 964/2000\n",
      "14152/14152 [==============================] - 4s 305us/step - loss: 0.7155 - accuracy: 0.7112 - val_loss: 0.7142 - val_accuracy: 0.7230\n",
      "Epoch 965/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7204 - accuracy: 0.7081 - val_loss: 0.7065 - val_accuracy: 0.7272\n",
      "Epoch 966/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.7196 - accuracy: 0.7070 - val_loss: 0.7117 - val_accuracy: 0.7256\n",
      "Epoch 967/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7204 - accuracy: 0.7078 - val_loss: 0.7029 - val_accuracy: 0.7250\n",
      "Epoch 968/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.7196 - accuracy: 0.7085 - val_loss: 0.7058 - val_accuracy: 0.7241\n",
      "Epoch 969/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.7274 - accuracy: 0.7101 - val_loss: 0.7026 - val_accuracy: 0.7301\n",
      "Epoch 970/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7289 - accuracy: 0.7062 - val_loss: 0.7051 - val_accuracy: 0.7261\n",
      "Epoch 971/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7188 - accuracy: 0.7091 - val_loss: 0.7121 - val_accuracy: 0.7275\n",
      "Epoch 972/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7272 - accuracy: 0.7102 - val_loss: 0.7049 - val_accuracy: 0.7253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 973/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7223 - accuracy: 0.7108 - val_loss: 0.7035 - val_accuracy: 0.7261\n",
      "Epoch 974/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7211 - accuracy: 0.7089 - val_loss: 0.7053 - val_accuracy: 0.7239\n",
      "Epoch 975/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7225 - accuracy: 0.7077 - val_loss: 0.7035 - val_accuracy: 0.7247\n",
      "Epoch 976/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7225 - accuracy: 0.7094 - val_loss: 0.7024 - val_accuracy: 0.7270\n",
      "Epoch 977/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7216 - accuracy: 0.7089 - val_loss: 0.7017 - val_accuracy: 0.7256\n",
      "Epoch 978/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7197 - accuracy: 0.7072 - val_loss: 0.7078 - val_accuracy: 0.7236\n",
      "Epoch 979/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7198 - accuracy: 0.7094 - val_loss: 0.7028 - val_accuracy: 0.7287\n",
      "Epoch 980/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7203 - accuracy: 0.7088 - val_loss: 0.7066 - val_accuracy: 0.7278\n",
      "Epoch 981/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7208 - accuracy: 0.7102 - val_loss: 0.7090 - val_accuracy: 0.7264\n",
      "Epoch 982/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7149 - accuracy: 0.7122 - val_loss: 0.7058 - val_accuracy: 0.7256\n",
      "Epoch 983/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7275 - accuracy: 0.7048 - val_loss: 0.7046 - val_accuracy: 0.7253\n",
      "Epoch 984/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.7171 - accuracy: 0.7092 - val_loss: 0.7018 - val_accuracy: 0.7267\n",
      "Epoch 985/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.7176 - accuracy: 0.7140 - val_loss: 0.7025 - val_accuracy: 0.7261\n",
      "Epoch 986/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7235 - accuracy: 0.7094 - val_loss: 0.6993 - val_accuracy: 0.7295\n",
      "Epoch 987/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 0.7219 - accuracy: 0.7077 - val_loss: 0.7019 - val_accuracy: 0.7289\n",
      "Epoch 988/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7135 - accuracy: 0.7082 - val_loss: 0.7033 - val_accuracy: 0.7264\n",
      "Epoch 989/2000\n",
      "14152/14152 [==============================] - 5s 373us/step - loss: 0.7169 - accuracy: 0.7112 - val_loss: 0.7087 - val_accuracy: 0.7222\n",
      "Epoch 990/2000\n",
      "14152/14152 [==============================] - 6s 427us/step - loss: 0.7212 - accuracy: 0.7080 - val_loss: 0.7034 - val_accuracy: 0.7258\n",
      "Epoch 991/2000\n",
      "14152/14152 [==============================] - 5s 367us/step - loss: 0.7183 - accuracy: 0.7110 - val_loss: 0.7043 - val_accuracy: 0.7253\n",
      "Epoch 992/2000\n",
      "14152/14152 [==============================] - 5s 370us/step - loss: 0.7128 - accuracy: 0.7129 - val_loss: 0.7064 - val_accuracy: 0.7270\n",
      "Epoch 993/2000\n",
      "14152/14152 [==============================] - 5s 384us/step - loss: 0.7226 - accuracy: 0.7071 - val_loss: 0.7024 - val_accuracy: 0.7256\n",
      "Epoch 994/2000\n",
      "14152/14152 [==============================] - 5s 345us/step - loss: 0.7202 - accuracy: 0.7111 - val_loss: 0.7015 - val_accuracy: 0.7284\n",
      "Epoch 995/2000\n",
      "14152/14152 [==============================] - 5s 349us/step - loss: 0.7083 - accuracy: 0.7137 - val_loss: 0.7006 - val_accuracy: 0.7270\n",
      "Epoch 996/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 0.7175 - accuracy: 0.7100 - val_loss: 0.7031 - val_accuracy: 0.7264\n",
      "Epoch 997/2000\n",
      "14152/14152 [==============================] - 5s 335us/step - loss: 0.7214 - accuracy: 0.7105 - val_loss: 0.7007 - val_accuracy: 0.7247\n",
      "Epoch 998/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 0.7188 - accuracy: 0.7072 - val_loss: 0.7031 - val_accuracy: 0.7278\n",
      "Epoch 999/2000\n",
      "14152/14152 [==============================] - 5s 335us/step - loss: 0.7004 - accuracy: 0.7152 - val_loss: 0.6981 - val_accuracy: 0.7264\n",
      "Epoch 1000/2000\n",
      "14152/14152 [==============================] - 5s 339us/step - loss: 0.7128 - accuracy: 0.7110 - val_loss: 0.7037 - val_accuracy: 0.7264\n",
      "Epoch 1001/2000\n",
      "14152/14152 [==============================] - 5s 382us/step - loss: 0.7057 - accuracy: 0.7121 - val_loss: 0.6988 - val_accuracy: 0.7292\n",
      "Epoch 1002/2000\n",
      "14152/14152 [==============================] - 5s 343us/step - loss: 0.7221 - accuracy: 0.7068 - val_loss: 0.6992 - val_accuracy: 0.7278\n",
      "Epoch 1003/2000\n",
      "14152/14152 [==============================] - 5s 348us/step - loss: 0.7162 - accuracy: 0.7079 - val_loss: 0.6979 - val_accuracy: 0.7270\n",
      "Epoch 1004/2000\n",
      "14152/14152 [==============================] - 5s 351us/step - loss: 0.7187 - accuracy: 0.7101 - val_loss: 0.7074 - val_accuracy: 0.7272\n",
      "Epoch 1005/2000\n",
      "14152/14152 [==============================] - 5s 331us/step - loss: 0.7117 - accuracy: 0.7140 - val_loss: 0.7018 - val_accuracy: 0.7267\n",
      "Epoch 1006/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 0.7112 - accuracy: 0.7150 - val_loss: 0.6914 - val_accuracy: 0.7298\n",
      "Epoch 1007/2000\n",
      "14152/14152 [==============================] - 5s 349us/step - loss: 0.7089 - accuracy: 0.7093 - val_loss: 0.7017 - val_accuracy: 0.7256\n",
      "Epoch 1008/2000\n",
      "14152/14152 [==============================] - 5s 347us/step - loss: 0.7131 - accuracy: 0.7111 - val_loss: 0.6942 - val_accuracy: 0.7337\n",
      "Epoch 1009/2000\n",
      "14152/14152 [==============================] - 5s 318us/step - loss: 0.7187 - accuracy: 0.7106 - val_loss: 0.7045 - val_accuracy: 0.7272\n",
      "Epoch 1010/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.7087 - accuracy: 0.7125 - val_loss: 0.7007 - val_accuracy: 0.7256\n",
      "Epoch 1011/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.7143 - accuracy: 0.7111 - val_loss: 0.6960 - val_accuracy: 0.7287\n",
      "Epoch 1012/2000\n",
      "14152/14152 [==============================] - 5s 330us/step - loss: 0.7117 - accuracy: 0.7107 - val_loss: 0.7004 - val_accuracy: 0.7261\n",
      "Epoch 1013/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.7101 - accuracy: 0.7104 - val_loss: 0.6933 - val_accuracy: 0.7306\n",
      "Epoch 1014/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.7088 - accuracy: 0.7152 - val_loss: 0.6983 - val_accuracy: 0.7278\n",
      "Epoch 1015/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 0.7166 - accuracy: 0.7100 - val_loss: 0.7070 - val_accuracy: 0.7227\n",
      "Epoch 1016/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7102 - accuracy: 0.7119 - val_loss: 0.6992 - val_accuracy: 0.7284\n",
      "Epoch 1017/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 0.7099 - accuracy: 0.7083 - val_loss: 0.6925 - val_accuracy: 0.7329\n",
      "Epoch 1018/2000\n",
      "14152/14152 [==============================] - 5s 352us/step - loss: 0.7095 - accuracy: 0.7136 - val_loss: 0.7007 - val_accuracy: 0.7315\n",
      "Epoch 1019/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 0.7095 - accuracy: 0.7161 - val_loss: 0.6967 - val_accuracy: 0.7281\n",
      "Epoch 1020/2000\n",
      "14152/14152 [==============================] - 5s 340us/step - loss: 0.7087 - accuracy: 0.7113 - val_loss: 0.7004 - val_accuracy: 0.7272\n",
      "Epoch 1021/2000\n",
      "14152/14152 [==============================] - 5s 328us/step - loss: 0.7145 - accuracy: 0.7070 - val_loss: 0.6881 - val_accuracy: 0.7349\n",
      "Epoch 1022/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 0.7114 - accuracy: 0.7147 - val_loss: 0.6950 - val_accuracy: 0.7287\n",
      "Epoch 1023/2000\n",
      "14152/14152 [==============================] - 5s 328us/step - loss: 0.7119 - accuracy: 0.7113 - val_loss: 0.6936 - val_accuracy: 0.7287\n",
      "Epoch 1024/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 0.7054 - accuracy: 0.7123 - val_loss: 0.6985 - val_accuracy: 0.7264\n",
      "Epoch 1025/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 0.7111 - accuracy: 0.7098 - val_loss: 0.7050 - val_accuracy: 0.7247\n",
      "Epoch 1026/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 0.7085 - accuracy: 0.7116 - val_loss: 0.7003 - val_accuracy: 0.7270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1027/2000\n",
      "14152/14152 [==============================] - 5s 346us/step - loss: 0.7098 - accuracy: 0.7116 - val_loss: 0.7016 - val_accuracy: 0.7272\n",
      "Epoch 1028/2000\n",
      "14152/14152 [==============================] - 5s 346us/step - loss: 0.7095 - accuracy: 0.7119 - val_loss: 0.6952 - val_accuracy: 0.7284\n",
      "Epoch 1029/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.7147 - accuracy: 0.7123 - val_loss: 0.6948 - val_accuracy: 0.7312\n",
      "Epoch 1030/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.7114 - accuracy: 0.7122 - val_loss: 0.6910 - val_accuracy: 0.7309\n",
      "Epoch 1031/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.7106 - accuracy: 0.7082 - val_loss: 0.6941 - val_accuracy: 0.7275\n",
      "Epoch 1032/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.7054 - accuracy: 0.7145 - val_loss: 0.6965 - val_accuracy: 0.7284\n",
      "Epoch 1033/2000\n",
      "14152/14152 [==============================] - 5s 333us/step - loss: 0.7128 - accuracy: 0.7123 - val_loss: 0.6942 - val_accuracy: 0.7301\n",
      "Epoch 1034/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.7077 - accuracy: 0.7109 - val_loss: 0.6960 - val_accuracy: 0.7312\n",
      "Epoch 1035/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.7116 - accuracy: 0.7097 - val_loss: 0.6918 - val_accuracy: 0.7304\n",
      "Epoch 1036/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.7060 - accuracy: 0.7109 - val_loss: 0.6865 - val_accuracy: 0.7321\n",
      "Epoch 1037/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 0.7080 - accuracy: 0.7151 - val_loss: 0.6944 - val_accuracy: 0.7278\n",
      "Epoch 1038/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 0.7020 - accuracy: 0.7151 - val_loss: 0.6968 - val_accuracy: 0.7278\n",
      "Epoch 1039/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 0.7091 - accuracy: 0.7123 - val_loss: 0.7028 - val_accuracy: 0.7247\n",
      "Epoch 1040/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.7033 - accuracy: 0.7157 - val_loss: 0.6946 - val_accuracy: 0.7267\n",
      "Epoch 1041/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.7036 - accuracy: 0.7120 - val_loss: 0.6884 - val_accuracy: 0.7298\n",
      "Epoch 1042/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7160 - accuracy: 0.7088 - val_loss: 0.6909 - val_accuracy: 0.7315\n",
      "Epoch 1043/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.7139 - accuracy: 0.7080 - val_loss: 0.6916 - val_accuracy: 0.7284\n",
      "Epoch 1044/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7092 - accuracy: 0.7089 - val_loss: 0.6926 - val_accuracy: 0.7301\n",
      "Epoch 1045/2000\n",
      "14152/14152 [==============================] - 5s 370us/step - loss: 0.6983 - accuracy: 0.7160 - val_loss: 0.6956 - val_accuracy: 0.7281\n",
      "Epoch 1046/2000\n",
      "14152/14152 [==============================] - 5s 340us/step - loss: 0.7081 - accuracy: 0.7094 - val_loss: 0.6949 - val_accuracy: 0.7301\n",
      "Epoch 1047/2000\n",
      "14152/14152 [==============================] - 5s 357us/step - loss: 0.7070 - accuracy: 0.7140 - val_loss: 0.6951 - val_accuracy: 0.7264\n",
      "Epoch 1048/2000\n",
      "14152/14152 [==============================] - 5s 329us/step - loss: 0.7047 - accuracy: 0.7123 - val_loss: 0.6897 - val_accuracy: 0.7312\n",
      "Epoch 1049/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.7027 - accuracy: 0.7140 - val_loss: 0.6897 - val_accuracy: 0.7326\n",
      "Epoch 1050/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 0.7041 - accuracy: 0.7140 - val_loss: 0.6942 - val_accuracy: 0.7275\n",
      "Epoch 1051/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 0.7074 - accuracy: 0.7144 - val_loss: 0.6938 - val_accuracy: 0.7289\n",
      "Epoch 1052/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.7074 - accuracy: 0.7162 - val_loss: 0.6822 - val_accuracy: 0.7349\n",
      "Epoch 1053/2000\n",
      "14152/14152 [==============================] - 5s 348us/step - loss: 0.7115 - accuracy: 0.7147 - val_loss: 0.6859 - val_accuracy: 0.7332\n",
      "Epoch 1054/2000\n",
      "14152/14152 [==============================] - 5s 357us/step - loss: 0.7074 - accuracy: 0.7111 - val_loss: 0.6963 - val_accuracy: 0.7272\n",
      "Epoch 1055/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.7031 - accuracy: 0.7153 - val_loss: 0.6913 - val_accuracy: 0.7309\n",
      "Epoch 1056/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.7082 - accuracy: 0.7128 - val_loss: 0.6909 - val_accuracy: 0.7289\n",
      "Epoch 1057/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.7046 - accuracy: 0.7130 - val_loss: 0.6858 - val_accuracy: 0.7354\n",
      "Epoch 1058/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.7068 - accuracy: 0.7123 - val_loss: 0.6963 - val_accuracy: 0.7272\n",
      "Epoch 1059/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.7016 - accuracy: 0.7185 - val_loss: 0.6915 - val_accuracy: 0.7292\n",
      "Epoch 1060/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7056 - accuracy: 0.7125 - val_loss: 0.6932 - val_accuracy: 0.7264\n",
      "Epoch 1061/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7067 - accuracy: 0.7106 - val_loss: 0.6969 - val_accuracy: 0.7309\n",
      "Epoch 1062/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.7063 - accuracy: 0.7116 - val_loss: 0.6836 - val_accuracy: 0.7309\n",
      "Epoch 1063/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.7036 - accuracy: 0.7186 - val_loss: 0.6908 - val_accuracy: 0.7323\n",
      "Epoch 1064/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 0.7005 - accuracy: 0.7168 - val_loss: 0.6843 - val_accuracy: 0.7335\n",
      "Epoch 1065/2000\n",
      "14152/14152 [==============================] - 5s 349us/step - loss: 0.7036 - accuracy: 0.7137 - val_loss: 0.6937 - val_accuracy: 0.7315\n",
      "Epoch 1066/2000\n",
      "14152/14152 [==============================] - 5s 342us/step - loss: 0.7034 - accuracy: 0.7115 - val_loss: 0.6981 - val_accuracy: 0.7261\n",
      "Epoch 1067/2000\n",
      "14152/14152 [==============================] - 5s 357us/step - loss: 0.7018 - accuracy: 0.7114 - val_loss: 0.6838 - val_accuracy: 0.7323\n",
      "Epoch 1068/2000\n",
      "14152/14152 [==============================] - 5s 341us/step - loss: 0.7005 - accuracy: 0.7172 - val_loss: 0.6890 - val_accuracy: 0.7329\n",
      "Epoch 1069/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.7014 - accuracy: 0.7160 - val_loss: 0.6896 - val_accuracy: 0.7301\n",
      "Epoch 1070/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.7027 - accuracy: 0.7117 - val_loss: 0.6877 - val_accuracy: 0.7321\n",
      "Epoch 1071/2000\n",
      "14152/14152 [==============================] - 5s 329us/step - loss: 0.6978 - accuracy: 0.7164 - val_loss: 0.6850 - val_accuracy: 0.7343\n",
      "Epoch 1072/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.7035 - accuracy: 0.7113 - val_loss: 0.6876 - val_accuracy: 0.7318\n",
      "Epoch 1073/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.7043 - accuracy: 0.7147 - val_loss: 0.6926 - val_accuracy: 0.7289\n",
      "Epoch 1074/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 0.6950 - accuracy: 0.7185 - val_loss: 0.6871 - val_accuracy: 0.7321\n",
      "Epoch 1075/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 0.7001 - accuracy: 0.7172 - val_loss: 0.6815 - val_accuracy: 0.7371\n",
      "Epoch 1076/2000\n",
      "14152/14152 [==============================] - 5s 347us/step - loss: 0.6958 - accuracy: 0.7171 - val_loss: 0.6851 - val_accuracy: 0.7318\n",
      "Epoch 1077/2000\n",
      "14152/14152 [==============================] - 5s 318us/step - loss: 0.7060 - accuracy: 0.7150 - val_loss: 0.6917 - val_accuracy: 0.7284\n",
      "Epoch 1078/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 0.7100 - accuracy: 0.7121 - val_loss: 0.6851 - val_accuracy: 0.7332\n",
      "Epoch 1079/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 0.6991 - accuracy: 0.7174 - val_loss: 0.6872 - val_accuracy: 0.7318\n",
      "Epoch 1080/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14152/14152 [==============================] - 5s 365us/step - loss: 0.6960 - accuracy: 0.7190 - val_loss: 0.6908 - val_accuracy: 0.7301\n",
      "Epoch 1081/2000\n",
      "14152/14152 [==============================] - 5s 368us/step - loss: 0.6944 - accuracy: 0.7164 - val_loss: 0.6800 - val_accuracy: 0.7312\n",
      "Epoch 1082/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 0.6971 - accuracy: 0.7157 - val_loss: 0.6893 - val_accuracy: 0.7287\n",
      "Epoch 1083/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 0.6977 - accuracy: 0.7145 - val_loss: 0.6851 - val_accuracy: 0.7366\n",
      "Epoch 1084/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 0.6984 - accuracy: 0.7130 - val_loss: 0.6866 - val_accuracy: 0.7326\n",
      "Epoch 1085/2000\n",
      "14152/14152 [==============================] - 5s 328us/step - loss: 0.6974 - accuracy: 0.7120 - val_loss: 0.6838 - val_accuracy: 0.7346\n",
      "Epoch 1086/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.6957 - accuracy: 0.7172 - val_loss: 0.6860 - val_accuracy: 0.7349\n",
      "Epoch 1087/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.7025 - accuracy: 0.7138 - val_loss: 0.6855 - val_accuracy: 0.7332\n",
      "Epoch 1088/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.6951 - accuracy: 0.7186 - val_loss: 0.6866 - val_accuracy: 0.7349\n",
      "Epoch 1089/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.6962 - accuracy: 0.7164 - val_loss: 0.6889 - val_accuracy: 0.7309\n",
      "Epoch 1090/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.7014 - accuracy: 0.7162 - val_loss: 0.6927 - val_accuracy: 0.7315\n",
      "Epoch 1091/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.7073 - accuracy: 0.7135 - val_loss: 0.6791 - val_accuracy: 0.7360\n",
      "Epoch 1092/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.7011 - accuracy: 0.7166 - val_loss: 0.6843 - val_accuracy: 0.7352\n",
      "Epoch 1093/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 0.6899 - accuracy: 0.7195 - val_loss: 0.6821 - val_accuracy: 0.7335\n",
      "Epoch 1094/2000\n",
      "14152/14152 [==============================] - 5s 333us/step - loss: 0.7049 - accuracy: 0.7128 - val_loss: 0.6871 - val_accuracy: 0.7332\n",
      "Epoch 1095/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.6948 - accuracy: 0.7179 - val_loss: 0.6801 - val_accuracy: 0.7360\n",
      "Epoch 1096/2000\n",
      "14152/14152 [==============================] - 5s 331us/step - loss: 0.6965 - accuracy: 0.7155 - val_loss: 0.6795 - val_accuracy: 0.7380\n",
      "Epoch 1097/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.6948 - accuracy: 0.7202 - val_loss: 0.6805 - val_accuracy: 0.7357\n",
      "Epoch 1098/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.6974 - accuracy: 0.7121 - val_loss: 0.6816 - val_accuracy: 0.7346\n",
      "Epoch 1099/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.7008 - accuracy: 0.7155 - val_loss: 0.6815 - val_accuracy: 0.7337\n",
      "Epoch 1100/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.6956 - accuracy: 0.7198 - val_loss: 0.6793 - val_accuracy: 0.7354\n",
      "Epoch 1101/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.6921 - accuracy: 0.7200 - val_loss: 0.6796 - val_accuracy: 0.7357\n",
      "Epoch 1102/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.6976 - accuracy: 0.7145 - val_loss: 0.6806 - val_accuracy: 0.7318\n",
      "Epoch 1103/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6969 - accuracy: 0.7145 - val_loss: 0.6785 - val_accuracy: 0.7369\n",
      "Epoch 1104/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.6912 - accuracy: 0.7159 - val_loss: 0.6884 - val_accuracy: 0.7352\n",
      "Epoch 1105/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 0.6980 - accuracy: 0.7182 - val_loss: 0.6919 - val_accuracy: 0.7292\n",
      "Epoch 1106/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.6997 - accuracy: 0.7140 - val_loss: 0.6836 - val_accuracy: 0.7335\n",
      "Epoch 1107/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.6928 - accuracy: 0.7179 - val_loss: 0.6778 - val_accuracy: 0.7326\n",
      "Epoch 1108/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.6959 - accuracy: 0.7147 - val_loss: 0.6765 - val_accuracy: 0.7366\n",
      "Epoch 1109/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.6978 - accuracy: 0.7147 - val_loss: 0.6791 - val_accuracy: 0.7323\n",
      "Epoch 1110/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.6944 - accuracy: 0.7181 - val_loss: 0.6784 - val_accuracy: 0.7349\n",
      "Epoch 1111/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.6939 - accuracy: 0.7159 - val_loss: 0.6821 - val_accuracy: 0.7312\n",
      "Epoch 1112/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.6929 - accuracy: 0.7132 - val_loss: 0.6840 - val_accuracy: 0.7332\n",
      "Epoch 1113/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.6921 - accuracy: 0.7170 - val_loss: 0.6825 - val_accuracy: 0.7315\n",
      "Epoch 1114/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.6920 - accuracy: 0.7173 - val_loss: 0.6792 - val_accuracy: 0.7337\n",
      "Epoch 1115/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 0.6969 - accuracy: 0.7139 - val_loss: 0.6770 - val_accuracy: 0.7340\n",
      "Epoch 1116/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.6864 - accuracy: 0.7219 - val_loss: 0.6810 - val_accuracy: 0.7332\n",
      "Epoch 1117/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 0.6911 - accuracy: 0.7151 - val_loss: 0.6732 - val_accuracy: 0.7394\n",
      "Epoch 1118/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 0.6965 - accuracy: 0.7159 - val_loss: 0.6800 - val_accuracy: 0.7354\n",
      "Epoch 1119/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 0.6957 - accuracy: 0.7157 - val_loss: 0.6835 - val_accuracy: 0.7335\n",
      "Epoch 1120/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.6919 - accuracy: 0.7191 - val_loss: 0.6808 - val_accuracy: 0.7309\n",
      "Epoch 1121/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.6913 - accuracy: 0.7171 - val_loss: 0.6820 - val_accuracy: 0.7371\n",
      "Epoch 1122/2000\n",
      "14152/14152 [==============================] - 5s 318us/step - loss: 0.6939 - accuracy: 0.7171 - val_loss: 0.6838 - val_accuracy: 0.7343\n",
      "Epoch 1123/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.6933 - accuracy: 0.7142 - val_loss: 0.6747 - val_accuracy: 0.7349\n",
      "Epoch 1124/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 0.6862 - accuracy: 0.7198 - val_loss: 0.6769 - val_accuracy: 0.7329\n",
      "Epoch 1125/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 0.6865 - accuracy: 0.7176 - val_loss: 0.6789 - val_accuracy: 0.7391\n",
      "Epoch 1126/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.6883 - accuracy: 0.7176 - val_loss: 0.6734 - val_accuracy: 0.7408\n",
      "Epoch 1127/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.6892 - accuracy: 0.7191 - val_loss: 0.6882 - val_accuracy: 0.7287\n",
      "Epoch 1128/2000\n",
      "14152/14152 [==============================] - 5s 318us/step - loss: 0.6893 - accuracy: 0.7207 - val_loss: 0.6702 - val_accuracy: 0.7394\n",
      "Epoch 1129/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.6957 - accuracy: 0.7166 - val_loss: 0.6829 - val_accuracy: 0.7318\n",
      "Epoch 1130/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.6932 - accuracy: 0.7133 - val_loss: 0.6735 - val_accuracy: 0.7391\n",
      "Epoch 1131/2000\n",
      "14152/14152 [==============================] - 5s 328us/step - loss: 0.6830 - accuracy: 0.7220 - val_loss: 0.6705 - val_accuracy: 0.7414\n",
      "Epoch 1132/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.6901 - accuracy: 0.7160 - val_loss: 0.6871 - val_accuracy: 0.7306\n",
      "Epoch 1133/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.6954 - accuracy: 0.7168 - val_loss: 0.6750 - val_accuracy: 0.7377\n",
      "Epoch 1134/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 0.6862 - accuracy: 0.7195 - val_loss: 0.6817 - val_accuracy: 0.7346\n",
      "Epoch 1135/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.6938 - accuracy: 0.7175 - val_loss: 0.6790 - val_accuracy: 0.7357\n",
      "Epoch 1136/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.6864 - accuracy: 0.7169 - val_loss: 0.6786 - val_accuracy: 0.7363\n",
      "Epoch 1137/2000\n",
      "14152/14152 [==============================] - 5s 329us/step - loss: 0.6920 - accuracy: 0.7161 - val_loss: 0.6772 - val_accuracy: 0.7366\n",
      "Epoch 1138/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.6837 - accuracy: 0.7191 - val_loss: 0.6754 - val_accuracy: 0.7312\n",
      "Epoch 1139/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.6856 - accuracy: 0.7203 - val_loss: 0.6708 - val_accuracy: 0.7397\n",
      "Epoch 1140/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.6868 - accuracy: 0.7171 - val_loss: 0.6740 - val_accuracy: 0.7377\n",
      "Epoch 1141/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.6911 - accuracy: 0.7157 - val_loss: 0.6765 - val_accuracy: 0.7352\n",
      "Epoch 1142/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.6901 - accuracy: 0.7176 - val_loss: 0.6717 - val_accuracy: 0.7335\n",
      "Epoch 1143/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 0.6844 - accuracy: 0.7203 - val_loss: 0.6792 - val_accuracy: 0.7306\n",
      "Epoch 1144/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 0.6877 - accuracy: 0.7205 - val_loss: 0.6741 - val_accuracy: 0.7360\n",
      "Epoch 1145/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.6919 - accuracy: 0.7160 - val_loss: 0.6754 - val_accuracy: 0.7371\n",
      "Epoch 1146/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 0.6876 - accuracy: 0.7196 - val_loss: 0.6796 - val_accuracy: 0.7363\n",
      "Epoch 1147/2000\n",
      "14152/14152 [==============================] - 5s 355us/step - loss: 0.6881 - accuracy: 0.7192 - val_loss: 0.6802 - val_accuracy: 0.7332\n",
      "Epoch 1148/2000\n",
      "14152/14152 [==============================] - 5s 330us/step - loss: 0.6804 - accuracy: 0.7233 - val_loss: 0.6760 - val_accuracy: 0.7357\n",
      "Epoch 1149/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.6851 - accuracy: 0.7172 - val_loss: 0.6702 - val_accuracy: 0.7400\n",
      "Epoch 1150/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.6912 - accuracy: 0.7191 - val_loss: 0.6706 - val_accuracy: 0.7369\n",
      "Epoch 1151/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 0.6865 - accuracy: 0.7174 - val_loss: 0.6768 - val_accuracy: 0.7326\n",
      "Epoch 1152/2000\n",
      "14152/14152 [==============================] - 5s 318us/step - loss: 0.6887 - accuracy: 0.7170 - val_loss: 0.6772 - val_accuracy: 0.7354\n",
      "Epoch 1153/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 0.6937 - accuracy: 0.7159 - val_loss: 0.6771 - val_accuracy: 0.7340\n",
      "Epoch 1154/2000\n",
      "14152/14152 [==============================] - 5s 363us/step - loss: 0.6879 - accuracy: 0.7198 - val_loss: 0.6676 - val_accuracy: 0.7386\n",
      "Epoch 1155/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 0.6984 - accuracy: 0.7154 - val_loss: 0.6746 - val_accuracy: 0.7357\n",
      "Epoch 1156/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 0.6839 - accuracy: 0.7240 - val_loss: 0.6646 - val_accuracy: 0.7402\n",
      "Epoch 1157/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 0.6846 - accuracy: 0.7200 - val_loss: 0.6745 - val_accuracy: 0.7323\n",
      "Epoch 1158/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.6769 - accuracy: 0.7221 - val_loss: 0.6663 - val_accuracy: 0.7357\n",
      "Epoch 1159/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.6854 - accuracy: 0.7165 - val_loss: 0.6763 - val_accuracy: 0.7386\n",
      "Epoch 1160/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.6862 - accuracy: 0.7204 - val_loss: 0.6735 - val_accuracy: 0.7371\n",
      "Epoch 1161/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.6895 - accuracy: 0.7174 - val_loss: 0.6730 - val_accuracy: 0.7352\n",
      "Epoch 1162/2000\n",
      "14152/14152 [==============================] - 5s 346us/step - loss: 0.6872 - accuracy: 0.7150 - val_loss: 0.6758 - val_accuracy: 0.7343\n",
      "Epoch 1163/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.6863 - accuracy: 0.7154 - val_loss: 0.6796 - val_accuracy: 0.7332\n",
      "Epoch 1164/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.6807 - accuracy: 0.7210 - val_loss: 0.6761 - val_accuracy: 0.7371\n",
      "Epoch 1165/2000\n",
      "14152/14152 [==============================] - 5s 333us/step - loss: 0.6899 - accuracy: 0.7159 - val_loss: 0.6711 - val_accuracy: 0.7335\n",
      "Epoch 1166/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 0.6863 - accuracy: 0.7188 - val_loss: 0.6724 - val_accuracy: 0.7380\n",
      "Epoch 1167/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.6805 - accuracy: 0.7207 - val_loss: 0.6719 - val_accuracy: 0.7371\n",
      "Epoch 1168/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 0.6791 - accuracy: 0.7215 - val_loss: 0.6782 - val_accuracy: 0.7337\n",
      "Epoch 1169/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.6796 - accuracy: 0.7222 - val_loss: 0.6759 - val_accuracy: 0.7354\n",
      "Epoch 1170/2000\n",
      "14152/14152 [==============================] - 5s 339us/step - loss: 0.6835 - accuracy: 0.7177 - val_loss: 0.6776 - val_accuracy: 0.7301\n",
      "Epoch 1171/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 0.6849 - accuracy: 0.7181 - val_loss: 0.6787 - val_accuracy: 0.7321\n",
      "Epoch 1172/2000\n",
      "14152/14152 [==============================] - 5s 349us/step - loss: 0.6918 - accuracy: 0.7182 - val_loss: 0.6800 - val_accuracy: 0.7332\n",
      "Epoch 1173/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 0.6858 - accuracy: 0.7204 - val_loss: 0.6690 - val_accuracy: 0.7380\n",
      "Epoch 1174/2000\n",
      "14152/14152 [==============================] - 5s 333us/step - loss: 0.6770 - accuracy: 0.7205 - val_loss: 0.6679 - val_accuracy: 0.7371\n",
      "Epoch 1175/2000\n",
      "14152/14152 [==============================] - 5s 329us/step - loss: 0.6884 - accuracy: 0.7213 - val_loss: 0.6765 - val_accuracy: 0.7318\n",
      "Epoch 1176/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 0.6862 - accuracy: 0.7188 - val_loss: 0.6638 - val_accuracy: 0.7394\n",
      "Epoch 1177/2000\n",
      "14152/14152 [==============================] - 5s 328us/step - loss: 0.6839 - accuracy: 0.7225 - val_loss: 0.6663 - val_accuracy: 0.7402\n",
      "Epoch 1178/2000\n",
      "14152/14152 [==============================] - 5s 356us/step - loss: 0.6894 - accuracy: 0.7200 - val_loss: 0.6674 - val_accuracy: 0.7369\n",
      "Epoch 1179/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.6854 - accuracy: 0.7184 - val_loss: 0.6689 - val_accuracy: 0.7388\n",
      "Epoch 1180/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 0.6843 - accuracy: 0.7217 - val_loss: 0.6729 - val_accuracy: 0.7329\n",
      "Epoch 1181/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.6840 - accuracy: 0.7207 - val_loss: 0.6664 - val_accuracy: 0.7354\n",
      "Epoch 1182/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.6790 - accuracy: 0.7191 - val_loss: 0.6737 - val_accuracy: 0.7349\n",
      "Epoch 1183/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.6812 - accuracy: 0.7206 - val_loss: 0.6605 - val_accuracy: 0.7349\n",
      "Epoch 1184/2000\n",
      "14152/14152 [==============================] - 5s 339us/step - loss: 0.6780 - accuracy: 0.7243 - val_loss: 0.6723 - val_accuracy: 0.7354\n",
      "Epoch 1185/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.6771 - accuracy: 0.7224 - val_loss: 0.6745 - val_accuracy: 0.7354\n",
      "Epoch 1186/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14152/14152 [==============================] - 5s 347us/step - loss: 0.6748 - accuracy: 0.7240 - val_loss: 0.6755 - val_accuracy: 0.7349\n",
      "Epoch 1187/2000\n",
      "14152/14152 [==============================] - 5s 365us/step - loss: 0.6814 - accuracy: 0.7188 - val_loss: 0.6686 - val_accuracy: 0.7357\n",
      "Epoch 1188/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 0.6753 - accuracy: 0.7248 - val_loss: 0.6618 - val_accuracy: 0.7422\n",
      "Epoch 1189/2000\n",
      "14152/14152 [==============================] - 5s 386us/step - loss: 0.6771 - accuracy: 0.7251 - val_loss: 0.6677 - val_accuracy: 0.7380\n",
      "Epoch 1190/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 0.6709 - accuracy: 0.7234 - val_loss: 0.6769 - val_accuracy: 0.7312\n",
      "Epoch 1191/2000\n",
      "14152/14152 [==============================] - 5s 339us/step - loss: 0.6797 - accuracy: 0.7244 - val_loss: 0.6637 - val_accuracy: 0.7386\n",
      "Epoch 1192/2000\n",
      "14152/14152 [==============================] - 5s 340us/step - loss: 0.6756 - accuracy: 0.7239 - val_loss: 0.6683 - val_accuracy: 0.7391\n",
      "Epoch 1193/2000\n",
      "14152/14152 [==============================] - 5s 346us/step - loss: 0.6780 - accuracy: 0.7205 - val_loss: 0.6708 - val_accuracy: 0.7363\n",
      "Epoch 1194/2000\n",
      "14152/14152 [==============================] - 5s 335us/step - loss: 0.6766 - accuracy: 0.7209 - val_loss: 0.6727 - val_accuracy: 0.7352\n",
      "Epoch 1195/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 0.6805 - accuracy: 0.7207 - val_loss: 0.6655 - val_accuracy: 0.7337\n",
      "Epoch 1196/2000\n",
      "14152/14152 [==============================] - 5s 343us/step - loss: 0.6807 - accuracy: 0.7219 - val_loss: 0.6607 - val_accuracy: 0.7400\n",
      "Epoch 1197/2000\n",
      "14152/14152 [==============================] - 5s 333us/step - loss: 0.6808 - accuracy: 0.7213 - val_loss: 0.6676 - val_accuracy: 0.7337\n",
      "Epoch 1198/2000\n",
      "14152/14152 [==============================] - 5s 356us/step - loss: 0.6798 - accuracy: 0.7214 - val_loss: 0.6689 - val_accuracy: 0.7337\n",
      "Epoch 1199/2000\n",
      "14152/14152 [==============================] - 5s 352us/step - loss: 0.6747 - accuracy: 0.7233 - val_loss: 0.6682 - val_accuracy: 0.7349\n",
      "Epoch 1200/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 0.6802 - accuracy: 0.7196 - val_loss: 0.6715 - val_accuracy: 0.7349\n",
      "Epoch 1201/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.6741 - accuracy: 0.7248 - val_loss: 0.6689 - val_accuracy: 0.7363\n",
      "Epoch 1202/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 0.6857 - accuracy: 0.7188 - val_loss: 0.6689 - val_accuracy: 0.7360\n",
      "Epoch 1203/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 0.6868 - accuracy: 0.7160 - val_loss: 0.6660 - val_accuracy: 0.7394\n",
      "Epoch 1204/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.6737 - accuracy: 0.7203 - val_loss: 0.6651 - val_accuracy: 0.7360\n",
      "Epoch 1205/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.6777 - accuracy: 0.7227 - val_loss: 0.6670 - val_accuracy: 0.7380\n",
      "Epoch 1206/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.6847 - accuracy: 0.7219 - val_loss: 0.6664 - val_accuracy: 0.7340\n",
      "Epoch 1207/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6774 - accuracy: 0.7207 - val_loss: 0.6650 - val_accuracy: 0.7374\n",
      "Epoch 1208/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.6738 - accuracy: 0.7203 - val_loss: 0.6709 - val_accuracy: 0.7335\n",
      "Epoch 1209/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.6796 - accuracy: 0.7240 - val_loss: 0.6694 - val_accuracy: 0.7369\n",
      "Epoch 1210/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.6845 - accuracy: 0.7204 - val_loss: 0.6611 - val_accuracy: 0.7388\n",
      "Epoch 1211/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 0.6804 - accuracy: 0.7224 - val_loss: 0.6716 - val_accuracy: 0.7349\n",
      "Epoch 1212/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.6754 - accuracy: 0.7232 - val_loss: 0.6683 - val_accuracy: 0.7349\n",
      "Epoch 1213/2000\n",
      "14152/14152 [==============================] - 5s 347us/step - loss: 0.6789 - accuracy: 0.7190 - val_loss: 0.6560 - val_accuracy: 0.7397\n",
      "Epoch 1214/2000\n",
      "14152/14152 [==============================] - 5s 339us/step - loss: 0.6753 - accuracy: 0.7227 - val_loss: 0.6663 - val_accuracy: 0.7337\n",
      "Epoch 1215/2000\n",
      "14152/14152 [==============================] - 5s 328us/step - loss: 0.6775 - accuracy: 0.7213 - val_loss: 0.6678 - val_accuracy: 0.7349\n",
      "Epoch 1216/2000\n",
      "14152/14152 [==============================] - 4s 318us/step - loss: 0.6748 - accuracy: 0.7230 - val_loss: 0.6591 - val_accuracy: 0.7391\n",
      "Epoch 1217/2000\n",
      "14152/14152 [==============================] - 5s 330us/step - loss: 0.6777 - accuracy: 0.7195 - val_loss: 0.6710 - val_accuracy: 0.7335\n",
      "Epoch 1218/2000\n",
      "14152/14152 [==============================] - 5s 346us/step - loss: 0.6774 - accuracy: 0.7231 - val_loss: 0.6650 - val_accuracy: 0.7332\n",
      "Epoch 1219/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 0.6781 - accuracy: 0.7225 - val_loss: 0.6580 - val_accuracy: 0.7394\n",
      "Epoch 1220/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.6769 - accuracy: 0.7231 - val_loss: 0.6683 - val_accuracy: 0.7354\n",
      "Epoch 1221/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 0.6701 - accuracy: 0.7246 - val_loss: 0.6703 - val_accuracy: 0.7337\n",
      "Epoch 1222/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.6752 - accuracy: 0.7266 - val_loss: 0.6650 - val_accuracy: 0.7371\n",
      "Epoch 1223/2000\n",
      "14152/14152 [==============================] - 5s 318us/step - loss: 0.6781 - accuracy: 0.7201 - val_loss: 0.6660 - val_accuracy: 0.7352\n",
      "Epoch 1224/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.6722 - accuracy: 0.7234 - val_loss: 0.6693 - val_accuracy: 0.7363\n",
      "Epoch 1225/2000\n",
      "14152/14152 [==============================] - 4s 312us/step - loss: 0.6711 - accuracy: 0.7240 - val_loss: 0.6572 - val_accuracy: 0.7388\n",
      "Epoch 1226/2000\n",
      "14152/14152 [==============================] - 5s 331us/step - loss: 0.6745 - accuracy: 0.7220 - val_loss: 0.6687 - val_accuracy: 0.7329\n",
      "Epoch 1227/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.6751 - accuracy: 0.7236 - val_loss: 0.6684 - val_accuracy: 0.7346\n",
      "Epoch 1228/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 0.6761 - accuracy: 0.7224 - val_loss: 0.6654 - val_accuracy: 0.7391\n",
      "Epoch 1229/2000\n",
      "14152/14152 [==============================] - 6s 410us/step - loss: 0.6738 - accuracy: 0.7262 - val_loss: 0.6573 - val_accuracy: 0.7400\n",
      "Epoch 1230/2000\n",
      "14152/14152 [==============================] - 5s 357us/step - loss: 0.6743 - accuracy: 0.7245 - val_loss: 0.6675 - val_accuracy: 0.7349\n",
      "Epoch 1231/2000\n",
      "14152/14152 [==============================] - 6s 396us/step - loss: 0.6723 - accuracy: 0.7236 - val_loss: 0.6595 - val_accuracy: 0.7394\n",
      "Epoch 1232/2000\n",
      "14152/14152 [==============================] - 5s 384us/step - loss: 0.6723 - accuracy: 0.7243 - val_loss: 0.6747 - val_accuracy: 0.7306\n",
      "Epoch 1233/2000\n",
      "14152/14152 [==============================] - 5s 369us/step - loss: 0.6737 - accuracy: 0.7243 - val_loss: 0.6542 - val_accuracy: 0.7431\n",
      "Epoch 1234/2000\n",
      "14152/14152 [==============================] - 5s 377us/step - loss: 0.6764 - accuracy: 0.7218 - val_loss: 0.6637 - val_accuracy: 0.7357\n",
      "Epoch 1235/2000\n",
      "14152/14152 [==============================] - 5s 364us/step - loss: 0.6734 - accuracy: 0.7249 - val_loss: 0.6707 - val_accuracy: 0.7335\n",
      "Epoch 1236/2000\n",
      "14152/14152 [==============================] - 6s 406us/step - loss: 0.6733 - accuracy: 0.7274 - val_loss: 0.6666 - val_accuracy: 0.7369\n",
      "Epoch 1237/2000\n",
      "14152/14152 [==============================] - 5s 379us/step - loss: 0.6715 - accuracy: 0.7205 - val_loss: 0.6669 - val_accuracy: 0.7352\n",
      "Epoch 1238/2000\n",
      "14152/14152 [==============================] - 5s 373us/step - loss: 0.6744 - accuracy: 0.7222 - val_loss: 0.6600 - val_accuracy: 0.7346\n",
      "Epoch 1239/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14152/14152 [==============================] - 5s 355us/step - loss: 0.6791 - accuracy: 0.7234 - val_loss: 0.6538 - val_accuracy: 0.7360\n",
      "Epoch 1240/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.6781 - accuracy: 0.7241 - val_loss: 0.6626 - val_accuracy: 0.7360\n",
      "Epoch 1241/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.6734 - accuracy: 0.7251 - val_loss: 0.6652 - val_accuracy: 0.7337\n",
      "Epoch 1242/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 0.6696 - accuracy: 0.7241 - val_loss: 0.6680 - val_accuracy: 0.7335\n",
      "Epoch 1243/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 0.6791 - accuracy: 0.7192 - val_loss: 0.6535 - val_accuracy: 0.7371\n",
      "Epoch 1244/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6692 - accuracy: 0.7253 - val_loss: 0.6580 - val_accuracy: 0.7377\n",
      "Epoch 1245/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6729 - accuracy: 0.7221 - val_loss: 0.6643 - val_accuracy: 0.7394\n",
      "Epoch 1246/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.6781 - accuracy: 0.7235 - val_loss: 0.6622 - val_accuracy: 0.7363\n",
      "Epoch 1247/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.6698 - accuracy: 0.7234 - val_loss: 0.6646 - val_accuracy: 0.7352\n",
      "Epoch 1248/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.6771 - accuracy: 0.7232 - val_loss: 0.6568 - val_accuracy: 0.7408\n",
      "Epoch 1249/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 0.6625 - accuracy: 0.7266 - val_loss: 0.6699 - val_accuracy: 0.7346\n",
      "Epoch 1250/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.6735 - accuracy: 0.7247 - val_loss: 0.6596 - val_accuracy: 0.7394\n",
      "Epoch 1251/2000\n",
      "14152/14152 [==============================] - 5s 331us/step - loss: 0.6692 - accuracy: 0.7250 - val_loss: 0.6674 - val_accuracy: 0.7346\n",
      "Epoch 1252/2000\n",
      "14152/14152 [==============================] - 5s 318us/step - loss: 0.6693 - accuracy: 0.7249 - val_loss: 0.6603 - val_accuracy: 0.7383\n",
      "Epoch 1253/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6687 - accuracy: 0.7250 - val_loss: 0.6651 - val_accuracy: 0.7363\n",
      "Epoch 1254/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.6677 - accuracy: 0.7255 - val_loss: 0.6596 - val_accuracy: 0.7388\n",
      "Epoch 1255/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.6678 - accuracy: 0.7257 - val_loss: 0.6631 - val_accuracy: 0.7363\n",
      "Epoch 1256/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.6741 - accuracy: 0.7214 - val_loss: 0.6571 - val_accuracy: 0.7388\n",
      "Epoch 1257/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.6691 - accuracy: 0.7256 - val_loss: 0.6642 - val_accuracy: 0.7380\n",
      "Epoch 1258/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.6675 - accuracy: 0.7265 - val_loss: 0.6578 - val_accuracy: 0.7408\n",
      "Epoch 1259/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.6735 - accuracy: 0.7228 - val_loss: 0.6664 - val_accuracy: 0.7363\n",
      "Epoch 1260/2000\n",
      "14152/14152 [==============================] - 5s 333us/step - loss: 0.6671 - accuracy: 0.7258 - val_loss: 0.6581 - val_accuracy: 0.7386\n",
      "Epoch 1261/2000\n",
      "14152/14152 [==============================] - 5s 353us/step - loss: 0.6709 - accuracy: 0.7239 - val_loss: 0.6606 - val_accuracy: 0.7352\n",
      "Epoch 1262/2000\n",
      "14152/14152 [==============================] - 5s 355us/step - loss: 0.6678 - accuracy: 0.7222 - val_loss: 0.6577 - val_accuracy: 0.7340\n",
      "Epoch 1263/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 0.6645 - accuracy: 0.7249 - val_loss: 0.6628 - val_accuracy: 0.7332\n",
      "Epoch 1264/2000\n",
      "14152/14152 [==============================] - 5s 328us/step - loss: 0.6690 - accuracy: 0.7267 - val_loss: 0.6594 - val_accuracy: 0.7405\n",
      "Epoch 1265/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 0.6707 - accuracy: 0.7243 - val_loss: 0.6632 - val_accuracy: 0.7354\n",
      "Epoch 1266/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.6734 - accuracy: 0.7255 - val_loss: 0.6576 - val_accuracy: 0.7374\n",
      "Epoch 1267/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 0.6698 - accuracy: 0.7260 - val_loss: 0.6709 - val_accuracy: 0.7337\n",
      "Epoch 1268/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.6708 - accuracy: 0.7243 - val_loss: 0.6612 - val_accuracy: 0.7352\n",
      "Epoch 1269/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.6645 - accuracy: 0.7276 - val_loss: 0.6611 - val_accuracy: 0.7374\n",
      "Epoch 1270/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.6722 - accuracy: 0.7248 - val_loss: 0.6656 - val_accuracy: 0.7337\n",
      "Epoch 1271/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.6652 - accuracy: 0.7246 - val_loss: 0.6533 - val_accuracy: 0.7405\n",
      "Epoch 1272/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.6726 - accuracy: 0.7239 - val_loss: 0.6661 - val_accuracy: 0.7340\n",
      "Epoch 1273/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6642 - accuracy: 0.7271 - val_loss: 0.6650 - val_accuracy: 0.7380\n",
      "Epoch 1274/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.6732 - accuracy: 0.7241 - val_loss: 0.6626 - val_accuracy: 0.7400\n",
      "Epoch 1275/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 0.6701 - accuracy: 0.7260 - val_loss: 0.6636 - val_accuracy: 0.7371\n",
      "Epoch 1276/2000\n",
      "14152/14152 [==============================] - 5s 330us/step - loss: 0.6694 - accuracy: 0.7275 - val_loss: 0.6673 - val_accuracy: 0.7377\n",
      "Epoch 1277/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 0.6669 - accuracy: 0.7255 - val_loss: 0.6569 - val_accuracy: 0.7386\n",
      "Epoch 1278/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.6652 - accuracy: 0.7243 - val_loss: 0.6546 - val_accuracy: 0.7360\n",
      "Epoch 1279/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 0.6680 - accuracy: 0.7256 - val_loss: 0.6579 - val_accuracy: 0.7397\n",
      "Epoch 1280/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.6709 - accuracy: 0.7234 - val_loss: 0.6558 - val_accuracy: 0.7419\n",
      "Epoch 1281/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6672 - accuracy: 0.7303 - val_loss: 0.6604 - val_accuracy: 0.7431\n",
      "Epoch 1282/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6644 - accuracy: 0.7297 - val_loss: 0.6565 - val_accuracy: 0.7377\n",
      "Epoch 1283/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.6708 - accuracy: 0.7231 - val_loss: 0.6576 - val_accuracy: 0.7386\n",
      "Epoch 1284/2000\n",
      "14152/14152 [==============================] - 5s 330us/step - loss: 0.6613 - accuracy: 0.7251 - val_loss: 0.6534 - val_accuracy: 0.7419\n",
      "Epoch 1285/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.6686 - accuracy: 0.7224 - val_loss: 0.6585 - val_accuracy: 0.7366\n",
      "Epoch 1286/2000\n",
      "14152/14152 [==============================] - 5s 329us/step - loss: 0.6679 - accuracy: 0.7275 - val_loss: 0.6570 - val_accuracy: 0.7414\n",
      "Epoch 1287/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.6652 - accuracy: 0.7239 - val_loss: 0.6550 - val_accuracy: 0.7411\n",
      "Epoch 1288/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6722 - accuracy: 0.7249 - val_loss: 0.6582 - val_accuracy: 0.7388\n",
      "Epoch 1289/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6738 - accuracy: 0.7272 - val_loss: 0.6554 - val_accuracy: 0.7391\n",
      "Epoch 1290/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.6719 - accuracy: 0.7236 - val_loss: 0.6551 - val_accuracy: 0.7397\n",
      "Epoch 1291/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.6674 - accuracy: 0.7241 - val_loss: 0.6609 - val_accuracy: 0.7386\n",
      "Epoch 1292/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6664 - accuracy: 0.7241 - val_loss: 0.6538 - val_accuracy: 0.7434\n",
      "Epoch 1293/2000\n",
      "14152/14152 [==============================] - 5s 363us/step - loss: 0.6721 - accuracy: 0.7270 - val_loss: 0.6564 - val_accuracy: 0.7391\n",
      "Epoch 1294/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.6642 - accuracy: 0.7260 - val_loss: 0.6548 - val_accuracy: 0.7400\n",
      "Epoch 1295/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.6585 - accuracy: 0.7296 - val_loss: 0.6562 - val_accuracy: 0.7402\n",
      "Epoch 1296/2000\n",
      "14152/14152 [==============================] - 5s 320us/step - loss: 0.6623 - accuracy: 0.7275 - val_loss: 0.6554 - val_accuracy: 0.7386\n",
      "Epoch 1297/2000\n",
      "14152/14152 [==============================] - 5s 318us/step - loss: 0.6627 - accuracy: 0.7289 - val_loss: 0.6601 - val_accuracy: 0.7366\n",
      "Epoch 1298/2000\n",
      "14152/14152 [==============================] - 5s 357us/step - loss: 0.6621 - accuracy: 0.7281 - val_loss: 0.6559 - val_accuracy: 0.7369\n",
      "Epoch 1299/2000\n",
      "14152/14152 [==============================] - 5s 364us/step - loss: 0.6676 - accuracy: 0.7243 - val_loss: 0.6585 - val_accuracy: 0.7363\n",
      "Epoch 1300/2000\n",
      "14152/14152 [==============================] - 5s 347us/step - loss: 0.6675 - accuracy: 0.7246 - val_loss: 0.6576 - val_accuracy: 0.7386\n",
      "Epoch 1301/2000\n",
      "14152/14152 [==============================] - 4s 318us/step - loss: 0.6654 - accuracy: 0.7284 - val_loss: 0.6583 - val_accuracy: 0.7394\n",
      "Epoch 1302/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.6658 - accuracy: 0.7280 - val_loss: 0.6508 - val_accuracy: 0.7374\n",
      "Epoch 1303/2000\n",
      "14152/14152 [==============================] - 6s 447us/step - loss: 0.6661 - accuracy: 0.7284 - val_loss: 0.6519 - val_accuracy: 0.7414\n",
      "Epoch 1304/2000\n",
      "14152/14152 [==============================] - 5s 353us/step - loss: 0.6656 - accuracy: 0.7255 - val_loss: 0.6616 - val_accuracy: 0.7343\n",
      "Epoch 1305/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 0.6589 - accuracy: 0.7284 - val_loss: 0.6532 - val_accuracy: 0.7400\n",
      "Epoch 1306/2000\n",
      "14152/14152 [==============================] - 5s 335us/step - loss: 0.6655 - accuracy: 0.7261 - val_loss: 0.6524 - val_accuracy: 0.7400\n",
      "Epoch 1307/2000\n",
      "14152/14152 [==============================] - 5s 383us/step - loss: 0.6653 - accuracy: 0.7246 - val_loss: 0.6564 - val_accuracy: 0.7374\n",
      "Epoch 1308/2000\n",
      "14152/14152 [==============================] - 5s 340us/step - loss: 0.6616 - accuracy: 0.7289 - val_loss: 0.6545 - val_accuracy: 0.7374\n",
      "Epoch 1309/2000\n",
      "14152/14152 [==============================] - 5s 357us/step - loss: 0.6720 - accuracy: 0.7254 - val_loss: 0.6490 - val_accuracy: 0.7408\n",
      "Epoch 1310/2000\n",
      "14152/14152 [==============================] - 5s 342us/step - loss: 0.6616 - accuracy: 0.7222 - val_loss: 0.6677 - val_accuracy: 0.7335\n",
      "Epoch 1311/2000\n",
      "14152/14152 [==============================] - 5s 380us/step - loss: 0.6609 - accuracy: 0.7288 - val_loss: 0.6554 - val_accuracy: 0.7374\n",
      "Epoch 1312/2000\n",
      "14152/14152 [==============================] - 6s 434us/step - loss: 0.6631 - accuracy: 0.7272 - val_loss: 0.6522 - val_accuracy: 0.7400\n",
      "Epoch 1313/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 0.6733 - accuracy: 0.7229 - val_loss: 0.6482 - val_accuracy: 0.7414\n",
      "Epoch 1314/2000\n",
      "14152/14152 [==============================] - 5s 355us/step - loss: 0.6645 - accuracy: 0.7297 - val_loss: 0.6547 - val_accuracy: 0.7394\n",
      "Epoch 1315/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.6613 - accuracy: 0.7253 - val_loss: 0.6524 - val_accuracy: 0.7397\n",
      "Epoch 1316/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 0.6597 - accuracy: 0.7289 - val_loss: 0.6493 - val_accuracy: 0.7442\n",
      "Epoch 1317/2000\n",
      "14152/14152 [==============================] - 5s 331us/step - loss: 0.6626 - accuracy: 0.7245 - val_loss: 0.6499 - val_accuracy: 0.7439\n",
      "Epoch 1318/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.6659 - accuracy: 0.7236 - val_loss: 0.6524 - val_accuracy: 0.7391\n",
      "Epoch 1319/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.6622 - accuracy: 0.7287 - val_loss: 0.6602 - val_accuracy: 0.7357\n",
      "Epoch 1320/2000\n",
      "14152/14152 [==============================] - 5s 339us/step - loss: 0.6584 - accuracy: 0.7264 - val_loss: 0.6499 - val_accuracy: 0.7388\n",
      "Epoch 1321/2000\n",
      "14152/14152 [==============================] - 5s 328us/step - loss: 0.6608 - accuracy: 0.7294 - val_loss: 0.6617 - val_accuracy: 0.7369\n",
      "Epoch 1322/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.6612 - accuracy: 0.7275 - val_loss: 0.6541 - val_accuracy: 0.7414\n",
      "Epoch 1323/2000\n",
      "14152/14152 [==============================] - 5s 332us/step - loss: 0.6626 - accuracy: 0.7288 - val_loss: 0.6488 - val_accuracy: 0.7419\n",
      "Epoch 1324/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.6601 - accuracy: 0.7275 - val_loss: 0.6544 - val_accuracy: 0.7352\n",
      "Epoch 1325/2000\n",
      "14152/14152 [==============================] - 4s 313us/step - loss: 0.6602 - accuracy: 0.7267 - val_loss: 0.6530 - val_accuracy: 0.7400\n",
      "Epoch 1326/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.6627 - accuracy: 0.7278 - val_loss: 0.6500 - val_accuracy: 0.7374\n",
      "Epoch 1327/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.6571 - accuracy: 0.7272 - val_loss: 0.6500 - val_accuracy: 0.7397\n",
      "Epoch 1328/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.6586 - accuracy: 0.7294 - val_loss: 0.6633 - val_accuracy: 0.7363\n",
      "Epoch 1329/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 0.6653 - accuracy: 0.7276 - val_loss: 0.6503 - val_accuracy: 0.7402\n",
      "Epoch 1330/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.6601 - accuracy: 0.7251 - val_loss: 0.6588 - val_accuracy: 0.7366\n",
      "Epoch 1331/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 0.6544 - accuracy: 0.7283 - val_loss: 0.6590 - val_accuracy: 0.7369\n",
      "Epoch 1332/2000\n",
      "14152/14152 [==============================] - 5s 318us/step - loss: 0.6565 - accuracy: 0.7292 - val_loss: 0.6510 - val_accuracy: 0.7414\n",
      "Epoch 1333/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 0.6583 - accuracy: 0.7287 - val_loss: 0.6546 - val_accuracy: 0.7383\n",
      "Epoch 1334/2000\n",
      "14152/14152 [==============================] - 5s 385us/step - loss: 0.6639 - accuracy: 0.7283 - val_loss: 0.6494 - val_accuracy: 0.7434\n",
      "Epoch 1335/2000\n",
      "14152/14152 [==============================] - 5s 336us/step - loss: 0.6638 - accuracy: 0.7258 - val_loss: 0.6496 - val_accuracy: 0.7400\n",
      "Epoch 1336/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 0.6613 - accuracy: 0.7284 - val_loss: 0.6588 - val_accuracy: 0.7397\n",
      "Epoch 1337/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.6637 - accuracy: 0.7278 - val_loss: 0.6421 - val_accuracy: 0.7448\n",
      "Epoch 1338/2000\n",
      "14152/14152 [==============================] - 5s 380us/step - loss: 0.6578 - accuracy: 0.7320 - val_loss: 0.6472 - val_accuracy: 0.7400\n",
      "Epoch 1339/2000\n",
      "14152/14152 [==============================] - 5s 380us/step - loss: 0.6562 - accuracy: 0.7293 - val_loss: 0.6563 - val_accuracy: 0.7340\n",
      "Epoch 1340/2000\n",
      "14152/14152 [==============================] - 6s 395us/step - loss: 0.6549 - accuracy: 0.7260 - val_loss: 0.6610 - val_accuracy: 0.7329\n",
      "Epoch 1341/2000\n",
      "14152/14152 [==============================] - 5s 387us/step - loss: 0.6505 - accuracy: 0.7312 - val_loss: 0.6598 - val_accuracy: 0.7337\n",
      "Epoch 1342/2000\n",
      "14152/14152 [==============================] - 5s 378us/step - loss: 0.6597 - accuracy: 0.7264 - val_loss: 0.6522 - val_accuracy: 0.7425\n",
      "Epoch 1343/2000\n",
      "14152/14152 [==============================] - 5s 334us/step - loss: 0.6675 - accuracy: 0.7263 - val_loss: 0.6495 - val_accuracy: 0.7400\n",
      "Epoch 1344/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.6607 - accuracy: 0.7266 - val_loss: 0.6530 - val_accuracy: 0.7425\n",
      "Epoch 1345/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14152/14152 [==============================] - 5s 329us/step - loss: 0.6676 - accuracy: 0.7255 - val_loss: 0.6499 - val_accuracy: 0.7405\n",
      "Epoch 1346/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.6611 - accuracy: 0.7299 - val_loss: 0.6489 - val_accuracy: 0.7408\n",
      "Epoch 1347/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 0.6581 - accuracy: 0.7292 - val_loss: 0.6453 - val_accuracy: 0.7448\n",
      "Epoch 1348/2000\n",
      "14152/14152 [==============================] - 5s 330us/step - loss: 0.6583 - accuracy: 0.7294 - val_loss: 0.6481 - val_accuracy: 0.7400\n",
      "Epoch 1349/2000\n",
      "14152/14152 [==============================] - 5s 338us/step - loss: 0.6622 - accuracy: 0.7292 - val_loss: 0.6435 - val_accuracy: 0.7436\n",
      "Epoch 1350/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 0.6624 - accuracy: 0.7259 - val_loss: 0.6495 - val_accuracy: 0.7419\n",
      "Epoch 1351/2000\n",
      "14152/14152 [==============================] - 5s 326us/step - loss: 0.6638 - accuracy: 0.7290 - val_loss: 0.6543 - val_accuracy: 0.7380\n",
      "Epoch 1352/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 0.6604 - accuracy: 0.7277 - val_loss: 0.6493 - val_accuracy: 0.7414\n",
      "Epoch 1353/2000\n",
      "14152/14152 [==============================] - 5s 329us/step - loss: 0.6512 - accuracy: 0.7316 - val_loss: 0.6525 - val_accuracy: 0.7419\n",
      "Epoch 1354/2000\n",
      "14152/14152 [==============================] - 5s 322us/step - loss: 0.6577 - accuracy: 0.7338 - val_loss: 0.6474 - val_accuracy: 0.7431\n",
      "Epoch 1355/2000\n",
      "14152/14152 [==============================] - 5s 325us/step - loss: 0.6493 - accuracy: 0.7302 - val_loss: 0.6543 - val_accuracy: 0.7349\n",
      "Epoch 1356/2000\n",
      "14152/14152 [==============================] - 4s 316us/step - loss: 0.6590 - accuracy: 0.7304 - val_loss: 0.6498 - val_accuracy: 0.7436\n",
      "Epoch 1357/2000\n",
      "14152/14152 [==============================] - 5s 321us/step - loss: 0.6563 - accuracy: 0.7286 - val_loss: 0.6480 - val_accuracy: 0.7431\n",
      "Epoch 1358/2000\n",
      "14152/14152 [==============================] - 5s 327us/step - loss: 0.6567 - accuracy: 0.7307 - val_loss: 0.6450 - val_accuracy: 0.7417\n",
      "Epoch 1359/2000\n",
      "14152/14152 [==============================] - 5s 323us/step - loss: 0.6606 - accuracy: 0.7281 - val_loss: 0.6496 - val_accuracy: 0.7391\n",
      "Epoch 1360/2000\n",
      "14152/14152 [==============================] - 5s 337us/step - loss: 0.6612 - accuracy: 0.7257 - val_loss: 0.6616 - val_accuracy: 0.7352\n",
      "Epoch 1361/2000\n",
      "14152/14152 [==============================] - 5s 333us/step - loss: 0.6497 - accuracy: 0.7303 - val_loss: 0.6479 - val_accuracy: 0.7425\n",
      "Epoch 1362/2000\n",
      "14152/14152 [==============================] - 5s 347us/step - loss: 0.6625 - accuracy: 0.7260 - val_loss: 0.6582 - val_accuracy: 0.7386\n",
      "Epoch 1363/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.6527 - accuracy: 0.7321 - val_loss: 0.6493 - val_accuracy: 0.7391\n",
      "Epoch 1364/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.6568 - accuracy: 0.7321 - val_loss: 0.6489 - val_accuracy: 0.7400\n",
      "Epoch 1365/2000\n",
      "14152/14152 [==============================] - 5s 328us/step - loss: 0.6598 - accuracy: 0.7289 - val_loss: 0.6535 - val_accuracy: 0.7414\n",
      "Epoch 1366/2000\n",
      "14152/14152 [==============================] - 4s 317us/step - loss: 0.6583 - accuracy: 0.7287 - val_loss: 0.6434 - val_accuracy: 0.7422\n",
      "Epoch 1367/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6551 - accuracy: 0.7312 - val_loss: 0.6485 - val_accuracy: 0.7465\n",
      "Epoch 1368/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.6666 - accuracy: 0.7229 - val_loss: 0.6514 - val_accuracy: 0.7397\n",
      "Epoch 1369/2000\n",
      "14152/14152 [==============================] - 5s 318us/step - loss: 0.6540 - accuracy: 0.7284 - val_loss: 0.6565 - val_accuracy: 0.7388\n",
      "Epoch 1370/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.6513 - accuracy: 0.7295 - val_loss: 0.6588 - val_accuracy: 0.7340\n",
      "Epoch 1371/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.6554 - accuracy: 0.7318 - val_loss: 0.6456 - val_accuracy: 0.7482\n",
      "Epoch 1372/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.6566 - accuracy: 0.7296 - val_loss: 0.6443 - val_accuracy: 0.7459\n",
      "Epoch 1373/2000\n",
      "14152/14152 [==============================] - 4s 315us/step - loss: 0.6470 - accuracy: 0.7343 - val_loss: 0.6412 - val_accuracy: 0.7456\n",
      "Epoch 1374/2000\n",
      "14152/14152 [==============================] - 5s 324us/step - loss: 0.6499 - accuracy: 0.7313 - val_loss: 0.6523 - val_accuracy: 0.7391\n",
      "Epoch 1375/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.6513 - accuracy: 0.7306 - val_loss: 0.6473 - val_accuracy: 0.7445\n",
      "Epoch 1376/2000\n",
      "14152/14152 [==============================] - 5s 343us/step - loss: 0.6574 - accuracy: 0.7296 - val_loss: 0.6508 - val_accuracy: 0.7343\n",
      "Epoch 1377/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6522 - accuracy: 0.7293 - val_loss: 0.6481 - val_accuracy: 0.7405\n",
      "Epoch 1378/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6508 - accuracy: 0.7333 - val_loss: 0.6519 - val_accuracy: 0.7402\n",
      "Epoch 1379/2000\n",
      "14152/14152 [==============================] - 4s 311us/step - loss: 0.6543 - accuracy: 0.7282 - val_loss: 0.6468 - val_accuracy: 0.7434\n",
      "Epoch 1380/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6560 - accuracy: 0.7340 - val_loss: 0.6475 - val_accuracy: 0.7388\n",
      "Epoch 1381/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.6512 - accuracy: 0.7330 - val_loss: 0.6549 - val_accuracy: 0.7374\n",
      "Epoch 1382/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.6586 - accuracy: 0.7296 - val_loss: 0.6444 - val_accuracy: 0.7445\n",
      "Epoch 1383/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6516 - accuracy: 0.7333 - val_loss: 0.6387 - val_accuracy: 0.7453\n",
      "Epoch 1384/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6542 - accuracy: 0.7291 - val_loss: 0.6423 - val_accuracy: 0.7465\n",
      "Epoch 1385/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6507 - accuracy: 0.7314 - val_loss: 0.6467 - val_accuracy: 0.7397\n",
      "Epoch 1386/2000\n",
      "14152/14152 [==============================] - 5s 319us/step - loss: 0.6506 - accuracy: 0.7313 - val_loss: 0.6422 - val_accuracy: 0.7451\n",
      "Epoch 1387/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6524 - accuracy: 0.7306 - val_loss: 0.6473 - val_accuracy: 0.7377\n",
      "Epoch 1388/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6561 - accuracy: 0.7324 - val_loss: 0.6402 - val_accuracy: 0.7504\n",
      "Epoch 1389/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.6501 - accuracy: 0.7352 - val_loss: 0.6428 - val_accuracy: 0.7397\n",
      "Epoch 1390/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.6549 - accuracy: 0.7311 - val_loss: 0.6427 - val_accuracy: 0.7448\n",
      "Epoch 1391/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6580 - accuracy: 0.7350 - val_loss: 0.6446 - val_accuracy: 0.7448\n",
      "Epoch 1392/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6542 - accuracy: 0.7343 - val_loss: 0.6587 - val_accuracy: 0.7394\n",
      "Epoch 1393/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.6504 - accuracy: 0.7333 - val_loss: 0.6485 - val_accuracy: 0.7428\n",
      "Epoch 1394/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.6469 - accuracy: 0.7316 - val_loss: 0.6446 - val_accuracy: 0.7465\n",
      "Epoch 1395/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.6520 - accuracy: 0.7325 - val_loss: 0.6483 - val_accuracy: 0.7453\n",
      "Epoch 1396/2000\n",
      "14152/14152 [==============================] - 4s 310us/step - loss: 0.6486 - accuracy: 0.7317 - val_loss: 0.6484 - val_accuracy: 0.7465\n",
      "Epoch 1397/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6521 - accuracy: 0.7339 - val_loss: 0.6443 - val_accuracy: 0.7462\n",
      "Epoch 1398/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.6491 - accuracy: 0.7334 - val_loss: 0.6405 - val_accuracy: 0.7453\n",
      "Epoch 1399/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6493 - accuracy: 0.7337 - val_loss: 0.6381 - val_accuracy: 0.7459\n",
      "Epoch 1400/2000\n",
      "14152/14152 [==============================] - 4s 306us/step - loss: 0.6438 - accuracy: 0.7357 - val_loss: 0.6493 - val_accuracy: 0.7428\n",
      "Epoch 1401/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6565 - accuracy: 0.7330 - val_loss: 0.6402 - val_accuracy: 0.7445\n",
      "Epoch 1402/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6523 - accuracy: 0.7322 - val_loss: 0.6470 - val_accuracy: 0.7442\n",
      "Epoch 1403/2000\n",
      "14152/14152 [==============================] - 4s 307us/step - loss: 0.6497 - accuracy: 0.7328 - val_loss: 0.6434 - val_accuracy: 0.7417\n",
      "Epoch 1404/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.6532 - accuracy: 0.7272 - val_loss: 0.6419 - val_accuracy: 0.7473\n",
      "Epoch 1405/2000\n",
      "14152/14152 [==============================] - 4s 309us/step - loss: 0.6497 - accuracy: 0.7342 - val_loss: 0.6348 - val_accuracy: 0.7462\n",
      "Epoch 1406/2000\n",
      "14152/14152 [==============================] - 4s 308us/step - loss: 0.6455 - accuracy: 0.7327 - val_loss: 0.6499 - val_accuracy: 0.7434\n",
      "Epoch 1407/2000\n",
      "14152/14152 [==============================] - 5s 362us/step - loss: 0.6468 - accuracy: 0.7344 - val_loss: 0.6429 - val_accuracy: 0.7470\n",
      "Epoch 1408/2000\n",
      "14152/14152 [==============================] - 5s 355us/step - loss: 0.6555 - accuracy: 0.7314 - val_loss: 0.6391 - val_accuracy: 0.7470\n",
      "Epoch 1409/2000\n",
      "14152/14152 [==============================] - 5s 382us/step - loss: 0.6409 - accuracy: 0.7374 - val_loss: 0.6448 - val_accuracy: 0.7431\n",
      "Epoch 1410/2000\n",
      "14152/14152 [==============================] - 5s 359us/step - loss: 0.6435 - accuracy: 0.7341 - val_loss: 0.6448 - val_accuracy: 0.7445\n",
      "Epoch 1411/2000\n",
      "14152/14152 [==============================] - 5s 347us/step - loss: 0.6523 - accuracy: 0.7307 - val_loss: 0.6400 - val_accuracy: 0.7465\n",
      "Epoch 1412/2000\n",
      "14152/14152 [==============================] - 5s 358us/step - loss: 0.6435 - accuracy: 0.7358 - val_loss: 0.6381 - val_accuracy: 0.7507\n",
      "Epoch 1413/2000\n",
      "14152/14152 [==============================] - 5s 350us/step - loss: 0.6478 - accuracy: 0.7360 - val_loss: 0.6390 - val_accuracy: 0.7428\n",
      "Epoch 1414/2000\n",
      "14152/14152 [==============================] - 5s 343us/step - loss: 0.6591 - accuracy: 0.7282 - val_loss: 0.6424 - val_accuracy: 0.7411\n",
      "Epoch 1415/2000\n",
      "14152/14152 [==============================] - 5s 347us/step - loss: 0.6458 - accuracy: 0.7335 - val_loss: 0.6458 - val_accuracy: 0.7425\n",
      "Epoch 1416/2000\n",
      "14152/14152 [==============================] - 5s 346us/step - loss: 0.6534 - accuracy: 0.7328 - val_loss: 0.6397 - val_accuracy: 0.7484\n",
      "Epoch 1417/2000\n",
      "14152/14152 [==============================] - 5s 345us/step - loss: 0.6494 - accuracy: 0.7317 - val_loss: 0.6448 - val_accuracy: 0.7414\n",
      "Epoch 1418/2000\n",
      "14152/14152 [==============================] - 5s 346us/step - loss: 0.6565 - accuracy: 0.7306 - val_loss: 0.6469 - val_accuracy: 0.7445\n",
      "Epoch 1419/2000\n",
      "14152/14152 [==============================] - 5s 344us/step - loss: 0.6475 - accuracy: 0.7337 - val_loss: 0.6465 - val_accuracy: 0.7431\n",
      "Epoch 1420/2000\n",
      "14152/14152 [==============================] - 5s 344us/step - loss: 0.6412 - accuracy: 0.7398 - val_loss: 0.6500 - val_accuracy: 0.7408\n",
      "Epoch 1421/2000\n",
      "14152/14152 [==============================] - 5s 345us/step - loss: 0.6519 - accuracy: 0.7341 - val_loss: 0.6446 - val_accuracy: 0.7431\n",
      "Epoch 1422/2000\n",
      "14152/14152 [==============================] - 5s 348us/step - loss: 0.6457 - accuracy: 0.7353 - val_loss: 0.6380 - val_accuracy: 0.7484\n",
      "Epoch 1423/2000\n",
      "14152/14152 [==============================] - 5s 347us/step - loss: 0.6489 - accuracy: 0.7325 - val_loss: 0.6466 - val_accuracy: 0.7400\n",
      "Epoch 1424/2000\n",
      "14152/14152 [==============================] - 5s 347us/step - loss: 0.6586 - accuracy: 0.7338 - val_loss: 0.6487 - val_accuracy: 0.7448\n",
      "Epoch 1425/2000\n",
      "14152/14152 [==============================] - 5s 347us/step - loss: 0.6555 - accuracy: 0.7286 - val_loss: 0.6378 - val_accuracy: 0.7490\n",
      "Epoch 1426/2000\n",
      "14152/14152 [==============================] - 5s 346us/step - loss: 0.6432 - accuracy: 0.7366 - val_loss: 0.6474 - val_accuracy: 0.7453\n",
      "Epoch 1427/2000\n",
      "14152/14152 [==============================] - 5s 351us/step - loss: 0.6486 - accuracy: 0.7341 - val_loss: 0.6434 - val_accuracy: 0.7425\n",
      "Epoch 1428/2000\n",
      "14152/14152 [==============================] - 5s 346us/step - loss: 0.6472 - accuracy: 0.7333 - val_loss: 0.6374 - val_accuracy: 0.7476\n",
      "Epoch 1429/2000\n",
      "14152/14152 [==============================] - 5s 347us/step - loss: 0.6540 - accuracy: 0.7303 - val_loss: 0.6369 - val_accuracy: 0.7436\n",
      "Epoch 1430/2000\n",
      "14152/14152 [==============================] - 5s 346us/step - loss: 0.6514 - accuracy: 0.7323 - val_loss: 0.6440 - val_accuracy: 0.7462\n",
      "Epoch 1431/2000\n",
      "14152/14152 [==============================] - 5s 365us/step - loss: 0.6483 - accuracy: 0.7333 - val_loss: 0.6360 - val_accuracy: 0.7484\n",
      "Epoch 1432/2000\n",
      "14152/14152 [==============================] - 5s 345us/step - loss: 0.6456 - accuracy: 0.7380 - val_loss: 0.6445 - val_accuracy: 0.7442\n",
      "Epoch 1433/2000\n",
      "14152/14152 [==============================] - 5s 349us/step - loss: 0.6461 - accuracy: 0.7362 - val_loss: 0.6380 - val_accuracy: 0.7473\n",
      "Epoch 1434/2000\n",
      "14152/14152 [==============================] - 5s 348us/step - loss: 0.6490 - accuracy: 0.7316 - val_loss: 0.6476 - val_accuracy: 0.7402\n",
      "Epoch 1435/2000\n",
      "14152/14152 [==============================] - 5s 349us/step - loss: 0.6477 - accuracy: 0.7331 - val_loss: 0.6423 - val_accuracy: 0.7408\n",
      "Epoch 1436/2000\n",
      "14152/14152 [==============================] - 5s 346us/step - loss: 0.6426 - accuracy: 0.7333 - val_loss: 0.6423 - val_accuracy: 0.7462\n",
      "Epoch 1437/2000\n",
      "14152/14152 [==============================] - 5s 351us/step - loss: 0.6479 - accuracy: 0.7319 - val_loss: 0.6390 - val_accuracy: 0.7516\n",
      "Epoch 1438/2000\n",
      "14152/14152 [==============================] - 5s 348us/step - loss: 0.6587 - accuracy: 0.7349 - val_loss: 0.6524 - val_accuracy: 0.7397\n",
      "Epoch 1439/2000\n",
      "14152/14152 [==============================] - 5s 349us/step - loss: 0.6476 - accuracy: 0.7306 - val_loss: 0.6430 - val_accuracy: 0.7434\n",
      "Epoch 1440/2000\n",
      "14152/14152 [==============================] - 5s 346us/step - loss: 0.6440 - accuracy: 0.7369 - val_loss: 0.6434 - val_accuracy: 0.7465\n",
      "Epoch 1441/2000\n",
      "14152/14152 [==============================] - 5s 348us/step - loss: 0.6459 - accuracy: 0.7357 - val_loss: 0.6398 - val_accuracy: 0.7459\n",
      "Epoch 1442/2000\n",
      "14152/14152 [==============================] - 5s 348us/step - loss: 0.6477 - accuracy: 0.7346 - val_loss: 0.6354 - val_accuracy: 0.7482\n",
      "Epoch 1443/2000\n",
      "14152/14152 [==============================] - 5s 353us/step - loss: 0.6451 - accuracy: 0.7357 - val_loss: 0.6354 - val_accuracy: 0.7496\n",
      "Epoch 1444/2000\n",
      "14152/14152 [==============================] - 5s 347us/step - loss: 0.6509 - accuracy: 0.7341 - val_loss: 0.6378 - val_accuracy: 0.7496\n",
      "Epoch 1445/2000\n",
      "14152/14152 [==============================] - 5s 351us/step - loss: 0.6496 - accuracy: 0.7309 - val_loss: 0.6349 - val_accuracy: 0.7501\n",
      "Epoch 01445: early stopping\n",
      "CPU times: user 2h 28min 53s, sys: 26min 2s, total: 2h 54min 55s\n",
      "Wall time: 1h 52min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(12345)\n",
    "set_random_seed(12345)\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64, activation='relu',\\\n",
    "                input_dim=train_features.shape[1]))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "# add output layer\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "opt=keras.optimizers.Adam(learning_rate=1e-6, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# enable early stopping based on mean_squared_error\n",
    "earlystopping=EarlyStopping(monitor=\"val_loss\", patience=40, verbose=10, mode='auto')\n",
    "\n",
    "one_hot_train_labels = keras.utils.to_categorical(train_labels, num_classes=4)\n",
    "one_hot_eval_labels = keras.utils.to_categorical(eval_labels, num_classes=4)\n",
    "# fit model\n",
    "result = model.fit(train_features, one_hot_train_labels, epochs=2000, batch_size=16,\\\n",
    "                   validation_data=(eval_features, one_hot_eval_labels),callbacks=[earlystopping])\n",
    "# get predictions\n",
    "eval_pred = model.predict(eval_features)\n",
    "\n",
    "test_pred=model.predict(test_features_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3gVVfrA8e+bTkIJSWgJJaFIFZAmSBFQqQr2gr2hYt8FwbXvb11727WwFmwINhBQUVCkKEWKAlKlSAm9JaSQes/vj5kk9yY3yU3Izb1J3s/z5MnMmTkzbyj3zZxz5hwxxqCUUkoVFuDrAJRSSvknTRBKKaXc0gShlFLKLU0QSiml3NIEoZRSyi1NEEoppdzSBKGUUsotTRBKKaXc0gShlI+IRf8PKr+l/zhVjScik0Rkh4ikiMgmEbnE6djtIrLZ6Vg3u7yZiMwUkSMickxEXrfLnxSRqU7140XEiEiQvb9IRJ4WkaVAOtBSRG52usdOEbmjUHyjRWStiJy04xwmIleIyJpC5/1dRGZ5709K1TSaIJSCHUB/oB7wFDBVRJqIyBXAk8ANQF1gFHBMRAKBb4DdQDwQB3xahvtdD4wF6tjXOAxcaN/jZuAVp0TUC/gImABEAgOAXcAcIEFE2jtd9zrg4zL95EqVQBOEqvGMMV8YY/YbYxzGmM+AbUAv4DbgeWPMKmPZbozZbR+LBSYYY9KMMRnGmF/KcMsPjDEbjTE5xphsY8y3xpgd9j0WA/OxEhbArcAUY8wPdnz7jDFbjDGZwGdYSQER6YiVrL6pgD8SpQBNEEohIjfYTThJIpIEdAJigGZYTxeFNQN2G2NyynnLvYXuP1xEVojIcfv+I+z7593LXQwAHwJjRESwnko+txOHUhVCE4Sq0USkBfAOcA8QbYyJBDYAgvVB3spNtb1A87x+hULSgHCn/cZuzsmfQllEQoEZwItAI/v+c+37593LXQwYY1YAWVhPG2PQ5iVVwTRBqJouAusD+wiAiNyM9QQB8C4wXkS62yOOWtsJZSVwAHhWRCJEJExE+tp11gIDRKS5iNQDHi7l/iFAqH3/HBEZDgxxOv4ecLOInCciASISJyLtnI5/BLwO5JSxmUupUmmCUDWaMWYT8BKwHDgEnAkstY99ATwNTANSgFlAlDEmF7gIaA3sARKBq+w6P2D1DawH1lBKn4AxJgW4D/gcOIH1JDDH6fhK7I5rIBlYDLRwusTHWAlNnx5UhRNdMEipqktEamGNgupmjNnm63hU9aJPEEpVbXcBqzQ5KG9w18mmlKoCRGQXVmf2xT4ORVVT2sSklFLKLW1iUkop5Va1amKKiYkx8fHxvg5DKaWqjDVr1hw1xjRwd6xaJYj4+HhWr17t6zCUUqrKEJHdxR3TJiallFJuaYJQSinlliYIpZRSblWrPgh3srOzSUxMJCMjw9eheFVYWBhNmzYlODjY16EopaqJap8gEhMTqVOnDvHx8VizIlc/xhiOHTtGYmIiCQkJvg5HKVVNVPsmpoyMDKKjo6ttcgAQEaKjo6v9U5JSqnJ5NUHYa+duFZHtIjLJzfEJ9kIta0Vkg4jkikiUfWyXiPxhHzutsavVOTnkqQk/o1KqcnktQdjr9r4BDAc6ANeISAfnc4wxLxhjuhpjumLNm7/YGHPc6ZRB9vEe3opTKaWqpNxs+O1jcDi8dgtvPkH0ArYbY3YaY7KwFnUfXcL51wDTvRiPTyQlJfHmm2+Wud6IESNISkryQkRKKb+2dxX8/olr2fov4NNrXct+eRXm3ANz/24dd+RWeCjeTBBxuK69m2iXFSEi4cAwrKUX8xhgvoisEZGxxd1ERMaKyGoRWX3kyJEKCLtiFZcgcnNL/sucO3cukZGR3gpLKeWv3jsfZo9zLZt5G2z5BnKyYPM3sGcFHLeXKl89BX543CsJwpujmNw1ihc3dexFwNJCzUt9jTH7RaQh8IOIbDHGLClyQWPeBt4G6NGjh99NTTtp0iR27NhB165dCQ4Opnbt2jRp0oS1a9eyadMmLr74Yvbu3UtGRgb3338/Y8dauTBv2pDU1FSGDx9Ov379WLZsGXFxccyePZtatWr5+CdTSlWI1CPwRk+4/iuIPavo8cNbCrb/5XbKJOh0KQSFVHho3kwQiUAzp/2mwP5izr2aQs1Lxpj99vfDIvIVVpNVkQRRFk99vZFN+0+eziWK6BBblycu6ljs8WeffZYNGzawdu1aFi1axMiRI9mwYUP+cNQpU6YQFRXFqVOn6NmzJ5dddhnR0dEu19i2bRvTp0/nnXfe4corr2TGjBlcd911FfpzKKV85LPr4NQJWPw8DH26oPyl9tBqMKydWvo12o/ySmjebGJaBbQRkQQRCcFKAnMKn2Qv7H4uMNupLEJE6uRtYy3ivsGLsVaaXr16ubyr8J///IcuXbrQu3dv9u7dy7ZtRRcGS0hIoGvXrgB0796dXbt2VVa4SqmKkJsDiWvAGNi70voOkHYM9q6wtnf9Am/1K6iTst+z5AAQ06Zi47V57QnCGJMjIvcA84BAYIoxZqOI3Gkfn2yfegkw3xiT5lS9EfCVPXQzCJhmjPn+dGMq6Tf9yhIREZG/vWjRIn788UeWL19OeHg4AwcOdPsuQ2hoaP52YGAgp06dqpRYlVIeSDkEwWEQVq+g7PuHITsdLnrN2p89DtZ/Bn3vh6WvQe9x0KQrNOtZUCfzNFo3wqPKX7cEXn2T2hgzF5hbqGxyof0PgA8Kle0EungztspSp04dUlJS3B5LTk6mfv36hIeHs2XLFlasWFHJ0SlVgxzdBhIA0a2KP2f3cnh/GNy5FBp3Kvlar/eAKz6EL26E8Bh4aEfB8RX2wJRBj0BEAys5gJUcnI/7uWo/1YavRUdH07dvXzp16kStWrVo1KhR/rFhw4YxefJkOnfuTNu2bendu7cPI1WqCjmxC9ZOh/rx0PUaz+q8br9O9WRy8eds+cb6vmOBlSD2r4VGHa2EUK8phESAccCmWdZ5X9xofU8/aiWXP7+DaKfmnhfbwGXvleUnc6/VYNjxk/tjQWGnf/1iaIKoBNOmTXNbHhoaynfffef2WF4/Q0xMDBs2FHS/jB8/vsLjU6rKOHXCar9/zamBISQcWg6CVe9A+nHY9TPcYY9nyUyBj0bD0H8XnL/gn9DpMgiPtp4oDqyDqJZWAlj+unWOIxf2/ApThljnbnAegV+M94e5L59xa/l+VoAJO2DnImh9PjzXoqC8992w4g1rO7Ru+a9fCk0QSin/lXbMal/Pm0rmuQSKjJb//AZoOdD6IM2zayk0bA9/zoN9a2DK0IJjP79kfZXEkWslB/AsOXhLRAycebm1/WQyvSd9xAvB/6PfOfeSXieBiB/Gw+BHvHb7aj9Zn1Kqiji6DbbMtaaOcDjg6HZ4oSUseRFO7LZfBCvmVSfn5ADwwQh4PgFm3Vm+WI5sLl89T3S6zLPzJu0BICM7l799vpbDJzM4SDTXZ/+DGdty6fh1LL9cu4M98Vd6LVQxxu/eLSu3Hj16mMJrUm/evJn27dv7KKLKVZN+VlUNOByQvBfq200nTzqNAmo5yOpnOPGXT0IrkxtmW81Ynpqw00p8eWrVt5rO8ox8GdO0B9KkC8u2H2XviXQmzvijxEuue3wI9cLLtxaMiKwpbr47fYJQSnnPrLutZp7C9v8O/6wPr3WG4zshK931+M6F/p8cRrxoJYeWA623oEsy7teC7dA6rsdq1c/fdMQPYJrjAhJeS+ThmesZ8+6vpSYHgMsmL/M87jLQBKGUqliz7oapl0FyovWi1zQ3TSBf31+w/c3f4N9NKi++8hi72HW//UXQ63YrOYA1yuhqp8kgwqOt0VJXT4OHE6Fhu4JjgcE8HPUa39QbYyWWloPyDz20rT3/+MpKCNNXOk9lV7JAL033r53USqmKlff27ytOL6YaA398AcG1IOYMa+RQnp0LKyeu+P7WCCeAKz+C9GNWHLuXwdE/Xc89+06r2WfU69YcR8ZAvWZWP0jb4a6jovK0GwGPHYV5j0C36+2ykQA4HIYT4S2Jyj2KiDB9fwOmcyEH9sUTXO9OPstszVbTDEc5f2evFRJYrnql0QThZUlJSUybNo1x48aVfnIhr776KmPHjiU8PNwLkSlVTllpEBgKgWX4+HiqnDMTT9wFz8V7dm7vcdYH8u5l0O9B+L8Y1+ssetZKEDd9C/H9itbPSi94krngnxBUMIMBIvCg62w/6/Ym0SmuHoEBTr+9BwbDiOeLXHr5zmPcdvxRBNjkVP703LzO8BZF6pTFsbTM06pfHG1i8rLyrgcBVoJIT08v/USlKtO/Y+HTMdZv1Sd2w+y7IeOk1en8yysVd5+h/7ba5wdMsPYbdoDbf4K+DxScE9cDBj8KdeOg913WB/+5D1kf1M3sF08fP25d5/yn4PIp0KKv+/uFhFsvtTVoB4Elz4y662gao99YyqOzNpCcns3glxaxdm8SWTkOlu04Svykb/nn15v474Jt/O3ztUxbuYdThJFOGPGTvvXox186aTD3n1f8HEsDzmhAp7i6RIQE0jO+Ck61oVyn+77gggto2LAhn3/+OZmZmVxyySU89dRTpKWlceWVV5KYmEhubi6PPfYYhw4dYv/+/QwaNIiYmBgWLqykx3ClCjvyJ+xZbn1Ax1qTRrJtHrzZp2A46M4lkLyn4u7Z41boc7e13fFSWPICXP6+1ZYf192essLA7Qusc/KSiLMxn0HqYQiwm1+Cw0ofYnrm5eR2vAzjMAQFFm3XT0rPIivXweYD1rxJX6zeS8fYuuw8ksbFbyx1OXfK0vJ3sm/651DCQ4IIsPsWerSoz+rdJzg7IYpf/7JWRZhyYw+CAr37O37NShDfTYKDpY8IKJPGZ8LwZ4s97Dzd9/z58/nyyy9ZuXIlxhhGjRrFkiVLOHLkCLGxsXz7rfWbRXJyMvXq1ePll19m4cKFxMTEFHt9pbwiJxNWvAWtBsH/Brg/x/ldgYpIDvesseYsatwJWl9QUN6oQ9HpMf62CdKOlny9WpHWlwe2H05l7/F0OsbV5e5PfmPVrhPsenYkP205RIvoCA6fzOR4WhZ3T/vNpV6Ow/DorIqZaHr23X2Zv+kg44e0zV9jPirCGro6qF1D4urX4oHzzyAlI5vwkCCvJweoaQnCx+bPn8/8+fM56yxrUZDU1FS2bdtG//79GT9+PBMnTuTCCy+kf//+Po5UVXnpxyE3C+o0tvY3f2PNF9T9JtfzjCl4Sxms9vtProSz74CfX4Qfn6iYeOo0sZp4vnJaHPLiya4vskU28/yt4Lqx1lcZHD6Zwe7j6fSMjyIzJ5eQwID8D+LzX15c5PzJi3fw7HdbipR7S5dmkXRp5prQrunVHGN/D66EhFBYzUoQJfymXxmMMTz88MPccccdRY6tWbOGuXPn8vDDDzNkyBAef/xxH0Soqqz04xAcbjWjgDVJnCMHHjlkTTv9mb2ecepha1rqZr2sdQm+e8hqwukw2ipb/DxkpRQ/MVxp4npYTwBrPrD2Ww6yRim1u9Ca6wisKa8HPWJ1Ap95BWSnQdJe107hctqXdIq1e5IY2dl12OyCzYe49UPrJdppt53NmHd/5freLWjbuA7Xnt3c7bUqIjl8fU8/rvjfMjKyHTxxUQd+3HyIyPAQxvRqTlRECC/O28qCLYeLrR8UGMANfeJPO47yqlkJwgecp/seOnQojz32GNdeey21a9dm3759BAcHk5OTQ1RUFNdddx21a9fmgw8+cKmrTUyqVM8n2B22j0BwhJUcwJqD6MDagvMW2iuWSYA1KynAxpnWl7P9rk0pxTrrevj9Y2v7pm+tforwKGuW1R+ftJJFn7utzuPgWnDNZ9D6PKsTGayRUIH1oHG94u7gkT3H0hnwQkE/3dCOw8nIcRAgsHZPUn5yABjzrvXS2scrdgNUSBPRkA6NiKtfi46x9Rj/RcEQ3ujaIWx6ahgiICLc3DfBpd57N/X0uNPaFzRBeJnzdN/Dhw9nzJgx9OnTB4DatWszdepUtm/fzoQJEwgICCA4OJi33noLgLFjxzJ8+HCaNGmindTKknHSWlgm77dxgGx7kal9q+HjS1zPd04OzvKSQ1mcfRf89qH18lfLgdabznViCxKE89DRjpfCzy/DWTdAgzMKytsWM+Oph0b+52cu69aUW/olMOK1n6kTFsRnd/Rh8Z+uv4U/P28rby/ZeVr3ylMnLIiUDCvhPjqyPbf1b0lGdi5nPjmP7FzDuzf04PwO1jT+SelZ+fWWTBhEbGTpa8c/fmEHGtY9/acnb9C5mKqRmvSz1lhvnA1Htlidtkl74LPri08CFeHMK+GPz63tsYsg9qyi5yTvA0e29dRQgRwOw7SVe9h9LI37zmtDnbDg/N+2b+uXwLu/WKOEru7ZjE9Xef7WcUnuGtiKtxYVLPzTuWk9xg1szaSZ63n/pp50aRpJgNN7D0dTM4mp7frhvuNIKnGRtQgL9s7LaxWtpLmY9AlCqarkiN0uPmWYNfS0onS81GpmGvYcHN4Ih7dYw0l73GKNVup5GzQuZpHHenHlvu22QymsS0xmxJmNOZaaxR/7ktly4CSD2zdyGTYaERrk8k5AXnIAypwcggKEHEfRX4yb1Atj3MBWjOnVnLun/cb6xGQ6xdVjWKfGDOvU2O21CicHgFYNapcpHn+mTxDVSE36WWucDTPgy1tO/zqj/gtz7rW2u1wD66Zb00l0ux4ObbKGlFaivCeCBnVCOZLinbeB88TUDuHxizoyqkts/n2n3no2Hy3fxd+GnEG7xgUL7+Q6DG8u3M6NfeOpG1a+WVKrihr/BGGMyR/OVl1Vp0Rf42SmWkNNQyJcy7950HpD2JEDi545/fvkJYK8BDHsGbjEaYn4Ck4OJzOymbfhIFf0aJZflpyezcb9yQQGCKt2Hc8v91ZyuKZXcyYOa0tAgLj9oO/bOpp+bYoOAgkMEO4t4S3mmqLaJ4iwsDCOHTtGdHR0tU0SxhiOHTtGWJj31qZVXvRMnNXZ2/tOqxO6RR9rNtTTNew5+H5iwX6zs12PBxdKSKfp2/UHWLbjKJHhwZwZF8mdU9cAEBYcyIAzGvDN+v088lXFvFTmTkJMBH8dTXMpe+bSM92ee+3ZzZmzdn+1/UyoKNW+iSk7O5vExEQyMjJ8FFXlCAsLo2nTpgQHV+/H4Srr8Gb4cBTc9iOseBOiW1sL4mycBScTT+/aweFw49fWHELP2P0Bo/4L3W5wf/5Xd8G6aUXfTi6Ht5fsYOqKPUy5qafbl83yNI8KZ8/xip9XLDBAWPXI+WRk5xIbWYt1e5M4kZ7F5MU7eGpUJ9o2rlP6RWq4kpqYqn2CUMqnEtdYC+LMvM3aD61rDVMtr2a9Ye8KazukNnS4GC62F6/PzYH/i4bGneHOn4u/hiMXcjKKNmmVwuEwJJ/Kpn6ENZFdrsPQ6h9zy/NTlNlTozryxJyNACwaP5D9SacY8+6vjBvYioeGtSultipJje+DUKrS/DnPelroZ884+u5g1+OeJofed1svlE29FNoMtSbHe3CTNWJo02xY8yFcX+jltsAga4WzRu6bVfIFBJYpOew6msZtH62mZ3x9pq/cy0tXdCE4KIDDJ737VP7zQ4O4+I2ltG9SlxvPieeKHk0JECEsOJD4mAh2PTvSq/dX+gShVMXKW1e5zz1wzr3wUlvP6/Yfb81/BPBEkuscSTmZFTIVRXl8sPQvnvx6U+knltEdA1qyYX8yS7cf49Z+CRxJyeSZS88kPCQQh8F1nQXlNfoEoVRlW/669VWaqFZwn9O0Fj+/aPUpFO489UJy2HU0jY9X7ObuQa2Z+Vsikxfv5MlRHTiRlsWRlEx2HUtnzrr95bp2y5gIdhbqMAaoHRpEaqb1VvLDI9pjjCEr10FokOtLZW5m2lY+oAlCqfLKzYbfPoJuN8LbAyGuW9mvMeYz1/0JO8u2UttpuOWDVew8msbUFbvJzLGm3rhn2u8Vcu35Dw5g0sw/+HKN1QF/dkIU3VrUp13jOtz/6Vq+u9+asVhEiiQH5T80QShVXivfgXkPW4vXJO2GQx6sNXLHz5BywOqs3jwHYgqNtY+IrvAw9x5PJyoihIjQIPYeTycoUNh5JC3/N/y85FBWwzs15rsNBwG449yWrNl1gtW7T3BjnxYEBQbw4hVd2HLwJNf3bsFVPQtmTD33jAZEhpe8YpvyD5oglCqPv362kgNYyaEk3W6wnjTAWmCqSWdru0Uf78UH3Pz+ShZuPQJAz/j6XNqtKQ/PPL0Fsz66pRdRESFk5jjo1jyShIfnUis4kIeHW2/wF+7T/ObeomubaHKoOryaIERkGPAaEAi8a4x5ttDxCcC1TrG0BxoYY46XVlcpn3DkWt/n3FPyeedOhEH/gGM7ILoVDH8B0o8V7VvwEmNMfnIAWLXrBKt2nSixzoShbXlh3tZij/8ycRBN64e7lL18ZRfOal4/f19fPKtevDaKSUQCgT+BC4BEYBVwjTHG7XAIEbkIeNAYM7isdfPoKCZVIU4egC9vthawP3UCNs2CJl1h1jjI9ODlsgv+D/re5/04bSkZ2Tz05XqST2Uz5aaeZOc66PPMT/mdwZ74x4h23N6/JTn2uw7pmblsPniS6IgQ/v7FOnYfS+evZ0ZoAqiGfDWKqRew3Riz0w7iU2A0UNyH/DXA9HLWVer07FoKzXtb7wh884A1U+orZZibqPX5sP3HosNTK9CJtCy+XJPI9X1a8OPmQ/RKiKJ+eAiXvLmM7YdTAWj32PfluvaAMxogIgQHijVDaW1oHm09LXxzbz/2HE/X5FADeTNBxAHO8/AmAme7O1FEwoFhQN5ze1nqjgXGAjRv7n7pQKWKlZMJ/2pYsB9az7OnBGd3r7LWU047WqHJ4UhKJpNmrCc0OIBzWsXkr3z29NzN5b7mkgmD8ldeW/XI+Rw6mcH8jQdp07D4KSnqhAXTMfb0VnxTVZM3E4S7/ynFtWddBCw1xuRN7+hxXWPM28DbYDUxlTVIVYMZU3QKbU+Sw4Sd1mijVe/B3AkQ1dIamhrZrPS6Hth+OJVWDSLo+fSP+WVz/zh42tf97zVn0Tw6nE5xdbmsW1Ma1AmlQZ1QOsXph79yz5sJIhFw/h/TFCjurZurKWheKmtdpUpmDGSfgpBwa2rtY9tg63eQtBe2fOO+zuVTCpJHx0sAsRbUuXxKwVDUnrdaXxVo7d4kl4VyPHXTOfGIwIGkDPq1sZ42xg1sxVuLd5DXzXhRl1jA/cgipdzxZoJYBbQRkQRgH1YSGFP4JBGpB5wLXFfWukq5tWMhzLgN7l8Lu5fBvEespOCJ2o1g/J/Wdl6CGPU6fH2/te2lQR3HUjMRkXIlh8UTBtIi2nVupXNaRdM8Kpz7zmvDpgMnCdJpK1Q5eC1BGGNyROQeYB7WUNUpxpiNInKnfTxvpZJLgPnGmLTS6norVlWNfP0ArHnf2p5xO/z5Xel1RrwIsd2sdZWjnV5cu20BrJ1mTWwX3coqC4+qkDB3HU3j17+O0b1FFBe8stjjvHPHgJb8b8lOl7LmUeFFzmtpL3sZFAjdnIahKlUWOlmfqjocDsjNsraDQl07hHf9Ah94OLunc/NR/QTrSaM0udnWKKW2w8sWs+2577eQkpHNoyM7kJ3r4Kx//uB2XeTCAgRu6BPPk6M65pfFT/qWlg0i+OnvA8sVi1LOdLI+VT18+yCs+aBo+bDn4OeXPL9Op8tg/+/QuAt0vsKzOoHBJSaHkxnZRZa0TM/KITvHkJqVw1uLdgAwdcWeUm81464+zPxtH5/8uocnLurIjefEuxyffXdfmrl5alCqommCUP5t7TRr9bXIFu6TA7guq1mStiMLprcY8q8KCQ9g55FUBr+0mGcvPZOrexUMtb7g5SXsSzpV5ut1bxHFavut55jaRWdx7dIssvzBKlUGmiCU/9q3BmbdVTHXeuivCus/KGznEav77PuNB7m6V3NyHYaOT3xPRnbpk+Bd2LkJq3Yd59DJTG7tl8Aoe6TRbf1b0iI6nKEdG3slZqU8oQlC+a+V75Tt/AEToOft8NIZ1v6tP0BYPesJJMA7U0p/98cBFv9pzXm0aOsRBr+0iIZ1QotNDiGBAWTlWsfevr47Q+wEkJ3rIDgwIP+8wABhWKcmXolZKU9pglD+4dQJ62v2PRAeDX3uhnXTS6/nbNAjVsf1eU9Yw1Wb9fJOrFjrM7/0w1beWLjDpXznkbT8Jwp3Pr2jN9sPpdIjvn7+SCPAJTko5S80QSjfOboNajeEwBB4Lt712OY57utc/BYse92aKbVJF5AAa86kXmMLRjX1/5tXw1645TA3f7CqxHPCggP49R/n0+Wp+QDs/PcIsh3Wymk67FRVFZogVOU6tgM+uBBGvgSfXgMBQeAoYdbRRw8XzJV05pXQdYz15Wzcr1C/hfditp3KymXBlkOlrrr22tVdOb99IyJCg3j+ss40qBtKQIAQ6qVmLqW8RROE8j5jYOdCCIuE1e9Byn4rOUDJyaF2I9e1mC8rpk+iYbuKi9WWk+tg+c5j1A4NYn9SBn1aRdPt/34otd7957VhdNe4/P0re1bM/ExK+YImCOV9m+fA5zeUvd4t8yo+Fg/996ftvLagYHqOmwq9i+Bs3gMDiAgNZNKMP7i5b/HnKVXVaIJQ3nek+FXKSpQ3LHXcCkjeV3HxuJGd6yA9K5c9x9JZvy/JJTkAfLBsV5E6M+46h8b1woiLrAXA1NvczkivVJWlCUJVvKx0OLDO+oCfcy/s/bXk8+9fDz8+ac2WCtD8HDjrOmuIKkDD9tZXBdt84CQN6oRSPzyENo94MGeTk+ZR4XRvoZ3NqnrTBKEq3hc3wrb5JZ9z6bsw8zZru34L683murHQ/SZrfYVK6NAd/trPHp0XF1nL5Y3o+Ohwvn9ggLfCUspvaIJQFWvn4tKTA0DHiwsSBEC9OBj6tPfiAtYnJtG2cR0GvrCIA8kZHtWJDA9m6aTBfLZqDxNn/OF2am2lqitNEOr0GQOz74aoBPjJgzmOJu6yJr+7biY4cr0eHsC2QymMer1say388OAAou25kK7q2ZyreuqStqpm0QShyu/4X5B6CLLSYBPeD5AAAB3gSURBVO0nnterZbfdtz7PK2EdS83kisnL+d/13WndsDYiwsj//uJR3fjocHYdS0cE2jQqfp1mpWoCTRCq/P7Ttex1Wp9f8XE4eWPhdl6YZ42auv69lRw8mUHvllFk5bifG+mxCztwY58W/Oen7fxnwTb+dfGZnNGoNkE69YVSmiBUOWV70Ibf8zar+WnEC5CdDhJorQtd3lvmOvhuw0Eu6twEEdclNGev3cfstfv5acvh/LKDJ60YV+w87nLu85d35kRaFuEhgVzfJx6Aewe35qxmkfRrE1Pu+JSqbjRBqLLLzYanGxV/vPfd0ONmiHFavjP09Jtr3lq0g5d/+JNTWTlc2DmW7zccxGEME75c7/E1LuoSyxXdmxZJMMGBAQxq1/C0Y1SqOtEEoTzz5S2wYQbcvQre6Fn8eY8chOBaXgkh8UQ6ABNn/MEHy3az+cDJMl9jdJfYIslBKeWeJghVug0zreQAJScH8FpyAPh8dWL+dknJYdzAVkRFhNAprh7HUrN4cf5W7hjQkq2HUji3bQOvxadUdaMJQpXuy5uLP/bgJljxJmz9DrpeU6G33XkkldW7T3B5t6a0/MfcEs+9a2Cr/HWfJwxt6/KUMLKzLryjVHloglDl1/mqghfcKuAlt7V7k8h1mPwpLEa/vpSUzByS07NLrNe+SV0mDmvHxGEVP6urUjWZJghVfnUq5jfz2Wv3sWn/Sf63ZCcA7RrX4fb+LUnJtKYCf3ruZpfznxrVkSfmbKRlgwgysx08PFwTg1LeoAlCFZWTBSkHrDmS1nxQ/HnRrTy6nDGGiTPWc3n3ZvRKiMovW7LtKHd+vIZT2a5vU285mMLfv1jn9lq390/ghj4taB4dzoA2DQgM0A5npbxFE4Qqau54+O1DawK9+Y+6HmszFIb+21r0J76/R5c7lZ3L56sT+er3fax7YggfLttNdq6Dl3/40+OQPh3bm94to/P3B7XVIalKeZsmCFXAGDAO2LHQ2v/h8YJjtRvB7QutPgeAmNYeX/ZoShYA2bmGy99azqYyDk99fcxZLslBKVU5dD4BVWDmWPhnFCTvsfaN0/QUQWEFyaGMHvx8bf52ScnhiYs6sOvZkXw6trdL+YWdY8t1X6XU6dEnCGVZ9Bz88Xnxx6Vsv0ts2JfM9sOpDDijAWt2nyj9/KeGUjvU+ufYu2U0P/5tAAkxtct0T6VUxdIEoSyL/u2+vPc46z0HDxLE5gMnGf7azzx/WWcemuHZ9BdvXtsNID855GndUGdSVcrXvJogRGQY8BoQCLxrjHnWzTkDgVeBYOCoMeZcu3wXkALkAjnGmB7ejLXG2vgVfHFT8ce73ehRgth6MIV/fr0JoNjk0L1Ffabf3ptDJzOIigghLDhQRyEp5ce8liBEJBB4A7gASARWicgcY8wmp3MigTeBYcaYPSJSeGjKIGPMUW/FWKMtfgHqNLLWjC5O4zMhOMzajnbtlN5y8CT/WbCN/xvdibTMXIa+uqTE2z1/WWeu6GFNktcsqvwzuiqlKo83nyB6AduNMTsBRORTYDSwyemcMcBMY8weAGPM4SJXUd6x0IOV326YA+FRcPU0iO/ncujeab+z7XAqc/846NHthp/ZWCfJU6qK8WaCiAP2Ou0nAmcXOucMIFhEFgF1gNeMMR/ZxwwwX0QM8D9jzNtejLXmWPkOxHV3f6z1+RBWDzqMhuR9VnIAaDcy/5TMnFyenLOJbYdTS7zNRV1iCQsK4Is11gR7dcKCKyR8pVTl8WaCcPfronFz/+7AeUAtYLmIrDDG/An0Ncbst5udfhCRLcaYIu0YIjIWGAvQvLmuGVyqueOLP3bdjBKrrt2bxMVvlL6u8+Z/DiMsOAAR4Ys1iTTXJiWlqiSPEoSIzACmAN8ZY9yv3VhUItDMab8psN/NOUeNMWlAmogsAboAfxpj9oPV7CQiX2E1WRVJEPaTxdsAPXr0KJyAVJ6T+yEw5LQu4Uly+L/RHakVEpi/v3jCQCJrnd59lVK+4eng9rew+gu2icizIuLJ7GirgDYikiAiIcDVwJxC58wG+otIkIiEYzVBbRaRCBGpAyAiEcAQYIOHsSp3Xm4PL3g2d1JhmTm59PjXDyWe8/7N1joRhV9qaxEdQb1wbV5Sqiry6AnCGPMj8KOI1AOuwWry2Qu8A0w1xhSZj9kYkyMi9wDzsIa5TjHGbBSRO+3jk40xm0Xke2A94MAaCrtBRFoCX9mdmkHANGPM96f90yr3Ys6AUf8l12HYejCFDrF1AXhyzkYycxxMX7nHbbUFfz+Xm99fxStXdaF7iyh2PTvS7XlKqapJjPGsVUZEooHrgOuxmoo+AfoBZxpjBnorwLLo0aOHWb16ta/D8D8bZlhLhhan9zgY9gyv/biNV378k/du7MFrC7axPjG56Kkto1ix8zhPjerIjefEey9mpVSlEJE1xb1n5mkfxEygHfAxcJEx5oB96DMR0U9kfzf3oRIP56Qc5uXvt7B0xzEAbv3Q/V/p5Ou6MayTrs6mVE3h6Sim140xP7k7oG84VwHBxYwium4GTL2MSZtb8mX6jmKrPzSsLYeSMxjSobGXAlRK+SNPE0R7EfnNGJMEICL1gWuMMW96LzR12nYvh1XvFszOWoiJbsOY2Hks33ms2Etc37sF4wZ6PrW3Uqr68HQU0+15yQHAGHMCuN07IakK8/4w2PBlsYcHvvpricnhyh5Nue+8Nt6ITClVBXiaIALEaZ4Ee54lHdxexSVlFh2gcF67gumwnr+8Cw3qhFZmSEopP+JpE9M84HMRmYz1NvSdgA47rYIeyr6ddY5WXBK4lGQiihzPe8nt0rPKtziQUqr68DRBTATuAO7CmkJjPvCut4JSp2nttCKzrwIwfhuf/2sVAM/mFExLUq9WMHVrBfHJrb355NfdAAxur2s+K1XTefqinAPrbeq3vBuOqhCz7ipa1u1G1hxz/9e97okh+dsPXnAGjeqGMVyHsypV43n6HkQb4BmgAxCWV26MaemluFR5FfPi47KmtzLmreUuZRueGsrJU64vwYcFB3JLvwSvhaeUqjo87aR+H+vpIQcYBHyE9dKc8jeZKS67P41eSXzGNMZ8nuhSftM58dQODSI2slZlRqeUqkI87YOoZYxZICJijNkNPCkiPwNPeDE2VR7Tr3bZ/fdPB4qcsuDv59KqQe3KikgpVUV5miAyRCQAazbXe4B9gPZi+pvfP4HdrlNybz+S5rIfHhKoyUEp5RFPm5geAMKB+7AW+LkOuNFbQalyOJUEs8e5FP3qcJ2V/fLuTfn6XtelQ5VSqjilPkHYL8VdaYyZAKQCN3s9KlU2X98Paz5wKfp71p3McAzI3//rmRG6JrRSqkxKTRDGmFwR6W73P+iKbf7k1AmQwCLJAWCLKXjPIaZ2iCYHpVSZedoH8TswW0S+APIbtY0xM70SlSqdMfBcPAQVHYV0Q9ZENpp4AObe1582jbTPQSlVdp4miCjgGDDYqcwAmiB8Zd2n1vecU0UOLXF0AeA/15yVvzqcUkqVladvUmu/g7/ZUXR5jsk5F/GbozUjOzdhSIdGjOoS66aiUkp5xtM3qd/HemJwYYwpYR1L5VVHtxYpmpY7mJtGDtI3oZVSFcLTJqZvnLbDgEuw1qVWvpCTCYc2FSnOMCHcpOtEK6UqiKdNTDOc90VkOvCjVyJSpTt1AhzZbGl+Ne32fJpf3LddLAEBOlpJKVUxPH1RrrA2QPNSz1IVL2kPHLaeHt7cHu1y6MnRXX0RkVKqmvK0DyIF1z6Ig1hrRKjK9uqZ+ZtpBRPrwoAJ1Ius74OAlFLVladNTHW8HYgqu2yC6JvxGl/c2YfY+La+DkcpVc141MQkIpeISD2n/UgRudh7YakikvfB067DVn91tOfOiwdpclBKeYWnfRBPGGOS83aMMUnoVN+V6/BmyHadmXVwp+aM6aVdQUop7/B0mKu7ROJpXVURHNlFit66rrsPAlFK1RSePkGsFpGXRaSViLQUkVeANd4MTBWSkVz6OUopVYE8fQq4F3gM+Mzenw886pWIlHtf3QHANVmP0FSO8PzF7dE3HpRS3uTpKKY0YFJZLy4iw4DXgEDgXWPMs27OGQi8CgQDR40x53pat8ZI3pe/ucnRgsuuGIN0b+rDgJRSNYGno5h+EJFIp/36IjKvlDqBwBvAcKADcI2IdCh0TiTwJjDKGNMRuMLTujVGdga81hmAzY7mXNHvTC7X5KCUqgSeNjHF2COXADDGnBCR0tak7gVsN8bsBBCRT4HRgPMkQmOAmcaYPfZ1D5ehbs2w/jNw5AAwIuvf7BjR3scBKaVqCk87qR0ikj+eUkTicTO7ayFxwF6n/US7zNkZQH0RWSQia0TkhjLUrf6yT8HX9+XvGgJ0riWlVKXx9AniEeAXEVls7w8AxpZSx90nWeGkEgR0B84DagHLRWSFh3Wtm4iMzYulefNq9k7A043zN8/LfIE59/T1YTBKqZrGoycIY8z3QA9gK9ZIpr8DRZcyc5UINHPab0rRKcITge+NMWnGmKPAEqCLh3XzYnvbGNPDGNOjQYMGnvw4VcOfrl08O0wcbRvrjCdKqcrjaSf1bcACrMTwd+Bj4MlSqq0C2ohIgoiEAFcDcwqdMxvoLyJBIhIOnA1s9rBu9Xb8r/zNR7OtBf1CgwJ9FY1SqgbytA/ifqAnsNsYMwg4CzhSUgVjTA5wDzAP60P/c2PMRhG5U0TutM/ZDHwPrAdWYg1n3VBc3TL/dFVZUEj+ZgNJ4tWrdCpvpVTl8rQPIsMYkyEiiEioMWaLiJQ6Q5wxZi4wt1DZ5EL7LwAveFK3RnHk5m/uaTCI+7rq+tJKqcrlaYJItN9ZmAX8ICIn0CVHvceRC3PHA9Aq42MWXX8+Ijp6SSlVuTx9k/oSe/NJEVkI1MNqGlLecLLgzelcAmkWFe7DYJRSNVWZZ2Q1xiwu/Sx1WlJL7N5RSqlKUd41qZU3vTsYgGW5Hbh3cGsfB6OUqqk0QfibFQV9+F85+nHfeW18GIxSqibTBOFvvp+Yv/nAuc0JDtS/IqWUb+injz/Jcl1SNPacq3wUiFJKaYLwLwc3ALDD0YSney5Hapc2Ya5SSnmPJgh/kpkCwMTs22muQ1uVUj6mCcJfZCTDJ5cBcII6NKlXy8cBKaVqOk0Q/mLVe/mbaVKb89pr85JSyrc0QfgLUzD30pcPDtepNZRSPqcJwl+YgvWQ4mIiSzhRKaUqhyYIP+Fwmr1Vnx6UUv5AE4Q/yEwlYPGzAHTKeNfHwSillEUThD9IPZS/efk5HXwYiFJKFdAE4Q8yTwKQbQJ54iJNEEop/6AJwh9kJAMwOf4V7X9QSvkNTRB+ICv1OAD1o/TdB6WU/9AE4QfSvnsSgKjoGN8GopRSTjRB+Jgj5Qj1T+0GoG1CMx9Ho5RSBTRB+NjuLWvyt1vFNvJhJEop5UoThI8dPHmqYEc7qJVSfkQThI/FbJ4KQNbZ9/o4EqWUcqUJwsdOZWYBEHL+oz6ORCmlXGmC8LHkLOFAYCwEh/k6FKWUchHk6wBqMpNykP6Zi30dhlJKuaVPED50/M8Vvg5BKaWKpQnCh7LXzwBgy5CpPo5EKaWK8mqCEJFhIrJVRLaLyCQ3xweKSLKIrLW/Hnc6tktE/rDLV3szTl85mpwKQNPuw30ciVJKFeW1PggRCQTeAC4AEoFVIjLHGLOp0Kk/G2MuLOYyg4wxR70Vo6850o+zJbgD7UK1K0gp5X+8+QTRC9hujNlpjMkCPgVGe/F+VcuGGXTOWosJ0+VFlVL+yZsJIg7Y67SfaJcV1kdE1onIdyLS0ancAPNFZI2IjPVinL7x5S0AOKJb+zgQpZRyz5ttG+7mjTCF9n8DWhhjUkVkBDALaGMf62uM2S8iDYEfRGSLMWZJkZtYyWMsQPPmzSsuem/KzcnfbNSyqw8DUUqp4nnzCSIRcJ6etCmw3/kEY8xJY0yqvT0XCBaRGHt/v/39MPAVVpNVEcaYt40xPYwxPRo0aFDxP4U3LHstfzOyWVsfBqKUUsXzZoJYBbQRkQQRCQGuBuY4nyAijcVeQk1EetnxHBORCBGpY5dHAEOADV6MtXKd2J2/GRSjTUxKKf/ktSYmY0yOiNwDzAMCgSnGmI0icqd9fDJwOXCXiOQAp4CrjTFGRBoBX9m5IwiYZoz53luxVjZHVnpBZq7T2JehKKVUsbw6vtJuNppbqGyy0/brwOtu6u0EungzNl86kJRGHLCn4SCqSK+JUqoG0jepfWDhrkwAVnb5l48jUUqp4mmC8IFISWOHowmB4foOhFLKf2mCqGwOBxcGWpP0XdQ51sfBKKVU8TRBVDJzcD0AscGpBAXqH79Syn/pJ1QlS9m9FoDFXV/ycSRKKVUyTRCV7LcVC0k1YQS1HODrUJRSqkSaICrZkWPHSSaCJvXDfR2KUkqVSOeZrkzGcEWQNZ1UXGw9HwejlFIl0yeISmQykn0dglJKeUwTRCU6dWibr0NQSimPaYKoROEfnA/Ab/G3+zgSpZQqnSYIH1hdq6+vQ1BKqVJpgvCBK4ac6+sQlFKqVJogKktWGrkE8E3wUOrXj/J1NEopVSpNEJUk/eA2AnFwrGFvX4eilFIe0QRRSfZtWQlAy1a6xKhSqmrQBFFJ2iybAEB0TEMfR6KUUp7RBFEJcnId+dsNG2iCUEpVDZogKsHeoyfzt2Ma6hoQSqmqQRNEJUjcsxOAPef8CwKDfRyNUkp5RhNEJUjbtRqA6Da9fByJUkp5ThOElxljWLP2dwAiGusIJqVU1aEJwst2HEmjQ8BuThIBtSJ9HY5SSnlME4SX/fjbVi4JXEpYw9a+DkUppcpEE4QXZebk0mrpeABC+oz1cTRKKVU2miC8aNHWI7QP2EO2hEDXa30djlJKlYkmCC9656dNNJWjBPS8BUR8HY5SSpWJJggv+Wb9fi4+9DoAgQn9fByNUkqVXZCvA6iONu0/ySvTv2FB6AJOJQyhVrsLfR2SUkqVmVefIERkmIhsFZHtIjLJzfGBIpIsImvtr8c9reuvsnIcPDLrD54OngJArYEPavOSUqpK8toThIgEAm8AFwCJwCoRmWOM2VTo1J+NMReWs67fSDyRzuVvLefgyQz6BGykd8hm60CLc3wbmFJKlZM3m5h6AduNMTsBRORTYDTgyYf86dQtt4VbD3Pf9N/p3TKaOmFB5OQachwOsnMNObkOchyG2qFB5DgMDochK9fBloMpHEnJzL9Ge9nN9JCnrZ3zn/RmuEop5VXeTBBxwF6n/UTgbDfn9RGRdcB+YLwxZmMZ6laYR2f9wdQVewD4acthGtcNIzhQCAoMIChACAoUjIGDyRkEBkj+V1R4CA6H4cym9bjhrPoMnjXGumBkC+j7gDdDVkopr/JmgnDX8G4K7f8GtDDGpIrICGAW0MbDutZNRMYCYwGaN29erkBzHYYmq59nV9gcVtQbQe/e/SC6NRgH1IqCoFCIiAFHLgQEQb24gsqph+GXV2H/7zBrmVXW+Sq49O1yxaKUUv7CmwkiEWjmtN8U6ykhnzHmpNP2XBF5U0RiPKnrVO9t4G2AHj16uE0ipdn4+zLuDpoDQO/kuTBvbskVAoILOp5zs1yPDX4MBowvTxhKKeVXvJkgVgFtRCQB2AdcDYxxPkFEGgOHjDFGRHphjao6BiSVVrfC5GYTsfhJADIv/ZDQ4CAICoO/FsPJfVA31npyOLkfkvZAbFcIq+dSn/Ao6HaTVR6oI4eVUtWD1z7NjDE5InIPMA8IBKYYYzaKyJ328cnA5cBdIpIDnAKuNsYYwG1drwSafQrJTOGVsHE82PnigvI253vldkopVVV49dddY8xcYG6hsslO268Dr3ta1yvC6vK3iGepVzvc67dSSqmqRKfaAPYmZxMbWcvXYSillF+p8QnC4TAMOKMBvRLq+zoUpZTyKzW+RzUgQHjlqq6+DkMppfxOjX+CUEop5Z4mCKWUUm5pglBKKeWWJgillFJuaYJQSinlliYIpZRSbmmCUEop5ZYmCKWUUm6JNTde9SAiR4Dd5aweAxytwHC8ReOsWBpnxasqsWqclhbGmAbuDlSrBHE6RGS1MaaHr+MojcZZsTTOildVYtU4S6dNTEoppdzSBKGUUsotTRAFqsoi0hpnxdI4K15ViVXjLIX2QSillHJLnyCUUkq5pQlCKaWUWzU+QYjIMBHZKiLbRWSSj2NpJiILRWSziGwUkfvt8igR+UFEttnf6zvVediOfauIDK3keANF5HcR+cZf4xSRSBH5UkS22H+uffw0zgftv/MNIjJdRML8JU4RmSIih0Vkg1NZmWMTke4i8od97D8iIpUQ5wv23/16EflKRCL9MU6nY+NFxIhIjK/jBMAYU2O/gEBgB9ASCAHWAR18GE8ToJu9XQf4E+gAPA9MsssnAc/Z2x3smEOBBPtnCazEeP8GTAO+sff9Lk7gQ+A2ezsEiPS3OIE44C+glr3/OXCTv8QJDAC6ARucysocG7AS6AMI8B0wvBLiHAIE2dvP+WucdnkzYB7Wy74xvo7TGFPjnyB6AduNMTuNMVnAp8BoXwVjjDlgjPnN3k4BNmN9eIzG+qDD/n6xvT0a+NQYk2mM+QvYjvUzeZ2INAVGAu86FftVnCJSF+s/43sAxpgsY0ySv8VpCwJqiUgQEA7s95c4jTFLgOOFissUm4g0AeoaY5Yb69PtI6c6XovTGDPfGJNj764AmvpjnLZXgIcA55FDPosTtIkpDtjrtJ9ol/mciMQDZwG/Ao2MMQfASiJAQ/s0X8b/KtY/ZodTmb/F2RI4ArxvN4W9KyIR/hanMWYf8CKwBzgAJBtj5vtbnIWUNbY4e7tweWW6Bes3bfCzOEVkFLDPGLOu0CGfxlnTE4S7Njufj/sVkdrADOABY8zJkk51U+b1+EXkQuCwMWaNp1XclFXGn3MQ1qP8W8aYs4A0rOaQ4vjqz7M+1m+KCUAsECEi15VUxU2Zz//d2oqLzacxi8gjQA7wSV5RMfFUepwiEg48Ajzu7nAx8VRKnDU9QSRitfvlaYr1aO8zIhKMlRw+McbMtIsP2Y+U2N8P2+W+ir8vMEpEdmE1yw0Wkal+GGcikGiM+dXe/xIrYfhbnOcDfxljjhhjsoGZwDl+GKezssaWSEHzjnO514nIjcCFwLV2c4y/xdkK65eDdfb/qabAbyLS2Ndx1vQEsQpoIyIJIhICXA3M8VUw9iiE94DNxpiXnQ7NAW60t28EZjuVXy0ioSKSALTB6rjyKmPMw8aYpsaYeKw/s5+MMdf5YZwHgb0i0tYuOg/Y5G9xYjUt9RaRcPvfwHlY/U/+FqezMsVmN0OliEhv+2e8wamO14jIMGAiMMoYk14ofr+I0xjzhzGmoTEm3v4/lYg1WOWgz+Os6F7vqvYFjMAaLbQDeMTHsfTDekxcD6y1v0YA0cACYJv9PcqpziN27FvxwigGD2IeSMEoJr+LE+gKrLb/TGcB9f00zqeALcAG4GOsUSt+EScwHatvJBvrw+vW8sQG9LB/vh3A69gzOXg5zu1Ybfh5/58m+2OchY7vwh7F5Ms4jTE61YZSSin3anoTk1JKqWJoglBKKeWWJgillFJuaYJQSinlliYIpZRSbmmCUMoPiMhAsWfFVcpfaIJQSinlliYIpcpARK4TkZUislZE/ifWmhipIvKSiPwmIgtEpIF9blcRWeG0FkF9u7y1iPwoIuvsOq3sy9eWgrUrPvHK/P5KlYEmCKU8JCLtgauAvsaYrkAucC0QAfxmjOkGLAaesKt8BEw0xnQG/nAq/wR4wxjTBWvOpQN2+VnAA1hrALTEmvNKKZ8J8nUASlUh5wHdgVX2L/e1sCapcwCf2edMBWaKSD0g0hiz2C7/EPhCROoAccaYrwCMMRkA9vVWGmMS7f21QDzwi/d/LKXc0wShlOcE+NAY87BLochjhc4raf6akpqNMp22c9H/n8rHtIlJKc8tAC4XkYaQvy5zC6z/R5fb54wBfjHGJAMnRKS/XX49sNhY63skisjF9jVC7fUAlPI7+huKUh4yxmwSkUeB+SISgDUb591YCxF1FJE1QDJWPwVY02BPthPATuBmu/x64H8i8k/7GldU4o+hlMd0NlelTpOIpBpjavs6DqUqmjYxKaWUckufIJRSSrmlTxBKKaXc0gShlFLKLU0QSiml3NIEoZRSyi1NEEoppdz6fwtEYRfT/uI+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot loss\n",
    "plt.plot(result.history['loss'])\n",
    "plt.plot(result.history['val_loss'])\n",
    "plt.title('Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#Plot accuracy\n",
    "plt.plot(result.history['accuracy'])\n",
    "plt.plot(result.history['val_accuracy'])\n",
    "plt.title('accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x248676a90>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_pred_list=[]\n",
    "for i in range(len(eval_pred)):\n",
    "    if max(eval_pred[i].astype(int))==1:\n",
    "        eval_pred_list.append(np.asscalar(np.argwhere(eval_pred[i].astype(int)==1)))\n",
    "    else:\n",
    "        eval_pred_list.append(3)\n",
    "print(cohen_kappa_score(eval_pred_list,eval_labels))\n",
    "pd.DataFrame(eval_pred_list).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x259255e90>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_pred_list=[]\n",
    "for i in range(len(test_pred)):\n",
    "    if max(test_pred[i].astype(int))==1:\n",
    "        test_pred_list.append(np.asscalar(np.argwhere(test_pred[i].astype(int)==1)))\n",
    "    else:\n",
    "        test_pred_list.append(3)\n",
    "pd.DataFrame(test_pred_list).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion\n",
    "<br> Tried deep learning with adding different layers and changing other hyperparameters. But\n",
    "<br> not found any good result. Specially it failed totally to capture any accuracy group other than \n",
    "<br> 3. Cohen-Kappa score is also very low. Need different feature engineering to make better prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14152 samples, validate on 3538 samples\n",
      "Epoch 1/2000\n",
      "14152/14152 [==============================] - 4s 314us/step - loss: 4.7735 - mse: 4.7735 - mae: 1.8346 - val_loss: 4.3449 - val_mse: 4.3449 - val_mae: 1.7550\n",
      "Epoch 2/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 2.8008 - mse: 2.8008 - mae: 1.4382 - val_loss: 2.1352 - val_mse: 2.1352 - val_mae: 1.2684\n",
      "Epoch 3/2000\n",
      "14152/14152 [==============================] - 3s 198us/step - loss: 1.9260 - mse: 1.9260 - mae: 1.1901 - val_loss: 1.8473 - val_mse: 1.8473 - val_mae: 1.1959\n",
      "Epoch 4/2000\n",
      "14152/14152 [==============================] - 3s 190us/step - loss: 1.5381 - mse: 1.5381 - mae: 1.0797 - val_loss: 1.4257 - val_mse: 1.4257 - val_mae: 1.0332\n",
      "Epoch 5/2000\n",
      "14152/14152 [==============================] - 3s 182us/step - loss: 1.2106 - mse: 1.2106 - mae: 0.9488 - val_loss: 1.2042 - val_mse: 1.2042 - val_mae: 0.9431\n",
      "Epoch 6/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 1.0732 - mse: 1.0732 - mae: 0.8803 - val_loss: 1.1209 - val_mse: 1.1209 - val_mae: 0.9090\n",
      "Epoch 7/2000\n",
      "14152/14152 [==============================] - 3s 189us/step - loss: 0.9848 - mse: 0.9848 - mae: 0.8326 - val_loss: 1.0323 - val_mse: 1.0323 - val_mae: 0.8650\n",
      "Epoch 8/2000\n",
      "14152/14152 [==============================] - 3s 194us/step - loss: 0.9243 - mse: 0.9243 - mae: 0.8026 - val_loss: 1.0068 - val_mse: 1.0068 - val_mae: 0.8463\n",
      "Epoch 9/2000\n",
      "14152/14152 [==============================] - 3s 182us/step - loss: 0.8842 - mse: 0.8842 - mae: 0.7807 - val_loss: 0.9437 - val_mse: 0.9437 - val_mae: 0.8064\n",
      "Epoch 10/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.8336 - mse: 0.8336 - mae: 0.7552 - val_loss: 0.9078 - val_mse: 0.9078 - val_mae: 0.7819\n",
      "Epoch 11/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.8001 - mse: 0.8001 - mae: 0.7359 - val_loss: 0.8895 - val_mse: 0.8895 - val_mae: 0.7679\n",
      "Epoch 12/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.7834 - mse: 0.7834 - mae: 0.7241 - val_loss: 0.8687 - val_mse: 0.8687 - val_mae: 0.7572\n",
      "Epoch 13/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.7682 - mse: 0.7682 - mae: 0.7155 - val_loss: 0.8349 - val_mse: 0.8349 - val_mae: 0.7375\n",
      "Epoch 14/2000\n",
      "14152/14152 [==============================] - 3s 190us/step - loss: 0.7229 - mse: 0.7229 - mae: 0.6889 - val_loss: 0.8220 - val_mse: 0.8220 - val_mae: 0.7270\n",
      "Epoch 15/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.7051 - mse: 0.7051 - mae: 0.6787 - val_loss: 0.7833 - val_mse: 0.7833 - val_mae: 0.7166\n",
      "Epoch 16/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.7002 - mse: 0.7002 - mae: 0.6744 - val_loss: 0.7935 - val_mse: 0.7935 - val_mae: 0.7038\n",
      "Epoch 17/2000\n",
      "14152/14152 [==============================] - 3s 181us/step - loss: 0.6835 - mse: 0.6835 - mae: 0.6610 - val_loss: 0.7611 - val_mse: 0.7611 - val_mae: 0.6878\n",
      "Epoch 18/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.6667 - mse: 0.6667 - mae: 0.6480 - val_loss: 0.7494 - val_mse: 0.7494 - val_mae: 0.6830\n",
      "Epoch 19/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.6377 - mse: 0.6377 - mae: 0.6351 - val_loss: 0.7220 - val_mse: 0.7220 - val_mae: 0.6628\n",
      "Epoch 20/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.6305 - mse: 0.6305 - mae: 0.6272 - val_loss: 0.7318 - val_mse: 0.7318 - val_mae: 0.6652\n",
      "Epoch 21/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.6184 - mse: 0.6184 - mae: 0.6169 - val_loss: 0.6951 - val_mse: 0.6951 - val_mae: 0.6467\n",
      "Epoch 22/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.6057 - mse: 0.6057 - mae: 0.6097 - val_loss: 0.6819 - val_mse: 0.6819 - val_mae: 0.6450\n",
      "Epoch 23/2000\n",
      "14152/14152 [==============================] - 3s 192us/step - loss: 0.5967 - mse: 0.5967 - mae: 0.6007 - val_loss: 0.6691 - val_mse: 0.6691 - val_mae: 0.6295\n",
      "Epoch 24/2000\n",
      "14152/14152 [==============================] - 3s 197us/step - loss: 0.5863 - mse: 0.5863 - mae: 0.5950 - val_loss: 0.6461 - val_mse: 0.6461 - val_mae: 0.6192\n",
      "Epoch 25/2000\n",
      "14152/14152 [==============================] - 3s 186us/step - loss: 0.5712 - mse: 0.5712 - mae: 0.5837 - val_loss: 0.6619 - val_mse: 0.6619 - val_mae: 0.6149\n",
      "Epoch 26/2000\n",
      "14152/14152 [==============================] - 3s 186us/step - loss: 0.5597 - mse: 0.5597 - mae: 0.5744 - val_loss: 0.6481 - val_mse: 0.6481 - val_mae: 0.6070\n",
      "Epoch 27/2000\n",
      "14152/14152 [==============================] - 3s 191us/step - loss: 0.5563 - mse: 0.5563 - mae: 0.5713 - val_loss: 0.6470 - val_mse: 0.6470 - val_mae: 0.6004\n",
      "Epoch 28/2000\n",
      "14152/14152 [==============================] - 3s 197us/step - loss: 0.5360 - mse: 0.5360 - mae: 0.5558 - val_loss: 0.6255 - val_mse: 0.6255 - val_mae: 0.5818\n",
      "Epoch 29/2000\n",
      "14152/14152 [==============================] - 3s 186us/step - loss: 0.5383 - mse: 0.5383 - mae: 0.5585 - val_loss: 0.6088 - val_mse: 0.6088 - val_mae: 0.5817\n",
      "Epoch 30/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.5267 - mse: 0.5267 - mae: 0.5485 - val_loss: 0.6103 - val_mse: 0.6103 - val_mae: 0.5744\n",
      "Epoch 31/2000\n",
      "14152/14152 [==============================] - 3s 181us/step - loss: 0.5222 - mse: 0.5222 - mae: 0.5432 - val_loss: 0.6016 - val_mse: 0.6016 - val_mae: 0.5736\n",
      "Epoch 32/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.5139 - mse: 0.5139 - mae: 0.5389 - val_loss: 0.5935 - val_mse: 0.5935 - val_mae: 0.5633\n",
      "Epoch 33/2000\n",
      "14152/14152 [==============================] - 3s 186us/step - loss: 0.5096 - mse: 0.5096 - mae: 0.5324 - val_loss: 0.5996 - val_mse: 0.5996 - val_mae: 0.5682\n",
      "Epoch 34/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.5022 - mse: 0.5022 - mae: 0.5302 - val_loss: 0.5862 - val_mse: 0.5862 - val_mae: 0.5568\n",
      "Epoch 35/2000\n",
      "14152/14152 [==============================] - 3s 198us/step - loss: 0.5041 - mse: 0.5041 - mae: 0.5277 - val_loss: 0.5892 - val_mse: 0.5892 - val_mae: 0.5514\n",
      "Epoch 36/2000\n",
      "14152/14152 [==============================] - 3s 182us/step - loss: 0.5068 - mse: 0.5068 - mae: 0.5294 - val_loss: 0.5647 - val_mse: 0.5647 - val_mae: 0.5510\n",
      "Epoch 37/2000\n",
      "14152/14152 [==============================] - 3s 193us/step - loss: 0.4860 - mse: 0.4860 - mae: 0.5150 - val_loss: 0.5587 - val_mse: 0.5587 - val_mae: 0.5408\n",
      "Epoch 38/2000\n",
      "14152/14152 [==============================] - 3s 208us/step - loss: 0.4835 - mse: 0.4835 - mae: 0.5124 - val_loss: 0.5412 - val_mse: 0.5412 - val_mae: 0.5295\n",
      "Epoch 39/2000\n",
      "14152/14152 [==============================] - 3s 213us/step - loss: 0.4824 - mse: 0.4824 - mae: 0.5118 - val_loss: 0.5566 - val_mse: 0.5566 - val_mae: 0.5361\n",
      "Epoch 40/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.4875 - mse: 0.4875 - mae: 0.5132 - val_loss: 0.5835 - val_mse: 0.5835 - val_mae: 0.5546\n",
      "Epoch 41/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.4664 - mse: 0.4664 - mae: 0.4991 - val_loss: 0.5493 - val_mse: 0.5493 - val_mae: 0.5324\n",
      "Epoch 42/2000\n",
      "14152/14152 [==============================] - 3s 195us/step - loss: 0.4608 - mse: 0.4608 - mae: 0.4975 - val_loss: 0.5275 - val_mse: 0.5275 - val_mae: 0.5189\n",
      "Epoch 43/2000\n",
      "14152/14152 [==============================] - 3s 197us/step - loss: 0.4714 - mse: 0.4714 - mae: 0.5011 - val_loss: 0.5521 - val_mse: 0.5521 - val_mae: 0.5275\n",
      "Epoch 44/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.4665 - mse: 0.4665 - mae: 0.4984 - val_loss: 0.5289 - val_mse: 0.5289 - val_mae: 0.5153\n",
      "Epoch 45/2000\n",
      "14152/14152 [==============================] - 3s 182us/step - loss: 0.4595 - mse: 0.4595 - mae: 0.4939 - val_loss: 0.5404 - val_mse: 0.5404 - val_mae: 0.5159\n",
      "Epoch 46/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.4514 - mse: 0.4514 - mae: 0.4880 - val_loss: 0.5103 - val_mse: 0.5103 - val_mae: 0.5033\n",
      "Epoch 47/2000\n",
      "14152/14152 [==============================] - 3s 195us/step - loss: 0.4544 - mse: 0.4544 - mae: 0.4892 - val_loss: 0.5195 - val_mse: 0.5195 - val_mae: 0.5082\n",
      "Epoch 48/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14152/14152 [==============================] - 3s 195us/step - loss: 0.4490 - mse: 0.4490 - mae: 0.4856 - val_loss: 0.5238 - val_mse: 0.5238 - val_mae: 0.5031\n",
      "Epoch 49/2000\n",
      "14152/14152 [==============================] - 3s 187us/step - loss: 0.4557 - mse: 0.4557 - mae: 0.4866 - val_loss: 0.5129 - val_mse: 0.5129 - val_mae: 0.5055\n",
      "Epoch 50/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.4510 - mse: 0.4510 - mae: 0.4858 - val_loss: 0.4972 - val_mse: 0.4972 - val_mae: 0.4982\n",
      "Epoch 51/2000\n",
      "14152/14152 [==============================] - 3s 182us/step - loss: 0.4460 - mse: 0.4460 - mae: 0.4830 - val_loss: 0.5219 - val_mse: 0.5219 - val_mae: 0.5002\n",
      "Epoch 52/2000\n",
      "14152/14152 [==============================] - 3s 189us/step - loss: 0.4464 - mse: 0.4464 - mae: 0.4779 - val_loss: 0.4983 - val_mse: 0.4983 - val_mae: 0.4911\n",
      "Epoch 53/2000\n",
      "14152/14152 [==============================] - 3s 186us/step - loss: 0.4353 - mse: 0.4353 - mae: 0.4741 - val_loss: 0.5176 - val_mse: 0.5176 - val_mae: 0.4921\n",
      "Epoch 54/2000\n",
      "14152/14152 [==============================] - 3s 193us/step - loss: 0.4314 - mse: 0.4314 - mae: 0.4719 - val_loss: 0.4995 - val_mse: 0.4995 - val_mae: 0.4890\n",
      "Epoch 55/2000\n",
      "14152/14152 [==============================] - 3s 190us/step - loss: 0.4392 - mse: 0.4392 - mae: 0.4753 - val_loss: 0.5192 - val_mse: 0.5192 - val_mae: 0.5008\n",
      "Epoch 56/2000\n",
      "14152/14152 [==============================] - 3s 193us/step - loss: 0.4180 - mse: 0.4180 - mae: 0.4615 - val_loss: 0.4748 - val_mse: 0.4748 - val_mae: 0.4769\n",
      "Epoch 57/2000\n",
      "14152/14152 [==============================] - 3s 182us/step - loss: 0.4322 - mse: 0.4322 - mae: 0.4704 - val_loss: 0.4993 - val_mse: 0.4993 - val_mae: 0.4801\n",
      "Epoch 58/2000\n",
      "14152/14152 [==============================] - 3s 191us/step - loss: 0.4205 - mse: 0.4205 - mae: 0.4613 - val_loss: 0.4969 - val_mse: 0.4969 - val_mae: 0.4796\n",
      "Epoch 59/2000\n",
      "14152/14152 [==============================] - 3s 189us/step - loss: 0.4253 - mse: 0.4253 - mae: 0.4641 - val_loss: 0.4837 - val_mse: 0.4837 - val_mae: 0.4827\n",
      "Epoch 60/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.4183 - mse: 0.4183 - mae: 0.4602 - val_loss: 0.4981 - val_mse: 0.4981 - val_mae: 0.4833\n",
      "Epoch 61/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.4094 - mse: 0.4094 - mae: 0.4520 - val_loss: 0.4773 - val_mse: 0.4773 - val_mae: 0.4705\n",
      "Epoch 62/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.4078 - mse: 0.4078 - mae: 0.4505 - val_loss: 0.4959 - val_mse: 0.4959 - val_mae: 0.4770\n",
      "Epoch 63/2000\n",
      "14152/14152 [==============================] - 3s 190us/step - loss: 0.4075 - mse: 0.4075 - mae: 0.4516 - val_loss: 0.4851 - val_mse: 0.4851 - val_mae: 0.4690\n",
      "Epoch 64/2000\n",
      "14152/14152 [==============================] - 3s 178us/step - loss: 0.4053 - mse: 0.4053 - mae: 0.4500 - val_loss: 0.4914 - val_mse: 0.4914 - val_mae: 0.4694\n",
      "Epoch 65/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.4082 - mse: 0.4082 - mae: 0.4481 - val_loss: 0.4677 - val_mse: 0.4677 - val_mae: 0.4628\n",
      "Epoch 66/2000\n",
      "14152/14152 [==============================] - 3s 198us/step - loss: 0.4129 - mse: 0.4129 - mae: 0.4511 - val_loss: 0.4658 - val_mse: 0.4658 - val_mae: 0.4705\n",
      "Epoch 67/2000\n",
      "14152/14152 [==============================] - 3s 190us/step - loss: 0.4120 - mse: 0.4120 - mae: 0.4529 - val_loss: 0.4701 - val_mse: 0.4701 - val_mae: 0.4601\n",
      "Epoch 68/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.4090 - mse: 0.4090 - mae: 0.4479 - val_loss: 0.4820 - val_mse: 0.4820 - val_mae: 0.4648\n",
      "Epoch 69/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.4048 - mse: 0.4048 - mae: 0.4439 - val_loss: 0.4607 - val_mse: 0.4607 - val_mae: 0.4594\n",
      "Epoch 70/2000\n",
      "14152/14152 [==============================] - 3s 182us/step - loss: 0.3992 - mse: 0.3992 - mae: 0.4417 - val_loss: 0.4770 - val_mse: 0.4770 - val_mae: 0.4722\n",
      "Epoch 71/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.4007 - mse: 0.4007 - mae: 0.4400 - val_loss: 0.4568 - val_mse: 0.4568 - val_mae: 0.4517\n",
      "Epoch 72/2000\n",
      "14152/14152 [==============================] - 3s 199us/step - loss: 0.3935 - mse: 0.3935 - mae: 0.4353 - val_loss: 0.4714 - val_mse: 0.4714 - val_mae: 0.4616\n",
      "Epoch 73/2000\n",
      "14152/14152 [==============================] - 3s 214us/step - loss: 0.3957 - mse: 0.3957 - mae: 0.4377 - val_loss: 0.4513 - val_mse: 0.4513 - val_mae: 0.4480\n",
      "Epoch 74/2000\n",
      "14152/14152 [==============================] - 3s 210us/step - loss: 0.3854 - mse: 0.3854 - mae: 0.4303 - val_loss: 0.4759 - val_mse: 0.4759 - val_mae: 0.4573\n",
      "Epoch 75/2000\n",
      "14152/14152 [==============================] - 4s 253us/step - loss: 0.3950 - mse: 0.3950 - mae: 0.4378 - val_loss: 0.4713 - val_mse: 0.4713 - val_mae: 0.4533\n",
      "Epoch 76/2000\n",
      "14152/14152 [==============================] - 4s 250us/step - loss: 0.3895 - mse: 0.3895 - mae: 0.4326 - val_loss: 0.4727 - val_mse: 0.4727 - val_mae: 0.4549\n",
      "Epoch 77/2000\n",
      "14152/14152 [==============================] - 3s 201us/step - loss: 0.3883 - mse: 0.3883 - mae: 0.4315 - val_loss: 0.4627 - val_mse: 0.4627 - val_mae: 0.4511\n",
      "Epoch 78/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.3938 - mse: 0.3938 - mae: 0.4349 - val_loss: 0.4731 - val_mse: 0.4731 - val_mae: 0.4506\n",
      "Epoch 79/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.3860 - mse: 0.3860 - mae: 0.4293 - val_loss: 0.4401 - val_mse: 0.4401 - val_mae: 0.4366\n",
      "Epoch 80/2000\n",
      "14152/14152 [==============================] - 3s 178us/step - loss: 0.3806 - mse: 0.3806 - mae: 0.4238 - val_loss: 0.4488 - val_mse: 0.4488 - val_mae: 0.4454\n",
      "Epoch 81/2000\n",
      "14152/14152 [==============================] - 3s 178us/step - loss: 0.3717 - mse: 0.3717 - mae: 0.4181 - val_loss: 0.4520 - val_mse: 0.4520 - val_mae: 0.4408\n",
      "Epoch 82/2000\n",
      "14152/14152 [==============================] - 3s 187us/step - loss: 0.3807 - mse: 0.3807 - mae: 0.4252 - val_loss: 0.4586 - val_mse: 0.4586 - val_mae: 0.4395\n",
      "Epoch 83/2000\n",
      "14152/14152 [==============================] - 3s 195us/step - loss: 0.3779 - mse: 0.3779 - mae: 0.4220 - val_loss: 0.4615 - val_mse: 0.4615 - val_mae: 0.4420\n",
      "Epoch 84/2000\n",
      "14152/14152 [==============================] - 3s 191us/step - loss: 0.3735 - mse: 0.3735 - mae: 0.4205 - val_loss: 0.4636 - val_mse: 0.4636 - val_mae: 0.4393\n",
      "Epoch 85/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.3682 - mse: 0.3682 - mae: 0.4154 - val_loss: 0.4531 - val_mse: 0.4531 - val_mae: 0.4331\n",
      "Epoch 86/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.3750 - mse: 0.3750 - mae: 0.4180 - val_loss: 0.4693 - val_mse: 0.4693 - val_mae: 0.4428\n",
      "Epoch 87/2000\n",
      "14152/14152 [==============================] - 3s 186us/step - loss: 0.3820 - mse: 0.3820 - mae: 0.4219 - val_loss: 0.4665 - val_mse: 0.4665 - val_mae: 0.4319\n",
      "Epoch 88/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.3702 - mse: 0.3702 - mae: 0.4167 - val_loss: 0.4420 - val_mse: 0.4420 - val_mae: 0.4369\n",
      "Epoch 89/2000\n",
      "14152/14152 [==============================] - 3s 182us/step - loss: 0.3691 - mse: 0.3691 - mae: 0.4176 - val_loss: 0.4729 - val_mse: 0.4729 - val_mae: 0.4423\n",
      "Epoch 90/2000\n",
      "14152/14152 [==============================] - 3s 187us/step - loss: 0.3657 - mse: 0.3657 - mae: 0.4128 - val_loss: 0.4409 - val_mse: 0.4409 - val_mae: 0.4276\n",
      "Epoch 91/2000\n",
      "14152/14152 [==============================] - 3s 200us/step - loss: 0.3653 - mse: 0.3653 - mae: 0.4099 - val_loss: 0.4634 - val_mse: 0.4634 - val_mae: 0.4310\n",
      "Epoch 92/2000\n",
      "14152/14152 [==============================] - 4s 269us/step - loss: 0.3785 - mse: 0.3785 - mae: 0.4173 - val_loss: 0.4491 - val_mse: 0.4491 - val_mae: 0.4315\n",
      "Epoch 93/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.3675 - mse: 0.3675 - mae: 0.4107 - val_loss: 0.4496 - val_mse: 0.4496 - val_mae: 0.4260\n",
      "Epoch 94/2000\n",
      "14152/14152 [==============================] - 3s 182us/step - loss: 0.3675 - mse: 0.3675 - mae: 0.4132 - val_loss: 0.4467 - val_mse: 0.4467 - val_mae: 0.4305\n",
      "Epoch 95/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14152/14152 [==============================] - 3s 182us/step - loss: 0.3602 - mse: 0.3602 - mae: 0.4066 - val_loss: 0.4440 - val_mse: 0.4440 - val_mae: 0.4309\n",
      "Epoch 96/2000\n",
      "14152/14152 [==============================] - 3s 187us/step - loss: 0.3648 - mse: 0.3648 - mae: 0.4087 - val_loss: 0.4572 - val_mse: 0.4572 - val_mae: 0.4272\n",
      "Epoch 97/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.3693 - mse: 0.3693 - mae: 0.4118 - val_loss: 0.4404 - val_mse: 0.4404 - val_mae: 0.4269\n",
      "Epoch 98/2000\n",
      "14152/14152 [==============================] - 3s 182us/step - loss: 0.3595 - mse: 0.3595 - mae: 0.4069 - val_loss: 0.4505 - val_mse: 0.4505 - val_mae: 0.4275\n",
      "Epoch 99/2000\n",
      "14152/14152 [==============================] - 3s 191us/step - loss: 0.3592 - mse: 0.3592 - mae: 0.4066 - val_loss: 0.4451 - val_mse: 0.4451 - val_mae: 0.4257\n",
      "Epoch 100/2000\n",
      "14152/14152 [==============================] - 3s 197us/step - loss: 0.3609 - mse: 0.3609 - mae: 0.4078 - val_loss: 0.4526 - val_mse: 0.4526 - val_mae: 0.4216\n",
      "Epoch 101/2000\n",
      "14152/14152 [==============================] - 3s 189us/step - loss: 0.3560 - mse: 0.3560 - mae: 0.4038 - val_loss: 0.4554 - val_mse: 0.4554 - val_mae: 0.4366\n",
      "Epoch 102/2000\n",
      "14152/14152 [==============================] - 3s 191us/step - loss: 0.3409 - mse: 0.3409 - mae: 0.3945 - val_loss: 0.4328 - val_mse: 0.4328 - val_mae: 0.4212\n",
      "Epoch 103/2000\n",
      "14152/14152 [==============================] - 3s 192us/step - loss: 0.3546 - mse: 0.3546 - mae: 0.4007 - val_loss: 0.4368 - val_mse: 0.4368 - val_mae: 0.4277\n",
      "Epoch 104/2000\n",
      "14152/14152 [==============================] - 3s 195us/step - loss: 0.3477 - mse: 0.3477 - mae: 0.3974 - val_loss: 0.4307 - val_mse: 0.4307 - val_mae: 0.4162\n",
      "Epoch 105/2000\n",
      "14152/14152 [==============================] - 3s 199us/step - loss: 0.3509 - mse: 0.3509 - mae: 0.4016 - val_loss: 0.4345 - val_mse: 0.4345 - val_mae: 0.4217\n",
      "Epoch 106/2000\n",
      "14152/14152 [==============================] - 3s 192us/step - loss: 0.3486 - mse: 0.3486 - mae: 0.3969 - val_loss: 0.4203 - val_mse: 0.4203 - val_mae: 0.4065\n",
      "Epoch 107/2000\n",
      "14152/14152 [==============================] - 3s 197us/step - loss: 0.3479 - mse: 0.3479 - mae: 0.3943 - val_loss: 0.4387 - val_mse: 0.4387 - val_mae: 0.4172\n",
      "Epoch 108/2000\n",
      "14152/14152 [==============================] - 3s 198us/step - loss: 0.3528 - mse: 0.3528 - mae: 0.3997 - val_loss: 0.4192 - val_mse: 0.4192 - val_mae: 0.4093\n",
      "Epoch 109/2000\n",
      "14152/14152 [==============================] - 3s 190us/step - loss: 0.3405 - mse: 0.3405 - mae: 0.3923 - val_loss: 0.4277 - val_mse: 0.4277 - val_mae: 0.4093\n",
      "Epoch 110/2000\n",
      "14152/14152 [==============================] - 3s 237us/step - loss: 0.3407 - mse: 0.3407 - mae: 0.3922 - val_loss: 0.4287 - val_mse: 0.4287 - val_mae: 0.4136\n",
      "Epoch 111/2000\n",
      "14152/14152 [==============================] - 3s 198us/step - loss: 0.3543 - mse: 0.3543 - mae: 0.3967 - val_loss: 0.4340 - val_mse: 0.4340 - val_mae: 0.4143\n",
      "Epoch 112/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.3480 - mse: 0.3480 - mae: 0.3983 - val_loss: 0.4369 - val_mse: 0.4369 - val_mae: 0.4112\n",
      "Epoch 113/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.3412 - mse: 0.3412 - mae: 0.3929 - val_loss: 0.4197 - val_mse: 0.4197 - val_mae: 0.4077\n",
      "Epoch 114/2000\n",
      "14152/14152 [==============================] - 3s 208us/step - loss: 0.3301 - mse: 0.3301 - mae: 0.3842 - val_loss: 0.4292 - val_mse: 0.4292 - val_mae: 0.4083\n",
      "Epoch 115/2000\n",
      "14152/14152 [==============================] - 3s 217us/step - loss: 0.3348 - mse: 0.3348 - mae: 0.3860 - val_loss: 0.4297 - val_mse: 0.4297 - val_mae: 0.4140\n",
      "Epoch 116/2000\n",
      "14152/14152 [==============================] - 4s 262us/step - loss: 0.3300 - mse: 0.3300 - mae: 0.3862 - val_loss: 0.4134 - val_mse: 0.4134 - val_mae: 0.4038\n",
      "Epoch 117/2000\n",
      "14152/14152 [==============================] - 4s 300us/step - loss: 0.3399 - mse: 0.3399 - mae: 0.3886 - val_loss: 0.4321 - val_mse: 0.4321 - val_mae: 0.4122\n",
      "Epoch 118/2000\n",
      "14152/14152 [==============================] - 3s 199us/step - loss: 0.3344 - mse: 0.3344 - mae: 0.3899 - val_loss: 0.4261 - val_mse: 0.4260 - val_mae: 0.4067\n",
      "Epoch 119/2000\n",
      "14152/14152 [==============================] - 3s 194us/step - loss: 0.3369 - mse: 0.3369 - mae: 0.3867 - val_loss: 0.4241 - val_mse: 0.4241 - val_mae: 0.4037\n",
      "Epoch 120/2000\n",
      "14152/14152 [==============================] - 3s 194us/step - loss: 0.3478 - mse: 0.3478 - mae: 0.3945 - val_loss: 0.4257 - val_mse: 0.4257 - val_mae: 0.4003\n",
      "Epoch 121/2000\n",
      "14152/14152 [==============================] - 3s 194us/step - loss: 0.3337 - mse: 0.3337 - mae: 0.3864 - val_loss: 0.4423 - val_mse: 0.4423 - val_mae: 0.4065\n",
      "Epoch 122/2000\n",
      "14152/14152 [==============================] - 3s 192us/step - loss: 0.3290 - mse: 0.3290 - mae: 0.3815 - val_loss: 0.4320 - val_mse: 0.4320 - val_mae: 0.4045\n",
      "Epoch 123/2000\n",
      "14152/14152 [==============================] - 3s 191us/step - loss: 0.3298 - mse: 0.3298 - mae: 0.3824 - val_loss: 0.4188 - val_mse: 0.4188 - val_mae: 0.3981\n",
      "Epoch 124/2000\n",
      "14152/14152 [==============================] - 3s 193us/step - loss: 0.3249 - mse: 0.3249 - mae: 0.3776 - val_loss: 0.4426 - val_mse: 0.4426 - val_mae: 0.3984\n",
      "Epoch 125/2000\n",
      "14152/14152 [==============================] - 3s 193us/step - loss: 0.3245 - mse: 0.3245 - mae: 0.3799 - val_loss: 0.4163 - val_mse: 0.4163 - val_mae: 0.3992\n",
      "Epoch 126/2000\n",
      "14152/14152 [==============================] - 3s 193us/step - loss: 0.3318 - mse: 0.3318 - mae: 0.3828 - val_loss: 0.4154 - val_mse: 0.4154 - val_mae: 0.4039\n",
      "Epoch 127/2000\n",
      "14152/14152 [==============================] - 3s 208us/step - loss: 0.3321 - mse: 0.3321 - mae: 0.3824 - val_loss: 0.4239 - val_mse: 0.4239 - val_mae: 0.3967\n",
      "Epoch 128/2000\n",
      "14152/14152 [==============================] - 3s 202us/step - loss: 0.3383 - mse: 0.3383 - mae: 0.3855 - val_loss: 0.4096 - val_mse: 0.4096 - val_mae: 0.3944\n",
      "Epoch 129/2000\n",
      "14152/14152 [==============================] - 3s 203us/step - loss: 0.3308 - mse: 0.3308 - mae: 0.3823 - val_loss: 0.4077 - val_mse: 0.4077 - val_mae: 0.3964\n",
      "Epoch 130/2000\n",
      "14152/14152 [==============================] - 3s 204us/step - loss: 0.3272 - mse: 0.3272 - mae: 0.3793 - val_loss: 0.4173 - val_mse: 0.4173 - val_mae: 0.4000\n",
      "Epoch 131/2000\n",
      "14152/14152 [==============================] - 3s 198us/step - loss: 0.3182 - mse: 0.3182 - mae: 0.3717 - val_loss: 0.4202 - val_mse: 0.4202 - val_mae: 0.3967\n",
      "Epoch 132/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.3250 - mse: 0.3250 - mae: 0.3778 - val_loss: 0.4112 - val_mse: 0.4112 - val_mae: 0.3984\n",
      "Epoch 133/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.3232 - mse: 0.3232 - mae: 0.3780 - val_loss: 0.4097 - val_mse: 0.4097 - val_mae: 0.3972\n",
      "Epoch 134/2000\n",
      "14152/14152 [==============================] - 3s 205us/step - loss: 0.3344 - mse: 0.3344 - mae: 0.3829 - val_loss: 0.4073 - val_mse: 0.4073 - val_mae: 0.3952\n",
      "Epoch 135/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.3244 - mse: 0.3244 - mae: 0.3778 - val_loss: 0.4082 - val_mse: 0.4082 - val_mae: 0.3955\n",
      "Epoch 136/2000\n",
      "14152/14152 [==============================] - 3s 192us/step - loss: 0.3217 - mse: 0.3217 - mae: 0.3753 - val_loss: 0.4394 - val_mse: 0.4394 - val_mae: 0.4099\n",
      "Epoch 137/2000\n",
      "14152/14152 [==============================] - 3s 193us/step - loss: 0.3204 - mse: 0.3204 - mae: 0.3735 - val_loss: 0.4346 - val_mse: 0.4346 - val_mae: 0.4006\n",
      "Epoch 138/2000\n",
      "14152/14152 [==============================] - 3s 187us/step - loss: 0.3139 - mse: 0.3139 - mae: 0.3718 - val_loss: 0.4056 - val_mse: 0.4056 - val_mae: 0.3923\n",
      "Epoch 139/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.3232 - mse: 0.3232 - mae: 0.3759 - val_loss: 0.4171 - val_mse: 0.4171 - val_mae: 0.3941\n",
      "Epoch 140/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.3379 - mse: 0.3379 - mae: 0.3842 - val_loss: 0.4493 - val_mse: 0.4493 - val_mae: 0.4081\n",
      "Epoch 141/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.3299 - mse: 0.3299 - mae: 0.3798 - val_loss: 0.4033 - val_mse: 0.4033 - val_mae: 0.3938\n",
      "Epoch 142/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.3165 - mse: 0.3165 - mae: 0.3723 - val_loss: 0.4219 - val_mse: 0.4219 - val_mae: 0.3900\n",
      "Epoch 143/2000\n",
      "14152/14152 [==============================] - 3s 213us/step - loss: 0.3158 - mse: 0.3158 - mae: 0.3721 - val_loss: 0.4239 - val_mse: 0.4239 - val_mae: 0.3952\n",
      "Epoch 144/2000\n",
      "14152/14152 [==============================] - 3s 202us/step - loss: 0.3243 - mse: 0.3243 - mae: 0.3763 - val_loss: 0.3986 - val_mse: 0.3986 - val_mae: 0.3884\n",
      "Epoch 145/2000\n",
      "14152/14152 [==============================] - 3s 207us/step - loss: 0.3222 - mse: 0.3222 - mae: 0.3731 - val_loss: 0.4205 - val_mse: 0.4205 - val_mae: 0.3967\n",
      "Epoch 146/2000\n",
      "14152/14152 [==============================] - 3s 194us/step - loss: 0.3143 - mse: 0.3143 - mae: 0.3684 - val_loss: 0.4196 - val_mse: 0.4196 - val_mae: 0.3968\n",
      "Epoch 147/2000\n",
      "14152/14152 [==============================] - 3s 194us/step - loss: 0.3179 - mse: 0.3179 - mae: 0.3718 - val_loss: 0.4289 - val_mse: 0.4289 - val_mae: 0.3985\n",
      "Epoch 148/2000\n",
      "14152/14152 [==============================] - 3s 216us/step - loss: 0.3134 - mse: 0.3134 - mae: 0.3690 - val_loss: 0.4133 - val_mse: 0.4133 - val_mae: 0.3886\n",
      "Epoch 149/2000\n",
      "14152/14152 [==============================] - 3s 186us/step - loss: 0.3141 - mse: 0.3141 - mae: 0.3694 - val_loss: 0.4343 - val_mse: 0.4343 - val_mae: 0.3955\n",
      "Epoch 150/2000\n",
      "14152/14152 [==============================] - 3s 189us/step - loss: 0.3074 - mse: 0.3074 - mae: 0.3656 - val_loss: 0.3964 - val_mse: 0.3964 - val_mae: 0.3819\n",
      "Epoch 151/2000\n",
      "14152/14152 [==============================] - 3s 218us/step - loss: 0.3090 - mse: 0.3090 - mae: 0.3655 - val_loss: 0.4178 - val_mse: 0.4178 - val_mae: 0.3917\n",
      "Epoch 152/2000\n",
      "14152/14152 [==============================] - 3s 187us/step - loss: 0.3069 - mse: 0.3069 - mae: 0.3650 - val_loss: 0.4050 - val_mse: 0.4050 - val_mae: 0.3862\n",
      "Epoch 153/2000\n",
      "14152/14152 [==============================] - 3s 200us/step - loss: 0.3069 - mse: 0.3069 - mae: 0.3629 - val_loss: 0.3930 - val_mse: 0.3930 - val_mae: 0.3819\n",
      "Epoch 154/2000\n",
      "14152/14152 [==============================] - 3s 203us/step - loss: 0.3134 - mse: 0.3134 - mae: 0.3667 - val_loss: 0.3989 - val_mse: 0.3989 - val_mae: 0.3823\n",
      "Epoch 155/2000\n",
      "14152/14152 [==============================] - 3s 205us/step - loss: 0.3054 - mse: 0.3054 - mae: 0.3624 - val_loss: 0.4031 - val_mse: 0.4031 - val_mae: 0.3857\n",
      "Epoch 156/2000\n",
      "14152/14152 [==============================] - 3s 189us/step - loss: 0.3133 - mse: 0.3133 - mae: 0.3666 - val_loss: 0.4002 - val_mse: 0.4002 - val_mae: 0.3833\n",
      "Epoch 157/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.3112 - mse: 0.3112 - mae: 0.3663 - val_loss: 0.4050 - val_mse: 0.4050 - val_mae: 0.3870\n",
      "Epoch 158/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.3140 - mse: 0.3140 - mae: 0.3680 - val_loss: 0.4390 - val_mse: 0.4390 - val_mae: 0.4006\n",
      "Epoch 159/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.3088 - mse: 0.3088 - mae: 0.3646 - val_loss: 0.4048 - val_mse: 0.4048 - val_mae: 0.3874\n",
      "Epoch 160/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.3031 - mse: 0.3031 - mae: 0.3607 - val_loss: 0.4104 - val_mse: 0.4104 - val_mae: 0.3849\n",
      "Epoch 161/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.3078 - mse: 0.3078 - mae: 0.3610 - val_loss: 0.4117 - val_mse: 0.4117 - val_mae: 0.3891\n",
      "Epoch 162/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.3085 - mse: 0.3085 - mae: 0.3623 - val_loss: 0.3968 - val_mse: 0.3968 - val_mae: 0.3827\n",
      "Epoch 163/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.3000 - mse: 0.3000 - mae: 0.3581 - val_loss: 0.3951 - val_mse: 0.3951 - val_mae: 0.3827\n",
      "Epoch 164/2000\n",
      "14152/14152 [==============================] - 3s 207us/step - loss: 0.3083 - mse: 0.3083 - mae: 0.3617 - val_loss: 0.3994 - val_mse: 0.3994 - val_mae: 0.3877\n",
      "Epoch 165/2000\n",
      "14152/14152 [==============================] - 3s 210us/step - loss: 0.3156 - mse: 0.3156 - mae: 0.3677 - val_loss: 0.3965 - val_mse: 0.3965 - val_mae: 0.3800\n",
      "Epoch 166/2000\n",
      "14152/14152 [==============================] - 3s 197us/step - loss: 0.3024 - mse: 0.3024 - mae: 0.3594 - val_loss: 0.4005 - val_mse: 0.4005 - val_mae: 0.3801\n",
      "Epoch 167/2000\n",
      "14152/14152 [==============================] - 3s 194us/step - loss: 0.3111 - mse: 0.3111 - mae: 0.3613 - val_loss: 0.4177 - val_mse: 0.4177 - val_mae: 0.3844\n",
      "Epoch 168/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.2918 - mse: 0.2918 - mae: 0.3537 - val_loss: 0.4019 - val_mse: 0.4019 - val_mae: 0.3826\n",
      "Epoch 169/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.2986 - mse: 0.2986 - mae: 0.3564 - val_loss: 0.4021 - val_mse: 0.4021 - val_mae: 0.3825\n",
      "Epoch 170/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.2993 - mse: 0.2993 - mae: 0.3559 - val_loss: 0.3897 - val_mse: 0.3897 - val_mae: 0.3786\n",
      "Epoch 171/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.2905 - mse: 0.2905 - mae: 0.3526 - val_loss: 0.3953 - val_mse: 0.3953 - val_mae: 0.3770\n",
      "Epoch 172/2000\n",
      "14152/14152 [==============================] - 3s 208us/step - loss: 0.3015 - mse: 0.3015 - mae: 0.3573 - val_loss: 0.4106 - val_mse: 0.4106 - val_mae: 0.3829\n",
      "Epoch 173/2000\n",
      "14152/14152 [==============================] - 3s 194us/step - loss: 0.3074 - mse: 0.3074 - mae: 0.3639 - val_loss: 0.4067 - val_mse: 0.4067 - val_mae: 0.3856\n",
      "Epoch 174/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.3088 - mse: 0.3088 - mae: 0.3610 - val_loss: 0.3944 - val_mse: 0.3944 - val_mae: 0.3850\n",
      "Epoch 175/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.3043 - mse: 0.3043 - mae: 0.3581 - val_loss: 0.4128 - val_mse: 0.4128 - val_mae: 0.3861\n",
      "Epoch 176/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.2952 - mse: 0.2952 - mae: 0.3513 - val_loss: 0.4044 - val_mse: 0.4044 - val_mae: 0.3832\n",
      "Epoch 177/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.2965 - mse: 0.2965 - mae: 0.3537 - val_loss: 0.3847 - val_mse: 0.3847 - val_mae: 0.3771\n",
      "Epoch 178/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.2915 - mse: 0.2915 - mae: 0.3499 - val_loss: 0.4054 - val_mse: 0.4054 - val_mae: 0.3852\n",
      "Epoch 179/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.2942 - mse: 0.2942 - mae: 0.3514 - val_loss: 0.4164 - val_mse: 0.4164 - val_mae: 0.3893\n",
      "Epoch 180/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.2925 - mse: 0.2925 - mae: 0.3522 - val_loss: 0.3911 - val_mse: 0.3911 - val_mae: 0.3684\n",
      "Epoch 181/2000\n",
      "14152/14152 [==============================] - 3s 187us/step - loss: 0.2976 - mse: 0.2976 - mae: 0.3532 - val_loss: 0.3999 - val_mse: 0.3999 - val_mae: 0.3862\n",
      "Epoch 182/2000\n",
      "14152/14152 [==============================] - 3s 192us/step - loss: 0.2896 - mse: 0.2896 - mae: 0.3483 - val_loss: 0.4004 - val_mse: 0.4004 - val_mae: 0.3812\n",
      "Epoch 183/2000\n",
      "14152/14152 [==============================] - 3s 201us/step - loss: 0.2833 - mse: 0.2833 - mae: 0.3461 - val_loss: 0.3858 - val_mse: 0.3858 - val_mae: 0.3707\n",
      "Epoch 184/2000\n",
      "14152/14152 [==============================] - 3s 195us/step - loss: 0.2856 - mse: 0.2856 - mae: 0.3465 - val_loss: 0.3983 - val_mse: 0.3983 - val_mae: 0.3840\n",
      "Epoch 185/2000\n",
      "14152/14152 [==============================] - 3s 202us/step - loss: 0.2888 - mse: 0.2888 - mae: 0.3485 - val_loss: 0.3889 - val_mse: 0.3889 - val_mae: 0.3700\n",
      "Epoch 186/2000\n",
      "14152/14152 [==============================] - 3s 191us/step - loss: 0.2932 - mse: 0.2932 - mae: 0.3495 - val_loss: 0.3823 - val_mse: 0.3823 - val_mae: 0.3728\n",
      "Epoch 187/2000\n",
      "14152/14152 [==============================] - 3s 202us/step - loss: 0.3036 - mse: 0.3036 - mae: 0.3583 - val_loss: 0.3891 - val_mse: 0.3891 - val_mae: 0.3773\n",
      "Epoch 188/2000\n",
      "14152/14152 [==============================] - 3s 198us/step - loss: 0.2815 - mse: 0.2815 - mae: 0.3459 - val_loss: 0.3922 - val_mse: 0.3922 - val_mae: 0.3715\n",
      "Epoch 189/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14152/14152 [==============================] - 3s 191us/step - loss: 0.2926 - mse: 0.2926 - mae: 0.3511 - val_loss: 0.3876 - val_mse: 0.3876 - val_mae: 0.3754\n",
      "Epoch 190/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.2918 - mse: 0.2918 - mae: 0.3509 - val_loss: 0.3857 - val_mse: 0.3857 - val_mae: 0.3693\n",
      "Epoch 191/2000\n",
      "14152/14152 [==============================] - 3s 191us/step - loss: 0.2920 - mse: 0.2920 - mae: 0.3502 - val_loss: 0.3919 - val_mse: 0.3919 - val_mae: 0.3774\n",
      "Epoch 192/2000\n",
      "14152/14152 [==============================] - 3s 192us/step - loss: 0.2948 - mse: 0.2948 - mae: 0.3505 - val_loss: 0.3778 - val_mse: 0.3778 - val_mae: 0.3697\n",
      "Epoch 193/2000\n",
      "14152/14152 [==============================] - 3s 193us/step - loss: 0.2930 - mse: 0.2930 - mae: 0.3492 - val_loss: 0.3896 - val_mse: 0.3896 - val_mae: 0.3723\n",
      "Epoch 194/2000\n",
      "14152/14152 [==============================] - 3s 200us/step - loss: 0.2918 - mse: 0.2918 - mae: 0.3475 - val_loss: 0.4113 - val_mse: 0.4113 - val_mae: 0.3741\n",
      "Epoch 195/2000\n",
      "14152/14152 [==============================] - 3s 198us/step - loss: 0.2944 - mse: 0.2944 - mae: 0.3513 - val_loss: 0.4025 - val_mse: 0.4025 - val_mae: 0.3772\n",
      "Epoch 196/2000\n",
      "14152/14152 [==============================] - 3s 196us/step - loss: 0.2833 - mse: 0.2833 - mae: 0.3443 - val_loss: 0.3993 - val_mse: 0.3993 - val_mae: 0.3759\n",
      "Epoch 197/2000\n",
      "14152/14152 [==============================] - 3s 192us/step - loss: 0.2781 - mse: 0.2781 - mae: 0.3422 - val_loss: 0.3999 - val_mse: 0.3999 - val_mae: 0.3774\n",
      "Epoch 198/2000\n",
      "14152/14152 [==============================] - 3s 181us/step - loss: 0.2824 - mse: 0.2824 - mae: 0.3441 - val_loss: 0.3992 - val_mse: 0.3992 - val_mae: 0.3784\n",
      "Epoch 199/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.2872 - mse: 0.2872 - mae: 0.3459 - val_loss: 0.3819 - val_mse: 0.3819 - val_mae: 0.3666\n",
      "Epoch 200/2000\n",
      "14152/14152 [==============================] - 3s 190us/step - loss: 0.2778 - mse: 0.2778 - mae: 0.3405 - val_loss: 0.3954 - val_mse: 0.3954 - val_mae: 0.3742\n",
      "Epoch 201/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.2941 - mse: 0.2941 - mae: 0.3499 - val_loss: 0.4043 - val_mse: 0.4043 - val_mae: 0.3761\n",
      "Epoch 202/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.2835 - mse: 0.2835 - mae: 0.3434 - val_loss: 0.3904 - val_mse: 0.3904 - val_mae: 0.3674\n",
      "Epoch 203/2000\n",
      "14152/14152 [==============================] - 3s 191us/step - loss: 0.2835 - mse: 0.2835 - mae: 0.3438 - val_loss: 0.3870 - val_mse: 0.3870 - val_mae: 0.3722\n",
      "Epoch 204/2000\n",
      "14152/14152 [==============================] - 3s 186us/step - loss: 0.2823 - mse: 0.2823 - mae: 0.3426 - val_loss: 0.3913 - val_mse: 0.3913 - val_mae: 0.3678\n",
      "Epoch 205/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.2756 - mse: 0.2756 - mae: 0.3393 - val_loss: 0.3905 - val_mse: 0.3905 - val_mae: 0.3689\n",
      "Epoch 206/2000\n",
      "14152/14152 [==============================] - 3s 186us/step - loss: 0.2815 - mse: 0.2815 - mae: 0.3426 - val_loss: 0.3744 - val_mse: 0.3744 - val_mae: 0.3660\n",
      "Epoch 207/2000\n",
      "14152/14152 [==============================] - 3s 182us/step - loss: 0.2874 - mse: 0.2874 - mae: 0.3471 - val_loss: 0.4093 - val_mse: 0.4093 - val_mae: 0.3788\n",
      "Epoch 208/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2758 - mse: 0.2758 - mae: 0.3375 - val_loss: 0.3978 - val_mse: 0.3978 - val_mae: 0.3721\n",
      "Epoch 209/2000\n",
      "14152/14152 [==============================] - 3s 196us/step - loss: 0.2802 - mse: 0.2802 - mae: 0.3408 - val_loss: 0.3944 - val_mse: 0.3944 - val_mae: 0.3692\n",
      "Epoch 210/2000\n",
      "14152/14152 [==============================] - 3s 181us/step - loss: 0.2861 - mse: 0.2861 - mae: 0.3460 - val_loss: 0.3953 - val_mse: 0.3953 - val_mae: 0.3706\n",
      "Epoch 211/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.2776 - mse: 0.2776 - mae: 0.3377 - val_loss: 0.4021 - val_mse: 0.4021 - val_mae: 0.3725\n",
      "Epoch 212/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.2751 - mse: 0.2751 - mae: 0.3394 - val_loss: 0.4018 - val_mse: 0.4018 - val_mae: 0.3715\n",
      "Epoch 213/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.2862 - mse: 0.2862 - mae: 0.3438 - val_loss: 0.3788 - val_mse: 0.3788 - val_mae: 0.3639\n",
      "Epoch 214/2000\n",
      "14152/14152 [==============================] - 3s 186us/step - loss: 0.2768 - mse: 0.2768 - mae: 0.3356 - val_loss: 0.3978 - val_mse: 0.3978 - val_mae: 0.3734\n",
      "Epoch 215/2000\n",
      "14152/14152 [==============================] - 3s 197us/step - loss: 0.2801 - mse: 0.2801 - mae: 0.3400 - val_loss: 0.4049 - val_mse: 0.4049 - val_mae: 0.3732\n",
      "Epoch 216/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.2732 - mse: 0.2732 - mae: 0.3354 - val_loss: 0.3883 - val_mse: 0.3883 - val_mae: 0.3639\n",
      "Epoch 217/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.2837 - mse: 0.2837 - mae: 0.3419 - val_loss: 0.3926 - val_mse: 0.3926 - val_mae: 0.3641\n",
      "Epoch 218/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.2777 - mse: 0.2777 - mae: 0.3393 - val_loss: 0.4019 - val_mse: 0.4019 - val_mae: 0.3655\n",
      "Epoch 219/2000\n",
      "14152/14152 [==============================] - 3s 191us/step - loss: 0.2742 - mse: 0.2742 - mae: 0.3346 - val_loss: 0.4012 - val_mse: 0.4012 - val_mae: 0.3815\n",
      "Epoch 220/2000\n",
      "14152/14152 [==============================] - 3s 193us/step - loss: 0.2663 - mse: 0.2663 - mae: 0.3328 - val_loss: 0.3814 - val_mse: 0.3814 - val_mae: 0.3623\n",
      "Epoch 221/2000\n",
      "14152/14152 [==============================] - 3s 189us/step - loss: 0.2744 - mse: 0.2744 - mae: 0.3364 - val_loss: 0.3719 - val_mse: 0.3719 - val_mae: 0.3592\n",
      "Epoch 222/2000\n",
      "14152/14152 [==============================] - 3s 195us/step - loss: 0.2811 - mse: 0.2811 - mae: 0.3424 - val_loss: 0.3839 - val_mse: 0.3839 - val_mae: 0.3637\n",
      "Epoch 223/2000\n",
      "14152/14152 [==============================] - 3s 210us/step - loss: 0.2836 - mse: 0.2836 - mae: 0.3388 - val_loss: 0.3998 - val_mse: 0.3998 - val_mae: 0.3669\n",
      "Epoch 224/2000\n",
      "14152/14152 [==============================] - 3s 197us/step - loss: 0.2734 - mse: 0.2734 - mae: 0.3340 - val_loss: 0.3909 - val_mse: 0.3909 - val_mae: 0.3729\n",
      "Epoch 225/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.2736 - mse: 0.2736 - mae: 0.3362 - val_loss: 0.3982 - val_mse: 0.3982 - val_mae: 0.3638\n",
      "Epoch 226/2000\n",
      "14152/14152 [==============================] - 3s 206us/step - loss: 0.2733 - mse: 0.2733 - mae: 0.3344 - val_loss: 0.3753 - val_mse: 0.3753 - val_mae: 0.3628\n",
      "Epoch 227/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.2785 - mse: 0.2785 - mae: 0.3357 - val_loss: 0.3826 - val_mse: 0.3826 - val_mae: 0.3632\n",
      "Epoch 228/2000\n",
      "14152/14152 [==============================] - 3s 181us/step - loss: 0.2743 - mse: 0.2743 - mae: 0.3357 - val_loss: 0.3963 - val_mse: 0.3963 - val_mae: 0.3725\n",
      "Epoch 229/2000\n",
      "14152/14152 [==============================] - 3s 178us/step - loss: 0.2640 - mse: 0.2640 - mae: 0.3310 - val_loss: 0.3865 - val_mse: 0.3865 - val_mae: 0.3653\n",
      "Epoch 230/2000\n",
      "14152/14152 [==============================] - 3s 178us/step - loss: 0.2750 - mse: 0.2750 - mae: 0.3357 - val_loss: 0.3957 - val_mse: 0.3957 - val_mae: 0.3693\n",
      "Epoch 231/2000\n",
      "14152/14152 [==============================] - 3s 178us/step - loss: 0.2678 - mse: 0.2678 - mae: 0.3291 - val_loss: 0.3767 - val_mse: 0.3767 - val_mae: 0.3594\n",
      "Epoch 232/2000\n",
      "14152/14152 [==============================] - 3s 178us/step - loss: 0.2741 - mse: 0.2741 - mae: 0.3331 - val_loss: 0.3904 - val_mse: 0.3904 - val_mae: 0.3686\n",
      "Epoch 233/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2591 - mse: 0.2591 - mae: 0.3253 - val_loss: 0.3865 - val_mse: 0.3865 - val_mae: 0.3604\n",
      "Epoch 234/2000\n",
      "14152/14152 [==============================] - 3s 191us/step - loss: 0.2701 - mse: 0.2701 - mae: 0.3302 - val_loss: 0.3862 - val_mse: 0.3862 - val_mae: 0.3636\n",
      "Epoch 235/2000\n",
      "14152/14152 [==============================] - 3s 186us/step - loss: 0.2645 - mse: 0.2645 - mae: 0.3281 - val_loss: 0.3875 - val_mse: 0.3875 - val_mae: 0.3628\n",
      "Epoch 236/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14152/14152 [==============================] - 3s 187us/step - loss: 0.2746 - mse: 0.2746 - mae: 0.3325 - val_loss: 0.3997 - val_mse: 0.3997 - val_mae: 0.3701\n",
      "Epoch 237/2000\n",
      "14152/14152 [==============================] - 3s 182us/step - loss: 0.2685 - mse: 0.2685 - mae: 0.3327 - val_loss: 0.3961 - val_mse: 0.3961 - val_mae: 0.3686\n",
      "Epoch 238/2000\n",
      "14152/14152 [==============================] - 3s 190us/step - loss: 0.2698 - mse: 0.2698 - mae: 0.3312 - val_loss: 0.3878 - val_mse: 0.3878 - val_mae: 0.3652\n",
      "Epoch 239/2000\n",
      "14152/14152 [==============================] - 3s 198us/step - loss: 0.2727 - mse: 0.2727 - mae: 0.3335 - val_loss: 0.4028 - val_mse: 0.4028 - val_mae: 0.3679\n",
      "Epoch 240/2000\n",
      "14152/14152 [==============================] - 3s 198us/step - loss: 0.2761 - mse: 0.2761 - mae: 0.3356 - val_loss: 0.3976 - val_mse: 0.3976 - val_mae: 0.3732\n",
      "Epoch 241/2000\n",
      "14152/14152 [==============================] - 3s 190us/step - loss: 0.2640 - mse: 0.2640 - mae: 0.3298 - val_loss: 0.3922 - val_mse: 0.3922 - val_mae: 0.3619\n",
      "Epoch 242/2000\n",
      "14152/14152 [==============================] - 3s 189us/step - loss: 0.2704 - mse: 0.2704 - mae: 0.3307 - val_loss: 0.3835 - val_mse: 0.3835 - val_mae: 0.3626\n",
      "Epoch 243/2000\n",
      "14152/14152 [==============================] - 3s 187us/step - loss: 0.2691 - mse: 0.2691 - mae: 0.3336 - val_loss: 0.3806 - val_mse: 0.3806 - val_mae: 0.3635\n",
      "Epoch 244/2000\n",
      "14152/14152 [==============================] - 3s 178us/step - loss: 0.2643 - mse: 0.2643 - mae: 0.3289 - val_loss: 0.3771 - val_mse: 0.3771 - val_mae: 0.3629\n",
      "Epoch 245/2000\n",
      "14152/14152 [==============================] - 3s 178us/step - loss: 0.2639 - mse: 0.2639 - mae: 0.3232 - val_loss: 0.3912 - val_mse: 0.3912 - val_mae: 0.3645\n",
      "Epoch 246/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.2570 - mse: 0.2570 - mae: 0.3228 - val_loss: 0.4002 - val_mse: 0.4002 - val_mae: 0.3682\n",
      "Epoch 247/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2724 - mse: 0.2724 - mae: 0.3328 - val_loss: 0.3955 - val_mse: 0.3955 - val_mae: 0.3715\n",
      "Epoch 248/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.2716 - mse: 0.2716 - mae: 0.3326 - val_loss: 0.3731 - val_mse: 0.3731 - val_mae: 0.3541\n",
      "Epoch 249/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.2602 - mse: 0.2602 - mae: 0.3243 - val_loss: 0.3878 - val_mse: 0.3878 - val_mae: 0.3670\n",
      "Epoch 250/2000\n",
      "14152/14152 [==============================] - 3s 191us/step - loss: 0.2740 - mse: 0.2740 - mae: 0.3336 - val_loss: 0.3730 - val_mse: 0.3730 - val_mae: 0.3579\n",
      "Epoch 251/2000\n",
      "14152/14152 [==============================] - 3s 194us/step - loss: 0.2659 - mse: 0.2659 - mae: 0.3282 - val_loss: 0.4014 - val_mse: 0.4014 - val_mae: 0.3724\n",
      "Epoch 252/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.2574 - mse: 0.2574 - mae: 0.3228 - val_loss: 0.4107 - val_mse: 0.4107 - val_mae: 0.3762\n",
      "Epoch 253/2000\n",
      "14152/14152 [==============================] - 3s 191us/step - loss: 0.2642 - mse: 0.2642 - mae: 0.3275 - val_loss: 0.3986 - val_mse: 0.3986 - val_mae: 0.3621\n",
      "Epoch 254/2000\n",
      "14152/14152 [==============================] - 3s 195us/step - loss: 0.2590 - mse: 0.2590 - mae: 0.3222 - val_loss: 0.3842 - val_mse: 0.3842 - val_mae: 0.3605\n",
      "Epoch 255/2000\n",
      "14152/14152 [==============================] - 3s 186us/step - loss: 0.2496 - mse: 0.2496 - mae: 0.3181 - val_loss: 0.3882 - val_mse: 0.3882 - val_mae: 0.3645\n",
      "Epoch 256/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.2596 - mse: 0.2596 - mae: 0.3244 - val_loss: 0.4014 - val_mse: 0.4014 - val_mae: 0.3612\n",
      "Epoch 257/2000\n",
      "14152/14152 [==============================] - 3s 200us/step - loss: 0.2735 - mse: 0.2735 - mae: 0.3305 - val_loss: 0.3823 - val_mse: 0.3823 - val_mae: 0.3584\n",
      "Epoch 258/2000\n",
      "14152/14152 [==============================] - 3s 199us/step - loss: 0.2635 - mse: 0.2635 - mae: 0.3272 - val_loss: 0.3861 - val_mse: 0.3861 - val_mae: 0.3587\n",
      "Epoch 259/2000\n",
      "14152/14152 [==============================] - 3s 190us/step - loss: 0.2585 - mse: 0.2585 - mae: 0.3240 - val_loss: 0.3866 - val_mse: 0.3866 - val_mae: 0.3609\n",
      "Epoch 260/2000\n",
      "14152/14152 [==============================] - 3s 186us/step - loss: 0.2725 - mse: 0.2725 - mae: 0.3295 - val_loss: 0.3827 - val_mse: 0.3827 - val_mae: 0.3636\n",
      "Epoch 261/2000\n",
      "14152/14152 [==============================] - 3s 187us/step - loss: 0.2544 - mse: 0.2544 - mae: 0.3187 - val_loss: 0.4004 - val_mse: 0.4004 - val_mae: 0.3615\n",
      "Epoch 262/2000\n",
      "14152/14152 [==============================] - 3s 187us/step - loss: 0.2566 - mse: 0.2566 - mae: 0.3205 - val_loss: 0.3963 - val_mse: 0.3963 - val_mae: 0.3622\n",
      "Epoch 263/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.2538 - mse: 0.2538 - mae: 0.3193 - val_loss: 0.3795 - val_mse: 0.3795 - val_mae: 0.3496\n",
      "Epoch 264/2000\n",
      "14152/14152 [==============================] - 3s 189us/step - loss: 0.2626 - mse: 0.2626 - mae: 0.3235 - val_loss: 0.4264 - val_mse: 0.4264 - val_mae: 0.3809\n",
      "Epoch 265/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.2614 - mse: 0.2614 - mae: 0.3239 - val_loss: 0.3819 - val_mse: 0.3819 - val_mae: 0.3606\n",
      "Epoch 266/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.2580 - mse: 0.2580 - mae: 0.3220 - val_loss: 0.3877 - val_mse: 0.3877 - val_mae: 0.3545\n",
      "Epoch 267/2000\n",
      "14152/14152 [==============================] - 3s 200us/step - loss: 0.2581 - mse: 0.2581 - mae: 0.3233 - val_loss: 0.3948 - val_mse: 0.3948 - val_mae: 0.3610\n",
      "Epoch 268/2000\n",
      "14152/14152 [==============================] - 3s 203us/step - loss: 0.2509 - mse: 0.2509 - mae: 0.3181 - val_loss: 0.4018 - val_mse: 0.4018 - val_mae: 0.3606\n",
      "Epoch 269/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.2599 - mse: 0.2599 - mae: 0.3205 - val_loss: 0.3955 - val_mse: 0.3955 - val_mae: 0.3650\n",
      "Epoch 270/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.2523 - mse: 0.2523 - mae: 0.3182 - val_loss: 0.3679 - val_mse: 0.3679 - val_mae: 0.3489\n",
      "Epoch 271/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.2601 - mse: 0.2601 - mae: 0.3202 - val_loss: 0.3779 - val_mse: 0.3779 - val_mae: 0.3530\n",
      "Epoch 272/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.2572 - mse: 0.2572 - mae: 0.3165 - val_loss: 0.3948 - val_mse: 0.3948 - val_mae: 0.3628\n",
      "Epoch 273/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.2545 - mse: 0.2545 - mae: 0.3207 - val_loss: 0.3933 - val_mse: 0.3933 - val_mae: 0.3622\n",
      "Epoch 274/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.2604 - mse: 0.2604 - mae: 0.3212 - val_loss: 0.3731 - val_mse: 0.3731 - val_mae: 0.3577\n",
      "Epoch 275/2000\n",
      "14152/14152 [==============================] - 3s 194us/step - loss: 0.2448 - mse: 0.2448 - mae: 0.3118 - val_loss: 0.3904 - val_mse: 0.3904 - val_mae: 0.3615\n",
      "Epoch 276/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.2439 - mse: 0.2439 - mae: 0.3134 - val_loss: 0.3822 - val_mse: 0.3822 - val_mae: 0.3537\n",
      "Epoch 277/2000\n",
      "14152/14152 [==============================] - 3s 182us/step - loss: 0.2590 - mse: 0.2590 - mae: 0.3206 - val_loss: 0.3880 - val_mse: 0.3880 - val_mae: 0.3595\n",
      "Epoch 278/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.2583 - mse: 0.2583 - mae: 0.3198 - val_loss: 0.3876 - val_mse: 0.3876 - val_mae: 0.3662\n",
      "Epoch 279/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.2610 - mse: 0.2610 - mae: 0.3235 - val_loss: 0.3899 - val_mse: 0.3899 - val_mae: 0.3641\n",
      "Epoch 280/2000\n",
      "14152/14152 [==============================] - 3s 187us/step - loss: 0.2498 - mse: 0.2498 - mae: 0.3168 - val_loss: 0.4145 - val_mse: 0.4145 - val_mae: 0.3693\n",
      "Epoch 281/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.2550 - mse: 0.2550 - mae: 0.3166 - val_loss: 0.3820 - val_mse: 0.3820 - val_mae: 0.3585\n",
      "Epoch 282/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.2605 - mse: 0.2605 - mae: 0.3236 - val_loss: 0.3884 - val_mse: 0.3884 - val_mae: 0.3680\n",
      "Epoch 283/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.2491 - mse: 0.2491 - mae: 0.3145 - val_loss: 0.3989 - val_mse: 0.3989 - val_mae: 0.3598\n",
      "Epoch 284/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.2539 - mse: 0.2539 - mae: 0.3180 - val_loss: 0.3794 - val_mse: 0.3794 - val_mae: 0.3546\n",
      "Epoch 285/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.2693 - mse: 0.2693 - mae: 0.3269 - val_loss: 0.4020 - val_mse: 0.4020 - val_mae: 0.3614\n",
      "Epoch 286/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.2557 - mse: 0.2557 - mae: 0.3204 - val_loss: 0.3802 - val_mse: 0.3802 - val_mae: 0.3579\n",
      "Epoch 287/2000\n",
      "14152/14152 [==============================] - 3s 187us/step - loss: 0.2526 - mse: 0.2526 - mae: 0.3193 - val_loss: 0.3960 - val_mse: 0.3960 - val_mae: 0.3616\n",
      "Epoch 288/2000\n",
      "14152/14152 [==============================] - 3s 192us/step - loss: 0.2459 - mse: 0.2459 - mae: 0.3127 - val_loss: 0.4042 - val_mse: 0.4042 - val_mae: 0.3638\n",
      "Epoch 289/2000\n",
      "14152/14152 [==============================] - 3s 196us/step - loss: 0.2569 - mse: 0.2569 - mae: 0.3190 - val_loss: 0.3867 - val_mse: 0.3867 - val_mae: 0.3623\n",
      "Epoch 290/2000\n",
      "14152/14152 [==============================] - 3s 193us/step - loss: 0.2533 - mse: 0.2533 - mae: 0.3185 - val_loss: 0.4031 - val_mse: 0.4031 - val_mae: 0.3656\n",
      "Epoch 291/2000\n",
      "14152/14152 [==============================] - 3s 198us/step - loss: 0.2580 - mse: 0.2580 - mae: 0.3215 - val_loss: 0.3950 - val_mse: 0.3950 - val_mae: 0.3678\n",
      "Epoch 292/2000\n",
      "14152/14152 [==============================] - 3s 181us/step - loss: 0.2473 - mse: 0.2473 - mae: 0.3159 - val_loss: 0.4013 - val_mse: 0.4013 - val_mae: 0.3653\n",
      "Epoch 293/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.2554 - mse: 0.2554 - mae: 0.3159 - val_loss: 0.3981 - val_mse: 0.3981 - val_mae: 0.3549\n",
      "Epoch 294/2000\n",
      "14152/14152 [==============================] - 3s 181us/step - loss: 0.2566 - mse: 0.2566 - mae: 0.3189 - val_loss: 0.4035 - val_mse: 0.4035 - val_mae: 0.3675\n",
      "Epoch 295/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.2474 - mse: 0.2474 - mae: 0.3146 - val_loss: 0.3965 - val_mse: 0.3965 - val_mae: 0.3629\n",
      "Epoch 296/2000\n",
      "14152/14152 [==============================] - 3s 181us/step - loss: 0.2479 - mse: 0.2479 - mae: 0.3141 - val_loss: 0.3920 - val_mse: 0.3920 - val_mae: 0.3560\n",
      "Epoch 297/2000\n",
      "14152/14152 [==============================] - 3s 192us/step - loss: 0.2528 - mse: 0.2528 - mae: 0.3165 - val_loss: 0.3803 - val_mse: 0.3803 - val_mae: 0.3508\n",
      "Epoch 298/2000\n",
      "14152/14152 [==============================] - 3s 211us/step - loss: 0.2380 - mse: 0.2380 - mae: 0.3088 - val_loss: 0.3957 - val_mse: 0.3957 - val_mae: 0.3572\n",
      "Epoch 299/2000\n",
      "14152/14152 [==============================] - 3s 187us/step - loss: 0.2523 - mse: 0.2523 - mae: 0.3148 - val_loss: 0.3857 - val_mse: 0.3857 - val_mae: 0.3564\n",
      "Epoch 300/2000\n",
      "14152/14152 [==============================] - 3s 197us/step - loss: 0.2393 - mse: 0.2393 - mae: 0.3084 - val_loss: 0.3922 - val_mse: 0.3922 - val_mae: 0.3564\n",
      "Epoch 301/2000\n",
      "14152/14152 [==============================] - 3s 192us/step - loss: 0.2492 - mse: 0.2492 - mae: 0.3143 - val_loss: 0.3980 - val_mse: 0.3980 - val_mae: 0.3615\n",
      "Epoch 302/2000\n",
      "14152/14152 [==============================] - 3s 199us/step - loss: 0.2479 - mse: 0.2479 - mae: 0.3125 - val_loss: 0.3979 - val_mse: 0.3979 - val_mae: 0.3631\n",
      "Epoch 303/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.2535 - mse: 0.2535 - mae: 0.3172 - val_loss: 0.4028 - val_mse: 0.4028 - val_mae: 0.3602\n",
      "Epoch 304/2000\n",
      "14152/14152 [==============================] - 3s 186us/step - loss: 0.2452 - mse: 0.2452 - mae: 0.3108 - val_loss: 0.4079 - val_mse: 0.4079 - val_mae: 0.3699\n",
      "Epoch 305/2000\n",
      "14152/14152 [==============================] - 3s 191us/step - loss: 0.2473 - mse: 0.2473 - mae: 0.3142 - val_loss: 0.4135 - val_mse: 0.4135 - val_mae: 0.3623\n",
      "Epoch 306/2000\n",
      "14152/14152 [==============================] - 3s 197us/step - loss: 0.2417 - mse: 0.2417 - mae: 0.3070 - val_loss: 0.3956 - val_mse: 0.3956 - val_mae: 0.3608\n",
      "Epoch 307/2000\n",
      "14152/14152 [==============================] - 3s 191us/step - loss: 0.2528 - mse: 0.2528 - mae: 0.3154 - val_loss: 0.4080 - val_mse: 0.4080 - val_mae: 0.3653\n",
      "Epoch 308/2000\n",
      "14152/14152 [==============================] - 3s 194us/step - loss: 0.2382 - mse: 0.2382 - mae: 0.3077 - val_loss: 0.3966 - val_mse: 0.3966 - val_mae: 0.3611\n",
      "Epoch 309/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.2436 - mse: 0.2436 - mae: 0.3126 - val_loss: 0.4139 - val_mse: 0.4139 - val_mae: 0.3762\n",
      "Epoch 310/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.2438 - mse: 0.2438 - mae: 0.3106 - val_loss: 0.3976 - val_mse: 0.3976 - val_mae: 0.3625\n",
      "Epoch 311/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2488 - mse: 0.2488 - mae: 0.3135 - val_loss: 0.3973 - val_mse: 0.3973 - val_mae: 0.3571\n",
      "Epoch 312/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2411 - mse: 0.2411 - mae: 0.3086 - val_loss: 0.3994 - val_mse: 0.3994 - val_mae: 0.3582\n",
      "Epoch 313/2000\n",
      "14152/14152 [==============================] - 3s 181us/step - loss: 0.2432 - mse: 0.2432 - mae: 0.3108 - val_loss: 0.4000 - val_mse: 0.4000 - val_mae: 0.3604\n",
      "Epoch 314/2000\n",
      "14152/14152 [==============================] - 3s 186us/step - loss: 0.2417 - mse: 0.2417 - mae: 0.3090 - val_loss: 0.3918 - val_mse: 0.3918 - val_mae: 0.3528\n",
      "Epoch 315/2000\n",
      "14152/14152 [==============================] - 3s 187us/step - loss: 0.2437 - mse: 0.2437 - mae: 0.3097 - val_loss: 0.3946 - val_mse: 0.3946 - val_mae: 0.3538\n",
      "Epoch 316/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.2470 - mse: 0.2470 - mae: 0.3119 - val_loss: 0.3868 - val_mse: 0.3868 - val_mae: 0.3565\n",
      "Epoch 317/2000\n",
      "14152/14152 [==============================] - 3s 186us/step - loss: 0.2409 - mse: 0.2409 - mae: 0.3079 - val_loss: 0.4020 - val_mse: 0.4020 - val_mae: 0.3614\n",
      "Epoch 318/2000\n",
      "14152/14152 [==============================] - 3s 193us/step - loss: 0.2333 - mse: 0.2333 - mae: 0.3024 - val_loss: 0.4042 - val_mse: 0.4042 - val_mae: 0.3631\n",
      "Epoch 319/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.2306 - mse: 0.2306 - mae: 0.2998 - val_loss: 0.3961 - val_mse: 0.3961 - val_mae: 0.3622\n",
      "Epoch 320/2000\n",
      "14152/14152 [==============================] - 3s 181us/step - loss: 0.2441 - mse: 0.2441 - mae: 0.3111 - val_loss: 0.3993 - val_mse: 0.3993 - val_mae: 0.3603\n",
      "Epoch 321/2000\n",
      "14152/14152 [==============================] - 3s 191us/step - loss: 0.2390 - mse: 0.2390 - mae: 0.3070 - val_loss: 0.4028 - val_mse: 0.4028 - val_mae: 0.3639\n",
      "Epoch 322/2000\n",
      "14152/14152 [==============================] - 3s 200us/step - loss: 0.2436 - mse: 0.2436 - mae: 0.3064 - val_loss: 0.3879 - val_mse: 0.3879 - val_mae: 0.3560\n",
      "Epoch 323/2000\n",
      "14152/14152 [==============================] - 3s 211us/step - loss: 0.2457 - mse: 0.2457 - mae: 0.3138 - val_loss: 0.3775 - val_mse: 0.3775 - val_mae: 0.3499\n",
      "Epoch 324/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.2437 - mse: 0.2437 - mae: 0.3083 - val_loss: 0.3994 - val_mse: 0.3994 - val_mae: 0.3585\n",
      "Epoch 325/2000\n",
      "14152/14152 [==============================] - 3s 190us/step - loss: 0.2411 - mse: 0.2411 - mae: 0.3093 - val_loss: 0.4246 - val_mse: 0.4246 - val_mae: 0.3713\n",
      "Epoch 326/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.2490 - mse: 0.2490 - mae: 0.3124 - val_loss: 0.4007 - val_mse: 0.4007 - val_mae: 0.3634\n",
      "Epoch 327/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.2381 - mse: 0.2381 - mae: 0.3046 - val_loss: 0.4024 - val_mse: 0.4024 - val_mae: 0.3592\n",
      "Epoch 328/2000\n",
      "14152/14152 [==============================] - 3s 189us/step - loss: 0.2358 - mse: 0.2358 - mae: 0.3060 - val_loss: 0.4088 - val_mse: 0.4088 - val_mae: 0.3632\n",
      "Epoch 329/2000\n",
      "14152/14152 [==============================] - 3s 192us/step - loss: 0.2366 - mse: 0.2366 - mae: 0.3047 - val_loss: 0.3940 - val_mse: 0.3940 - val_mae: 0.3601\n",
      "Epoch 330/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.2394 - mse: 0.2394 - mae: 0.3081 - val_loss: 0.3965 - val_mse: 0.3965 - val_mae: 0.3562\n",
      "Epoch 331/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.2266 - mse: 0.2266 - mae: 0.2992 - val_loss: 0.4159 - val_mse: 0.4159 - val_mae: 0.3677\n",
      "Epoch 332/2000\n",
      "14152/14152 [==============================] - 3s 187us/step - loss: 0.2429 - mse: 0.2429 - mae: 0.3079 - val_loss: 0.4041 - val_mse: 0.4041 - val_mae: 0.3671\n",
      "Epoch 333/2000\n",
      "14152/14152 [==============================] - 3s 187us/step - loss: 0.2330 - mse: 0.2330 - mae: 0.3012 - val_loss: 0.3929 - val_mse: 0.3929 - val_mae: 0.3593\n",
      "Epoch 334/2000\n",
      "14152/14152 [==============================] - 3s 181us/step - loss: 0.2404 - mse: 0.2404 - mae: 0.3065 - val_loss: 0.4025 - val_mse: 0.4025 - val_mae: 0.3642\n",
      "Epoch 335/2000\n",
      "14152/14152 [==============================] - 3s 181us/step - loss: 0.2320 - mse: 0.2320 - mae: 0.3060 - val_loss: 0.4046 - val_mse: 0.4046 - val_mae: 0.3563\n",
      "Epoch 336/2000\n",
      "14152/14152 [==============================] - 3s 190us/step - loss: 0.2426 - mse: 0.2426 - mae: 0.3065 - val_loss: 0.3945 - val_mse: 0.3945 - val_mae: 0.3557\n",
      "Epoch 337/2000\n",
      "14152/14152 [==============================] - 3s 191us/step - loss: 0.2441 - mse: 0.2441 - mae: 0.3094 - val_loss: 0.4276 - val_mse: 0.4276 - val_mae: 0.3685\n",
      "Epoch 338/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.2475 - mse: 0.2475 - mae: 0.3100 - val_loss: 0.4044 - val_mse: 0.4044 - val_mae: 0.3584\n",
      "Epoch 339/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2313 - mse: 0.2313 - mae: 0.3017 - val_loss: 0.3695 - val_mse: 0.3695 - val_mae: 0.3495\n",
      "Epoch 340/2000\n",
      "14152/14152 [==============================] - 3s 178us/step - loss: 0.2412 - mse: 0.2412 - mae: 0.3086 - val_loss: 0.3992 - val_mse: 0.3992 - val_mae: 0.3547\n",
      "Epoch 341/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.2328 - mse: 0.2328 - mae: 0.3015 - val_loss: 0.3961 - val_mse: 0.3961 - val_mae: 0.3599\n",
      "Epoch 342/2000\n",
      "14152/14152 [==============================] - 3s 181us/step - loss: 0.2331 - mse: 0.2331 - mae: 0.3031 - val_loss: 0.4202 - val_mse: 0.4202 - val_mae: 0.3726\n",
      "Epoch 343/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2372 - mse: 0.2372 - mae: 0.3044 - val_loss: 0.4007 - val_mse: 0.4007 - val_mae: 0.3599\n",
      "Epoch 344/2000\n",
      "14152/14152 [==============================] - 3s 178us/step - loss: 0.2343 - mse: 0.2343 - mae: 0.3032 - val_loss: 0.4017 - val_mse: 0.4017 - val_mae: 0.3565\n",
      "Epoch 345/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2348 - mse: 0.2348 - mae: 0.3020 - val_loss: 0.3909 - val_mse: 0.3909 - val_mae: 0.3544\n",
      "Epoch 346/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2400 - mse: 0.2400 - mae: 0.3044 - val_loss: 0.4025 - val_mse: 0.4025 - val_mae: 0.3575\n",
      "Epoch 347/2000\n",
      "14152/14152 [==============================] - 3s 182us/step - loss: 0.2365 - mse: 0.2365 - mae: 0.3068 - val_loss: 0.3949 - val_mse: 0.3949 - val_mae: 0.3570\n",
      "Epoch 348/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.2287 - mse: 0.2287 - mae: 0.3014 - val_loss: 0.4038 - val_mse: 0.4038 - val_mae: 0.3623\n",
      "Epoch 349/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2329 - mse: 0.2329 - mae: 0.3022 - val_loss: 0.4177 - val_mse: 0.4177 - val_mae: 0.3645\n",
      "Epoch 350/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2301 - mse: 0.2301 - mae: 0.3003 - val_loss: 0.4002 - val_mse: 0.4002 - val_mae: 0.3633\n",
      "Epoch 351/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2285 - mse: 0.2285 - mae: 0.2997 - val_loss: 0.4054 - val_mse: 0.4054 - val_mae: 0.3582\n",
      "Epoch 352/2000\n",
      "14152/14152 [==============================] - 3s 181us/step - loss: 0.2332 - mse: 0.2332 - mae: 0.3017 - val_loss: 0.4230 - val_mse: 0.4230 - val_mae: 0.3652\n",
      "Epoch 353/2000\n",
      "14152/14152 [==============================] - 3s 178us/step - loss: 0.2360 - mse: 0.2360 - mae: 0.3026 - val_loss: 0.4083 - val_mse: 0.4083 - val_mae: 0.3605\n",
      "Epoch 354/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2367 - mse: 0.2367 - mae: 0.3039 - val_loss: 0.4017 - val_mse: 0.4017 - val_mae: 0.3584\n",
      "Epoch 355/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2263 - mse: 0.2263 - mae: 0.2959 - val_loss: 0.4018 - val_mse: 0.4018 - val_mae: 0.3550\n",
      "Epoch 356/2000\n",
      "14152/14152 [==============================] - 3s 192us/step - loss: 0.2318 - mse: 0.2318 - mae: 0.3013 - val_loss: 0.3850 - val_mse: 0.3850 - val_mae: 0.3515\n",
      "Epoch 357/2000\n",
      "14152/14152 [==============================] - 3s 178us/step - loss: 0.2301 - mse: 0.2301 - mae: 0.2981 - val_loss: 0.3998 - val_mse: 0.3998 - val_mae: 0.3642\n",
      "Epoch 358/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2259 - mse: 0.2259 - mae: 0.2970 - val_loss: 0.4023 - val_mse: 0.4023 - val_mae: 0.3625\n",
      "Epoch 359/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2312 - mse: 0.2312 - mae: 0.3013 - val_loss: 0.4012 - val_mse: 0.4012 - val_mae: 0.3559\n",
      "Epoch 360/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2318 - mse: 0.2318 - mae: 0.2998 - val_loss: 0.4126 - val_mse: 0.4126 - val_mae: 0.3623\n",
      "Epoch 361/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2264 - mse: 0.2264 - mae: 0.2984 - val_loss: 0.4053 - val_mse: 0.4053 - val_mae: 0.3603\n",
      "Epoch 362/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2353 - mse: 0.2353 - mae: 0.3019 - val_loss: 0.3690 - val_mse: 0.3690 - val_mae: 0.3442\n",
      "Epoch 363/2000\n",
      "14152/14152 [==============================] - 3s 178us/step - loss: 0.2324 - mse: 0.2324 - mae: 0.2979 - val_loss: 0.4031 - val_mse: 0.4031 - val_mae: 0.3554\n",
      "Epoch 364/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2329 - mse: 0.2329 - mae: 0.2993 - val_loss: 0.3973 - val_mse: 0.3973 - val_mae: 0.3531\n",
      "Epoch 365/2000\n",
      "14152/14152 [==============================] - 3s 178us/step - loss: 0.2313 - mse: 0.2313 - mae: 0.2974 - val_loss: 0.3753 - val_mse: 0.3753 - val_mae: 0.3470\n",
      "Epoch 366/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2266 - mse: 0.2266 - mae: 0.2967 - val_loss: 0.4004 - val_mse: 0.4004 - val_mae: 0.3570\n",
      "Epoch 367/2000\n",
      "14152/14152 [==============================] - 3s 178us/step - loss: 0.2315 - mse: 0.2315 - mae: 0.2997 - val_loss: 0.4103 - val_mse: 0.4103 - val_mae: 0.3589\n",
      "Epoch 368/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2309 - mse: 0.2309 - mae: 0.2999 - val_loss: 0.4018 - val_mse: 0.4018 - val_mae: 0.3619\n",
      "Epoch 369/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.2272 - mse: 0.2272 - mae: 0.2981 - val_loss: 0.3927 - val_mse: 0.3927 - val_mae: 0.3549\n",
      "Epoch 370/2000\n",
      "14152/14152 [==============================] - 3s 178us/step - loss: 0.2332 - mse: 0.2332 - mae: 0.3014 - val_loss: 0.3871 - val_mse: 0.3871 - val_mae: 0.3531\n",
      "Epoch 371/2000\n",
      "14152/14152 [==============================] - 3s 181us/step - loss: 0.2281 - mse: 0.2281 - mae: 0.2970 - val_loss: 0.4064 - val_mse: 0.4064 - val_mae: 0.3671\n",
      "Epoch 372/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.2239 - mse: 0.2239 - mae: 0.2950 - val_loss: 0.3944 - val_mse: 0.3944 - val_mae: 0.3585\n",
      "Epoch 373/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.2214 - mse: 0.2214 - mae: 0.2942 - val_loss: 0.3907 - val_mse: 0.3907 - val_mae: 0.3533\n",
      "Epoch 374/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.2229 - mse: 0.2229 - mae: 0.2931 - val_loss: 0.3895 - val_mse: 0.3895 - val_mae: 0.3552\n",
      "Epoch 375/2000\n",
      "14152/14152 [==============================] - 3s 191us/step - loss: 0.2254 - mse: 0.2254 - mae: 0.2954 - val_loss: 0.3949 - val_mse: 0.3949 - val_mae: 0.3545\n",
      "Epoch 376/2000\n",
      "14152/14152 [==============================] - 3s 200us/step - loss: 0.2164 - mse: 0.2164 - mae: 0.2884 - val_loss: 0.3882 - val_mse: 0.3882 - val_mae: 0.3507\n",
      "Epoch 377/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14152/14152 [==============================] - 3s 196us/step - loss: 0.2400 - mse: 0.2400 - mae: 0.3039 - val_loss: 0.4069 - val_mse: 0.4069 - val_mae: 0.3596\n",
      "Epoch 378/2000\n",
      "14152/14152 [==============================] - 3s 196us/step - loss: 0.2154 - mse: 0.2154 - mae: 0.2903 - val_loss: 0.4015 - val_mse: 0.4015 - val_mae: 0.3572\n",
      "Epoch 379/2000\n",
      "14152/14152 [==============================] - 3s 192us/step - loss: 0.2277 - mse: 0.2277 - mae: 0.2957 - val_loss: 0.3973 - val_mse: 0.3973 - val_mae: 0.3559\n",
      "Epoch 380/2000\n",
      "14152/14152 [==============================] - 3s 193us/step - loss: 0.2224 - mse: 0.2224 - mae: 0.2940 - val_loss: 0.3920 - val_mse: 0.3920 - val_mae: 0.3533\n",
      "Epoch 381/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.2292 - mse: 0.2292 - mae: 0.2971 - val_loss: 0.4109 - val_mse: 0.4109 - val_mae: 0.3647\n",
      "Epoch 382/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.2212 - mse: 0.2212 - mae: 0.2922 - val_loss: 0.3993 - val_mse: 0.3993 - val_mae: 0.3568\n",
      "Epoch 383/2000\n",
      "14152/14152 [==============================] - 3s 189us/step - loss: 0.2257 - mse: 0.2257 - mae: 0.2956 - val_loss: 0.4021 - val_mse: 0.4021 - val_mae: 0.3593\n",
      "Epoch 384/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.2221 - mse: 0.2221 - mae: 0.2934 - val_loss: 0.3795 - val_mse: 0.3795 - val_mae: 0.3442\n",
      "Epoch 385/2000\n",
      "14152/14152 [==============================] - 3s 194us/step - loss: 0.2214 - mse: 0.2214 - mae: 0.2933 - val_loss: 0.3999 - val_mse: 0.3999 - val_mae: 0.3516\n",
      "Epoch 386/2000\n",
      "14152/14152 [==============================] - 3s 177us/step - loss: 0.2222 - mse: 0.2222 - mae: 0.2936 - val_loss: 0.3867 - val_mse: 0.3867 - val_mae: 0.3499\n",
      "Epoch 387/2000\n",
      "14152/14152 [==============================] - 3s 186us/step - loss: 0.2224 - mse: 0.2224 - mae: 0.2940 - val_loss: 0.4011 - val_mse: 0.4011 - val_mae: 0.3527\n",
      "Epoch 388/2000\n",
      "14152/14152 [==============================] - 3s 187us/step - loss: 0.2205 - mse: 0.2205 - mae: 0.2910 - val_loss: 0.3942 - val_mse: 0.3942 - val_mae: 0.3561\n",
      "Epoch 389/2000\n",
      "14152/14152 [==============================] - 3s 200us/step - loss: 0.2137 - mse: 0.2137 - mae: 0.2889 - val_loss: 0.3846 - val_mse: 0.3846 - val_mae: 0.3526\n",
      "Epoch 390/2000\n",
      "14152/14152 [==============================] - 3s 189us/step - loss: 0.2270 - mse: 0.2270 - mae: 0.2947 - val_loss: 0.4148 - val_mse: 0.4148 - val_mae: 0.3627\n",
      "Epoch 391/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.2341 - mse: 0.2341 - mae: 0.2989 - val_loss: 0.4205 - val_mse: 0.4205 - val_mae: 0.3613\n",
      "Epoch 392/2000\n",
      "14152/14152 [==============================] - 3s 181us/step - loss: 0.2198 - mse: 0.2198 - mae: 0.2903 - val_loss: 0.4014 - val_mse: 0.4014 - val_mae: 0.3605\n",
      "Epoch 393/2000\n",
      "14152/14152 [==============================] - 3s 189us/step - loss: 0.2154 - mse: 0.2154 - mae: 0.2886 - val_loss: 0.3850 - val_mse: 0.3850 - val_mae: 0.3502\n",
      "Epoch 394/2000\n",
      "14152/14152 [==============================] - 3s 181us/step - loss: 0.2160 - mse: 0.2160 - mae: 0.2922 - val_loss: 0.3929 - val_mse: 0.3929 - val_mae: 0.3543\n",
      "Epoch 395/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.2179 - mse: 0.2179 - mae: 0.2908 - val_loss: 0.3949 - val_mse: 0.3949 - val_mae: 0.3580\n",
      "Epoch 396/2000\n",
      "14152/14152 [==============================] - 3s 190us/step - loss: 0.2236 - mse: 0.2236 - mae: 0.2928 - val_loss: 0.4059 - val_mse: 0.4059 - val_mae: 0.3604\n",
      "Epoch 397/2000\n",
      "14152/14152 [==============================] - 3s 182us/step - loss: 0.2267 - mse: 0.2267 - mae: 0.2976 - val_loss: 0.4081 - val_mse: 0.4081 - val_mae: 0.3587\n",
      "Epoch 398/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.2134 - mse: 0.2134 - mae: 0.2862 - val_loss: 0.3939 - val_mse: 0.3939 - val_mae: 0.3543\n",
      "Epoch 399/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.2152 - mse: 0.2152 - mae: 0.2878 - val_loss: 0.3990 - val_mse: 0.3990 - val_mae: 0.3558\n",
      "Epoch 400/2000\n",
      "14152/14152 [==============================] - 3s 196us/step - loss: 0.2224 - mse: 0.2224 - mae: 0.2923 - val_loss: 0.4181 - val_mse: 0.4181 - val_mae: 0.3636\n",
      "Epoch 401/2000\n",
      "14152/14152 [==============================] - 3s 189us/step - loss: 0.2296 - mse: 0.2296 - mae: 0.2967 - val_loss: 0.4044 - val_mse: 0.4044 - val_mae: 0.3562\n",
      "Epoch 402/2000\n",
      "14152/14152 [==============================] - 3s 189us/step - loss: 0.2085 - mse: 0.2085 - mae: 0.2838 - val_loss: 0.4035 - val_mse: 0.4035 - val_mae: 0.3590\n",
      "Epoch 403/2000\n",
      "14152/14152 [==============================] - 3s 186us/step - loss: 0.2171 - mse: 0.2171 - mae: 0.2918 - val_loss: 0.4033 - val_mse: 0.4033 - val_mae: 0.3561\n",
      "Epoch 404/2000\n",
      "14152/14152 [==============================] - 3s 197us/step - loss: 0.2165 - mse: 0.2165 - mae: 0.2887 - val_loss: 0.4008 - val_mse: 0.4008 - val_mae: 0.3516\n",
      "Epoch 405/2000\n",
      "14152/14152 [==============================] - 3s 197us/step - loss: 0.2227 - mse: 0.2227 - mae: 0.2927 - val_loss: 0.3937 - val_mse: 0.3937 - val_mae: 0.3493\n",
      "Epoch 406/2000\n",
      "14152/14152 [==============================] - 3s 194us/step - loss: 0.2236 - mse: 0.2236 - mae: 0.2956 - val_loss: 0.4018 - val_mse: 0.4018 - val_mae: 0.3557\n",
      "Epoch 407/2000\n",
      "14152/14152 [==============================] - 3s 199us/step - loss: 0.2277 - mse: 0.2277 - mae: 0.2951 - val_loss: 0.3976 - val_mse: 0.3976 - val_mae: 0.3557\n",
      "Epoch 408/2000\n",
      "14152/14152 [==============================] - 3s 198us/step - loss: 0.2227 - mse: 0.2227 - mae: 0.2931 - val_loss: 0.3841 - val_mse: 0.3841 - val_mae: 0.3513\n",
      "Epoch 409/2000\n",
      "14152/14152 [==============================] - 3s 190us/step - loss: 0.2193 - mse: 0.2193 - mae: 0.2908 - val_loss: 0.3934 - val_mse: 0.3934 - val_mae: 0.3505\n",
      "Epoch 410/2000\n",
      "14152/14152 [==============================] - 3s 186us/step - loss: 0.2164 - mse: 0.2164 - mae: 0.2868 - val_loss: 0.3885 - val_mse: 0.3885 - val_mae: 0.3474\n",
      "Epoch 411/2000\n",
      "14152/14152 [==============================] - 3s 190us/step - loss: 0.2203 - mse: 0.2203 - mae: 0.2916 - val_loss: 0.3919 - val_mse: 0.3919 - val_mae: 0.3500\n",
      "Epoch 412/2000\n",
      "14152/14152 [==============================] - 3s 186us/step - loss: 0.2114 - mse: 0.2114 - mae: 0.2843 - val_loss: 0.3903 - val_mse: 0.3903 - val_mae: 0.3502\n",
      "Epoch 413/2000\n",
      "14152/14152 [==============================] - 3s 191us/step - loss: 0.2197 - mse: 0.2197 - mae: 0.2916 - val_loss: 0.3891 - val_mse: 0.3891 - val_mae: 0.3533\n",
      "Epoch 414/2000\n",
      "14152/14152 [==============================] - 3s 187us/step - loss: 0.2211 - mse: 0.2211 - mae: 0.2902 - val_loss: 0.3931 - val_mse: 0.3931 - val_mae: 0.3525\n",
      "Epoch 415/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.2148 - mse: 0.2148 - mae: 0.2871 - val_loss: 0.3896 - val_mse: 0.3896 - val_mae: 0.3521\n",
      "Epoch 416/2000\n",
      "14152/14152 [==============================] - 3s 181us/step - loss: 0.2180 - mse: 0.2180 - mae: 0.2881 - val_loss: 0.4009 - val_mse: 0.4009 - val_mae: 0.3558\n",
      "Epoch 417/2000\n",
      "14152/14152 [==============================] - 3s 182us/step - loss: 0.2198 - mse: 0.2198 - mae: 0.2927 - val_loss: 0.3961 - val_mse: 0.3961 - val_mae: 0.3505\n",
      "Epoch 418/2000\n",
      "14152/14152 [==============================] - 3s 194us/step - loss: 0.2063 - mse: 0.2063 - mae: 0.2801 - val_loss: 0.3886 - val_mse: 0.3886 - val_mae: 0.3502\n",
      "Epoch 419/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.2165 - mse: 0.2165 - mae: 0.2869 - val_loss: 0.3839 - val_mse: 0.3839 - val_mae: 0.3488\n",
      "Epoch 420/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2230 - mse: 0.2230 - mae: 0.2902 - val_loss: 0.3981 - val_mse: 0.3981 - val_mae: 0.3521\n",
      "Epoch 421/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.2090 - mse: 0.2090 - mae: 0.2823 - val_loss: 0.4074 - val_mse: 0.4074 - val_mae: 0.3546\n",
      "Epoch 422/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.2154 - mse: 0.2154 - mae: 0.2863 - val_loss: 0.3932 - val_mse: 0.3932 - val_mae: 0.3555\n",
      "Epoch 423/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.2124 - mse: 0.2124 - mae: 0.2864 - val_loss: 0.3859 - val_mse: 0.3859 - val_mae: 0.3494\n",
      "Epoch 424/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2161 - mse: 0.2161 - mae: 0.2873 - val_loss: 0.4088 - val_mse: 0.4088 - val_mae: 0.3553\n",
      "Epoch 425/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2182 - mse: 0.2182 - mae: 0.2889 - val_loss: 0.4067 - val_mse: 0.4067 - val_mae: 0.3562\n",
      "Epoch 426/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.2225 - mse: 0.2225 - mae: 0.2901 - val_loss: 0.4037 - val_mse: 0.4037 - val_mae: 0.3520\n",
      "Epoch 427/2000\n",
      "14152/14152 [==============================] - 3s 181us/step - loss: 0.2189 - mse: 0.2189 - mae: 0.2876 - val_loss: 0.3875 - val_mse: 0.3875 - val_mae: 0.3495\n",
      "Epoch 428/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.2115 - mse: 0.2115 - mae: 0.2850 - val_loss: 0.3964 - val_mse: 0.3964 - val_mae: 0.3586\n",
      "Epoch 429/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.2120 - mse: 0.2120 - mae: 0.2856 - val_loss: 0.3931 - val_mse: 0.3931 - val_mae: 0.3496\n",
      "Epoch 430/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.2153 - mse: 0.2153 - mae: 0.2871 - val_loss: 0.4113 - val_mse: 0.4113 - val_mae: 0.3554\n",
      "Epoch 431/2000\n",
      "14152/14152 [==============================] - 3s 181us/step - loss: 0.2218 - mse: 0.2218 - mae: 0.2921 - val_loss: 0.4019 - val_mse: 0.4019 - val_mae: 0.3560\n",
      "Epoch 432/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2119 - mse: 0.2119 - mae: 0.2836 - val_loss: 0.3899 - val_mse: 0.3899 - val_mae: 0.3531\n",
      "Epoch 433/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2126 - mse: 0.2126 - mae: 0.2853 - val_loss: 0.3965 - val_mse: 0.3965 - val_mae: 0.3516\n",
      "Epoch 434/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.2107 - mse: 0.2107 - mae: 0.2845 - val_loss: 0.3952 - val_mse: 0.3952 - val_mae: 0.3614\n",
      "Epoch 435/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.2175 - mse: 0.2175 - mae: 0.2897 - val_loss: 0.3996 - val_mse: 0.3996 - val_mae: 0.3523\n",
      "Epoch 436/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.2144 - mse: 0.2144 - mae: 0.2853 - val_loss: 0.3970 - val_mse: 0.3970 - val_mae: 0.3595\n",
      "Epoch 437/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.2223 - mse: 0.2223 - mae: 0.2894 - val_loss: 0.4122 - val_mse: 0.4122 - val_mae: 0.3585\n",
      "Epoch 438/2000\n",
      "14152/14152 [==============================] - 3s 197us/step - loss: 0.2191 - mse: 0.2191 - mae: 0.2893 - val_loss: 0.3921 - val_mse: 0.3921 - val_mae: 0.3481\n",
      "Epoch 439/2000\n",
      "14152/14152 [==============================] - 3s 189us/step - loss: 0.2115 - mse: 0.2115 - mae: 0.2847 - val_loss: 0.4083 - val_mse: 0.4083 - val_mae: 0.3612\n",
      "Epoch 440/2000\n",
      "14152/14152 [==============================] - 3s 185us/step - loss: 0.2076 - mse: 0.2076 - mae: 0.2821 - val_loss: 0.3892 - val_mse: 0.3892 - val_mae: 0.3461\n",
      "Epoch 441/2000\n",
      "14152/14152 [==============================] - 3s 181us/step - loss: 0.2243 - mse: 0.2243 - mae: 0.2911 - val_loss: 0.3970 - val_mse: 0.3970 - val_mae: 0.3589\n",
      "Epoch 442/2000\n",
      "14152/14152 [==============================] - 3s 187us/step - loss: 0.2122 - mse: 0.2122 - mae: 0.2849 - val_loss: 0.4055 - val_mse: 0.4055 - val_mae: 0.3571\n",
      "Epoch 443/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.2169 - mse: 0.2169 - mae: 0.2866 - val_loss: 0.4044 - val_mse: 0.4044 - val_mae: 0.3617\n",
      "Epoch 444/2000\n",
      "14152/14152 [==============================] - 3s 189us/step - loss: 0.2126 - mse: 0.2126 - mae: 0.2854 - val_loss: 0.3866 - val_mse: 0.3866 - val_mae: 0.3505\n",
      "Epoch 445/2000\n",
      "14152/14152 [==============================] - 3s 184us/step - loss: 0.2160 - mse: 0.2160 - mae: 0.2855 - val_loss: 0.3948 - val_mse: 0.3948 - val_mae: 0.3495\n",
      "Epoch 446/2000\n",
      "14152/14152 [==============================] - 3s 189us/step - loss: 0.2097 - mse: 0.2097 - mae: 0.2832 - val_loss: 0.4052 - val_mse: 0.4052 - val_mae: 0.3515\n",
      "Epoch 447/2000\n",
      "14152/14152 [==============================] - 3s 182us/step - loss: 0.2096 - mse: 0.2096 - mae: 0.2817 - val_loss: 0.3946 - val_mse: 0.3946 - val_mae: 0.3497\n",
      "Epoch 448/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.2087 - mse: 0.2087 - mae: 0.2835 - val_loss: 0.3816 - val_mse: 0.3816 - val_mae: 0.3462\n",
      "Epoch 449/2000\n",
      "14152/14152 [==============================] - 3s 195us/step - loss: 0.2057 - mse: 0.2057 - mae: 0.2810 - val_loss: 0.3839 - val_mse: 0.3839 - val_mae: 0.3484\n",
      "Epoch 450/2000\n",
      "14152/14152 [==============================] - 3s 200us/step - loss: 0.2091 - mse: 0.2091 - mae: 0.2815 - val_loss: 0.4079 - val_mse: 0.4079 - val_mae: 0.3601\n",
      "Epoch 451/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.2130 - mse: 0.2130 - mae: 0.2862 - val_loss: 0.4067 - val_mse: 0.4067 - val_mae: 0.3558\n",
      "Epoch 452/2000\n",
      "14152/14152 [==============================] - 3s 195us/step - loss: 0.2109 - mse: 0.2109 - mae: 0.2831 - val_loss: 0.3917 - val_mse: 0.3917 - val_mae: 0.3510\n",
      "Epoch 453/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.2067 - mse: 0.2067 - mae: 0.2813 - val_loss: 0.4109 - val_mse: 0.4109 - val_mae: 0.3565\n",
      "Epoch 454/2000\n",
      "14152/14152 [==============================] - 3s 187us/step - loss: 0.2097 - mse: 0.2097 - mae: 0.2831 - val_loss: 0.4461 - val_mse: 0.4461 - val_mae: 0.3691\n",
      "Epoch 455/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.2136 - mse: 0.2136 - mae: 0.2852 - val_loss: 0.4017 - val_mse: 0.4017 - val_mae: 0.3506\n",
      "Epoch 456/2000\n",
      "14152/14152 [==============================] - 3s 181us/step - loss: 0.2073 - mse: 0.2073 - mae: 0.2823 - val_loss: 0.4023 - val_mse: 0.4023 - val_mae: 0.3523\n",
      "Epoch 457/2000\n",
      "14152/14152 [==============================] - 3s 188us/step - loss: 0.2035 - mse: 0.2035 - mae: 0.2777 - val_loss: 0.4082 - val_mse: 0.4082 - val_mae: 0.3587\n",
      "Epoch 458/2000\n",
      "14152/14152 [==============================] - 3s 187us/step - loss: 0.2135 - mse: 0.2135 - mae: 0.2846 - val_loss: 0.4132 - val_mse: 0.4132 - val_mae: 0.3566\n",
      "Epoch 459/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.2064 - mse: 0.2064 - mae: 0.2806 - val_loss: 0.4076 - val_mse: 0.4076 - val_mae: 0.3535\n",
      "Epoch 460/2000\n",
      "14152/14152 [==============================] - 3s 180us/step - loss: 0.2083 - mse: 0.2083 - mae: 0.2817 - val_loss: 0.3962 - val_mse: 0.3962 - val_mae: 0.3509\n",
      "Epoch 461/2000\n",
      "14152/14152 [==============================] - 3s 191us/step - loss: 0.2156 - mse: 0.2156 - mae: 0.2872 - val_loss: 0.3952 - val_mse: 0.3952 - val_mae: 0.3511\n",
      "Epoch 462/2000\n",
      "14152/14152 [==============================] - 3s 190us/step - loss: 0.2087 - mse: 0.2087 - mae: 0.2819 - val_loss: 0.4086 - val_mse: 0.4086 - val_mae: 0.3566\n",
      "Epoch 463/2000\n",
      "14152/14152 [==============================] - 3s 194us/step - loss: 0.2113 - mse: 0.2113 - mae: 0.2833 - val_loss: 0.3965 - val_mse: 0.3965 - val_mae: 0.3514\n",
      "Epoch 464/2000\n",
      "14152/14152 [==============================] - 3s 187us/step - loss: 0.2027 - mse: 0.2027 - mae: 0.2786 - val_loss: 0.4150 - val_mse: 0.4150 - val_mae: 0.3626\n",
      "Epoch 465/2000\n",
      "14152/14152 [==============================] - 3s 179us/step - loss: 0.2144 - mse: 0.2144 - mae: 0.2865 - val_loss: 0.3984 - val_mse: 0.3984 - val_mae: 0.3547\n",
      "Epoch 466/2000\n",
      "14152/14152 [==============================] - 3s 181us/step - loss: 0.2090 - mse: 0.2090 - mae: 0.2813 - val_loss: 0.3973 - val_mse: 0.3973 - val_mae: 0.3521\n",
      "Epoch 467/2000\n",
      "14152/14152 [==============================] - 3s 199us/step - loss: 0.2226 - mse: 0.2226 - mae: 0.2915 - val_loss: 0.4033 - val_mse: 0.4033 - val_mae: 0.3565\n",
      "Epoch 468/2000\n",
      "14152/14152 [==============================] - 3s 183us/step - loss: 0.2139 - mse: 0.2139 - mae: 0.2835 - val_loss: 0.4109 - val_mse: 0.4109 - val_mae: 0.3607\n",
      "Epoch 469/2000\n",
      "14152/14152 [==============================] - 3s 191us/step - loss: 0.2089 - mse: 0.2089 - mae: 0.2835 - val_loss: 0.3850 - val_mse: 0.3850 - val_mae: 0.3523\n",
      "Epoch 470/2000\n",
      "14152/14152 [==============================] - 3s 186us/step - loss: 0.2073 - mse: 0.2073 - mae: 0.2825 - val_loss: 0.4131 - val_mse: 0.4131 - val_mae: 0.3611\n",
      "Epoch 00470: early stopping\n",
      "CPU times: user 27min 30s, sys: 4min 13s, total: 31min 44s\n",
      "Wall time: 21min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64, activation='relu',\\\n",
    "                input_dim=train_features.shape[1]))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "# add output layer\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "\n",
    "\n",
    "opt=keras.optimizers.Adam(learning_rate=5e-5, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(optimizer=opt, loss=\"mse\", metrics=[\"mse\",\"mae\"])\n",
    "\n",
    "# enable early stopping based on mean_squared_error\n",
    "earlystopping=EarlyStopping(monitor=\"val_loss\", patience=200, verbose=10, mode='auto')\n",
    "\n",
    "# fit model\n",
    "result = model.fit(train_features, train_labels, epochs=2000, batch_size=32,\\\n",
    "                   validation_data=(eval_features, eval_labels),callbacks=[earlystopping])\n",
    "# get predictions\n",
    "eval_pred = model.predict(eval_features)\n",
    "\n",
    "test_pred=model.predict(test_features_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEWCAYAAABsY4yMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deZxfVX3/8dfnu88+mSWTSQaykEAWzAIBWdRSVAREgUoVC9af9SH11w3tJv60/dXWn7Wr1tb+FCvVVqrlB+IGyiaLIAJJCCELZCPLZJktM5n9u57fH+c7k8nqJOTOd3Ln/Xw85jEz97ucc+98533PPffcc805h4iIhE+k1BUQEZFgKOBFREJKAS8iElIKeBGRkFLAi4iElAJeRCSkFPAiIiGlgJcpycx2mNnbSl0PkSAp4EVEQkoBLzKGmX3EzLaa2QEz+4GZzSwuNzP7gpm1m9lBM1tnZucXH7vWzDaaWZ+Z7TGzPy7tWoh4CniRIjO7Evhr4L1AM7AT+E7x4auAtwDnArXA+4Cu4mNfB37bOVcFnA/8dAKrLXJcsVJXQGQSuQW4yzm3BsDMPgl0m9kcIAtUAQuB551zm8a8LgssNrOXnHPdQPeE1lrkONSCFzlkJr7VDoBzrh/fSp/lnPsp8C/Al4E2M7vTzKqLT30PcC2w08yeNLNLJ7jeIsekgBc5ZC8we+QXM6sA6oE9AM65LznnLgSW4Ltq/qS4/AXn3PXAdOB7wD0TXG+RY1LAy1QWN7PUyBc+mD9kZsvNLAl8DnjOObfDzC4yszeaWRwYAIaBvJklzOwWM6txzmWBXiBfsjUSGUMBL1PZg8DQmK83A38G3AfsA84Bbi4+txr4Gr5/fSe+6+bvi499ANhhZr3AR4FbJ6j+IidkuuGHiEg4qQUvIhJSgQ6TNLMdQB++TzLnnFsZZHkiInLIRIyD/1XnXOcElCMiImOoi0ZEJKQCPclqZq/hRx044KvOuTuP8ZzbgNsAKioqLly4cGFg9RERCZvVq1d3Oucaj/VY0AE/0zm318ymA48Av++ce+p4z1+5cqVbtWpVYPUREQkbM1t9vPObgXbROOf2Fr+3A/cDFwdZnoiIHBJYwJtZhZlVjfyMn41vfVDliYjI4YIcRdME3G9mI+X8l3PuJwGWJyIiYwQW8M657cCy1/s+2WyW1tZWhoeHT0OtJq9UKkVLSwvxeLzUVRGRkJj088G3trZSVVXFnDlzKB4NhI5zjq6uLlpbW5k7d26pqyMiITHpx8EPDw9TX18f2nAHMDPq6+tDf5QiIhNr0gc8EOpwHzEV1lFEJtYZEfC/TFvvMH3D2VJXQ0RkUglFwHf0pelP5wJ5756eHv71X//1pF937bXX0tPTE0CNRETGJxQBDxDUBbnHC/h8/sQ37XnwwQepra0NplIiIuMw6UfRjEeQvdd33HEH27ZtY/ny5cTjcSorK2lubmbt2rVs3LiRG264gd27dzM8PMztt9/ObbfdBsCcOXNYtWoV/f39XHPNNbzpTW/i5z//ObNmzeL73/8+ZWVlAdZaROQMC/jP/HADG/f2HrV8MJMjFomQiJ38AcnimdX873ctOe7jn//851m/fj1r167liSee4J3vfCfr168fHc541113UVdXx9DQEBdddBHvec97qK+vP+w9tmzZwre//W2+9rWv8d73vpf77ruPW2/VXd1EJFhnVMBPBhdffPFhY9W/9KUvcf/99wOwe/dutmzZclTAz507l+XLlwNw4YUXsmPHjgmrr4hMXWdUwB+vpb1xby81ZXFmTQu+26OiomL05yeeeIJHH32UZ599lvLycq644opjjmVPJpOjP0ejUYaGhgKvp4hIaE6y+innT7+qqir6+vqO+djBgweZNm0a5eXlvPLKK/ziF78IpA4iIqfijGrBH5cFFe9QX1/P5Zdfzvnnn09ZWRlNTU2jj1199dV85StfYenSpZx33nlccsklAdVCROTkBXrDj5N1rBt+bNq0iUWLFp3wdZv29VKVitEyrTzI6gVuPOsqIjJWyW74MaEmz35KRGRSCEXAG8p3EZEjhSLgRUTkaOEIeE3EKCJylFAEvLpoRESOFoqAD3ScpIjIGSokAQ9ukiR8ZWVlqasgIgKEKOBFRORwobiSNci73X3iE59g9uzZ/M7v/A4Af/EXf4GZ8dRTT9Hd3U02m+Wzn/0s119/fXCVEBE5BWdWwP/4Dtj/8lGLF2T6yVsM4qmTf88Zb4BrPn/ch2+++WY+9rGPjQb8Pffcw09+8hM+/vGPU11dTWdnJ5dccgnvfve7dV9VEZlUzqyAL4EVK1bQ3t7O3r176ejoYNq0aTQ3N/Pxj3+cp556ikgkwp49e2hra2PGjBmlrq6IyKgzK+CP09Iu7H2JgUgNNTPmBFLsTTfdxL333sv+/fu5+eabufvuu+no6GD16tXE43HmzJlzzGmCRURK6cwK+BOwAEfR3HzzzXzkIx+hs7OTJ598knvuuYfp06cTj8d5/PHH2blzZ2Bli4icqtAEfJCDJJcsWUJfXx+zZs2iubmZW265hXe9612sXLmS5cuXs3DhwgBLFxE5NaEJ+KBPb7788qGTuw0NDTz77LPHfF5/f3/ANRERGZ9QjIN3mqxAROQooQh4ERE52hkR8JPprlNBmQrrKCITa9IHfCqVoqur65cH4BkckM45urq6SKVO4UItEZHjmPQnWVtaWmhtbaWjo+O4z8n3tJO1BKmezATW7PRKpVK0tLSUuhoiEiKTPuDj8Thz58494XO6/uo6Xoqv4Mo77p2gWomITH6TvotmPBwRNIpGRORwgQe8mUXN7EUz+1FQZTgiZ3QfvIhIECaiBX87sCnIApyBUQiyCBGRM06gAW9mLcA7gX8Lshy14EVEjhZ0C/6LwJ/C8ZvXZnabma0ys1UnGilzIg4LdLIxEZEzUWABb2bXAe3OudUnep5z7k7n3Ern3MrGxsZTKstZBHPqohERGSvIFvzlwLvNbAfwHeBKM/tWMEWZ+uBFRI4QWMA75z7pnGtxzs0BbgZ+6py7NZCyMI2SFBE5QjjGwVtELXgRkSNMyJWszrkngCcCe3910YiIHCUULXjMMA2TFBE5TCgCXlMViIgcLRQBj0XUghcROUIoAl598CIiRwtFwGO6klVE5EihCHjNRSMicrRwBLypi0ZE5EihCHgwIuqiERE5TCgC3pmGSYqIHCkUAQ+m2SRFRI4QioD3c9GoBS8iMlYoAh4zIjrJKiJymFAEvAvHaoiInFbhSEbd0UlE5CghCXgNkxQROVI4Al5z0YiIHCUUAe/HwYuIyFjhSEaLaBSNiMgRQhHwDt3RSUTkSKEIeNM4eBGRo4Qi4Ef64J1a8SIio0IR8OD74JXvIiKHhCPgi+PgC0p4EZFRIQl4P9lYQfkuIjIqFAHvivdkVQteROSQUAS8Hwfv1AcvIjJGOAIeP0xSLXgRkUPCEfAWwUABLyIyRngC3pzmkxQRGSM0AR+hgKaEFxE5JBwBj8bBi4gcKRwBH4lomKSIyBHCEfDFYZK60ElE5JCQBLy/0EmTjYmIHBJYwJtZysyeN7OXzGyDmX0mqLIOjYMPrgQRkTNNLMD3TgNXOuf6zSwOPG1mP3bO/eJ0F2QaBy8icpTAAt75/pL+4q/x4lcgCeyKwyQV8CIihwTaB29mUTNbC7QDjzjnnjvGc24zs1Vmtqqjo+NUyyn2wb/OCouIhEigAe+cyzvnlgMtwMVmdv4xnnOnc26lc25lY2PjqRU0OopGCS8iMmJCRtE453qAJ4CrAylA88GLiBwlyFE0jWZWW/y5DHgb8EowhakFLyJypCBH0TQD3zSzKH5Hco9z7kdBFGTFW/ZpHLyIyCFBjqJZB6wI6v0PYxFQF42IyGFCciWrumhERI4UooAvUNB0wSIio0IR8GZG1NSCFxEZKxwBH/Grkc+rCS8iMiIUAR+JRAHIqY9GRGRUKAJ+pAWfy+VKXBMRkckjFAEfMb8ahXy+xDUREZk8whHw0WILXl00IiKjQhHw/mJZyKsFLyIyKhQBH4kYoIAXERlrXAFvZrebWbV5XzezNWZ2VdCVG6/IyElWBbyIyKjxtuB/yznXC1wFNAIfAj4fWK1O0sgwybwmoxERGTXegLfi92uBf3fOvTRmWcmNBHyhoGGSIiIjxhvwq83sYXzAP2RmVcCkGbJixT74XG7SVElEpOTGO13wh4HlwHbn3KCZ1eG7aSaF6GgXjQJeRGTEeFvwlwKvOud6zOxW4NPAweCqdXIOzUWjLhoRkRHjDfj/Cwya2TLgT4GdwH8EVquTNHqSVZONiYiMGm/A55y/H971wD855/4JqAquWicnGvV98IWChkmKiIwYbx98n5l9EvgA8ObifVbjwVXr5IzOJpnXMEkRkRHjbcG/D0jjx8PvB2YBfxdYrU5SJKphkiIiRxpXwBdD/W6gxsyuA4adc5OnD950ww8RkSONd6qC9wLPA78OvBd4zsxuCrJiJ2NkqgJNFywicsh4++A/BVzknGsHMLNG4FHg3qAqdlJGWvCaqkBEZNR4++AjI+Fe1HUSrw3e6A0/1AcvIjJivC34n5jZQ8C3i7+/D3gwmCqdiuJ0wbqSVURk1LgC3jn3J2b2HuByfJre6Zy7P9CanYxiC95pHLyIyKjxtuBxzt0H3BdgXU6dqQUvInKkEwa8mfUBxzpzaYBzzlUHUquTpWGSIiJHOWHAO+cmzXQEJ2SaqkBE5EiTZyTM6zHSgncaJikiMiJUAa+TrCIih4Qj4CO+p8nlsyWuiIjI5BGOgE9U+G+5wRJXRERk8ghHwCf9ueBYfqDEFRERmTwCC3gzO8vMHjezTWa2wcxuD6osEj7gk3m14EVERoz7QqdTkAP+yDm3xsyqgNVm9ohzbuNpL6nYgk+oBS8iMiqwFrxzbp9zbk3x5z5gE/5GIadfstJ/U8CLiIyakD54M5sDrACeO8Zjt5nZKjNb1dHRcWoFxMspECFZGHo91RQRCZXAA97MKvFz2HzMOdd75OPOuTudcyudcysbGxtPtRCGI2WkCmrBi4iMCDTgzSyOD/e7nXPfDbKsYSsnpRa8iMioIEfRGPB1YJNz7h+DKmdEOlpOqqBRNCIiI4JswV8OfAC40szWFr+uDaqwTLSCpAJeRGRUYMMknXNPM3KrpQmQi5WTOrqLX0RkygrHlaxANlZJuVMLXkRkRGgC3sXKSLk0Od30Q0QECFHAEy8jZVmGspoyWEQEQhTwFk+RIsNQRgEvIgIhCnji5STJMKiAFxEBQhTw0UQZScsxlM6UuioiIpNCaAI+kigDYHhII2lERCBEAR9NlAOQHtJ8NCIiEKKAjyWLAT+sgBcRgRAGfE4BLyIChCjgEykf8Nm0+uBFRCCMAT+sgBcRgRAFfDxVAUA+o4AXEYEQBfxIC76Q0U0/REQgRAFvcR/weQW8iAgQooAnngLAZdVFIyICYQr4mL+S1WWHS1wREZHJITwBP9qCVxeNiAiEKeCLLfhITgEvIgKhCvgkOaJE1QcvIgKEKeDNGIxUkcrpxtsiIhCmgAeGolWUFfpKXQ0RkUkhVAE/HKumIq+AFxGBkAV8Jl5NhesvdTVERCaF0AV8lQJeRAQIWcDnEjXU0E82Xyh1VURESi5UAZ9P1lDNIIPpbKmrIiJScqEKeJeqJWKO4b6eUldFRKTkQhXwVlEHwFDP/hLXRESk9EIV8Knp5wIwsHdjiWsiIlJ6oQr4qrOXAlDYv6HENRERKb1QBXxjQz27C40kDrxa6qqIiJRcqAI+FY+yPTKbmt4tpa6KiEjJhSrgAfan5tGQ3gW5TKmrIiJSUoEFvJndZWbtZrY+qDKOpbd6ATHy0Ll5IosVEZl0gmzBfwO4OsD3P6Z48/kAZPe+NNFFi4hMKoEFvHPuKeBAUO9/PHWzl9DqGsg/889Q0JQFIjJ1lbwP3sxuM7NVZraqo6Pjdb/fuTOn8aXcjaS6NkHby6ehhiIiZ6aSB7xz7k7n3Ern3MrGxsbX/X7zGip5IbLM/7Lz2df9fiIiZ6qSB/zplohFaD57Ae2RRtjxs1JXR0SkZEIX8ACXzKvngcwFuM0PQX97qasjIlISQQ6T/DbwLHCembWa2YeDKutIVy1p4j/zb4dCDh76FDg3UUWLiEwaQY6ieb9zrtk5F3fOtTjnvh5UWUdaOKOa8pkL+Xb5LfDyPbDmmxNVtIjIpBHKLhqAdyyewae7rybTcik8/jnIpUtdJRGRCRXagL9y0XQKLsLPZnwQ+tvgwT8udZVERCZUaAN+cXM158+q5v+8MoPcxR+FNf8Bz/wTFPKlrpqIyIQIbcCbGR9/27ls7xzkj/Zf5Rc+8uew6q7SVkxEZIKENuAB3rqoib+8fgnf3zzMMxd+0S986u+gfVNpKyYiMgFCHfAAH7hkNmfVlXFnxxL46NPgCvCvl8DTXyh11UREAhX6gDcz3r1sJk9t6eDftlSQv+pz/oFH/wIOvFbSuomIBCn0AQ/wu786n5Wzp/HZBzbxic3n4T78KFgUfvgHMHyw1NUTEQnElAj48kSMe377Uv7gyvncu7qVO7fXwfX/Ajt/Dn+3AH54O/TsKnU1RUROqykR8FAcVfP2c3nrwul86bEtrJ52Ndz4VcinYfU34ItvgOe/VupqioicNlMm4MGH/CevXURZIsavf+VZvtS2lPxN34Bb7oVZF8JPPun75nt2l7qqIiKvm7lJNBHXypUr3apVqwIvZyCd49PfW8/9L+7hsnPq+cL7ltMUG4RvXAftG/yT4hWw6Dp442/78B/ognQv1M0NvH4iIuNlZqudcyuP9diUasGPqEjG+Mf3LuNv37OUtbt7uPHLz/BCO/DRn8HvPAcXfgjK62Hdf8N3boXOLX5o5ZeWQ3a41NUXERmXKdmCH2v9noN86Bsv0NGX5p1vaOaOaxZyVl25f3DH0/Ct90BuTKjHyuCsi+GNH4XzrgGzCa2viMhYJ2rBT/mABzgwkOFT97/Mj9fvJxmLcNOFLXzo8jnMn14F3TvgP38N5lwOe9fC/nWHXjjrQmhaAoveDfvWQt05kB2CWRfA9EUTvh4iMvUo4Mchmy/wi+1dfHfNHh54eR+ZXIGm6iRDmTxfufUCLphdR76nlYpN9/gx9I995sRveMFvQvUsSFRAJA7182HOmyCempgVEpEpQQF/ktp6h/nei3tYt+cg61p72H1gCDOYWVPGZ969hIJzXNE4QMJykB2E7/8+DPdAJAbdJ7g69tyrYfpi2Ph9mLnCBz5AxytQ0QiRKCy5EabN8XehSvfCQCccbPXdRJXT/etjyQnZDiIy+SngX4e9PUP88KW9rGs9yAMv7xtdXl+RYPlZtUQiRlUyxhvn1XHZOQ3Myu/Fvvtb2Ds+57t0Wp/3obz5Idi7ZhwlGix7P7z6oN9pHClZAys/BJVNsPF70LzMB377JnjzH8Hsy/zznPvl5wf62qCiAYa6/ffjyecgGoNdz/n3POvicayHyBS3Z40fdVc2LdBiFPCnyYa9B3lxVw/1FQn+6/ldbO8YoG84iwP6hnMkYhFiEWMwk+f65TO5eG4d171hJjXlcR+4rasgWQV9e6H9Fdj2U38zknd9EWIpPw7/tSchUQkLr4NUDXRtgY7N0Ns6vkrWz4fefZAdgJaLofZs2PULH9BnXwr5DESTUNkIP/9nmLHUnz948x/7ncX8t8Fglw/ymhbY9EO45zdh2W/A2m/5Mm66y++0+vbD1kdh/8uw4gN+WGm8zD+nvwO+fBG843Ow/DcOr2NmwHddHU8h749mwiY7DC/9l99W0Xipa3O0rm2Q6fefA4BCwX8ORhoKG74HDQv8eafMoD+qLK879PrMAMTLYcfP/GcvyO7Irm1+pFtZ7eHLh3v9/1ipBz8c3ANfWAzzroDf/P7Rj49tgGUGIZrw/6OnQAEfsGy+wCv7+rj7uZ3cs2o3hTGbtDoV44LZ06grT9Del2ZPzxBzGyq47Jx6zq4rZ1FzNQ2VSaIRw+XTJF57HJv/1sO7YXJpf/HVcA80L/czYh7YDhbxre9ps/2O4mf/CP3tsOvnh15bNROqZ8Ke421XA8ZUuLy+GPARmL7E74wGu8a3IVI1cN0XfV22PgZbH/HLpy+Giz/i69K23t9Ccfn7YcVvwnNf8e/ftRUWXAVDB2DbE/Cr/8ufqE5U+J3QSCCa+X+Oxz/nu7YWvcvvEGe8wd9/d9n7/foe3AMPfwpW3OpXr2uL7xJ75QH/mj2roXIGnHuVP9Jyed/SqmzyQQW+Swz8fEWtL/hts2c1nHetL8M52PmMf13TEt9ie+wzsPwWmPsr/oT8grf7I6DNP/bbZPW/w3Vf8DvizAAseAdk+nz33shOL5eG/7wR5r8V5hdvHp/PQO8e//fOZSBZ6Z9X1ezrM9Dhw27XL/wOe8P9vhHReJ7/rFz2+77MZJX/DO1b69el7hzfGDj3GviHc335S270XYa7n/efgyU3wr6XYP29/vEP/gju+7D/O1/9N/6z2r0Dnv0X//5D3dBykd8OdXN9OZt+CI2L/BFm/Xz/Odj2U7jgg3Bwt//sLPk1/xl/7qsQS8C0uf4q83lXFLszDQbafVkPf9p/rj74I9j+uH//2rPhha9Dy0qY9ytQMd1/tmcshabzDzUa8llfZvdrMOctvlH16o/932r1N/xObNlvwKYf+G3w1j/zf+v2jfDit/zrF13nd2Y9u2DxDbDlIf/Z69oGL/4HdLwKnZt9eee906/v0vf6v9kL/wblDXDZ78GWR/xnq2oGfORxSJSP739tDAX8BMoXHKt2HKClrpydXQP89wu72d4xwIGBDA1VSWbWpHhlfx+vdQ4c9rqKRJThXIEF0yu5YcUsCs6xuLma+ookD23Yzw0rZnJOYyVPb+2kuaaM+dMrcc5hx2qp9LX5ncBABzQv9S2xtvU+vDZ8158AzmcgVQs9O/3In+1P+g/93rUwa4X/QHdthXSfHxL6wtf8qKHND0P9PP8PtfxWOP9GP8XD5p/4i8OyY9arusW36HNpOPg65voprwfM17ms9sTzBsXLfd1zQ+N77/lvh22P+e0F/khq7LDY6Ut8kA12Hlo2EnrZYXj1gRO/f+1sv4MY290WTfh1Ad9abn/FT5kBPgT79/uQHC+LHKr/iKY3QNvL43+P40lU+lb9yRjpkjiZdQCOamycSCTmd3wnIxL3IV/IHXpt2bTD65ms9ufVTva9wX/2soOHfq+d7T+r5fV+p7Hr2eO/dsmN8OvfOPkyUcBPSq3dg7y6v4/ndxxgZ+cg9ZUJCs7x/GsH2NYxcMzXVCSiDGT8LQdn1ZaRKxRIxaNcuXA6DZVJFjdX+3MCqRjJWIRoxKhOxXl1fx9vXtBALHrourZcvkDBQSJ2ite6DR44/PC8a5tv9W191J9ATtUcejw77FtJ8TL/TzBzhd+xbLgfzrnSPzc77Fu5hYJvgaV7fWu0v913BZXV+q/Orb5FuODtfi6hvv2+Bbfuv+GN/xO2P+HLXPst38219H2+9X+wFZ74vG+N7V8P577D3xNg5zP+H/GcK33rOlEF0xf6VlWiyp9vSFXD3Lf48xznXu1bXeu+44P70t+DV37kW8kAt9zndxhdW/101Mkqf3RQv8C/Pt0HVU2+5brpB7DlYX9kM9DuQ6XpfB8GVTP9ehXyxZagg0t+1+8YmpdC40IfTntWw1N/7498Flzlh+w2LfFHdfvW+aOCWNLv7KNJf9TTtsEH0aW/C717/d/ugT/yR1lv+RO/A9rxlG9lltfB3heLV3N3wn/e4APxpn/3Rw7bHvPLG86Fs97oj0ZSNf6oZe3dfjte+Wf+ff6qeJ6nsskH6W8/5Y+62l+Bmct93Qp5/zk6/9f8dp77Fn8+6qefhVkr4ew3+i7Bt/+l//7qj/0R2fy3+brM/RVfVnbIH1Hl0/DyfZA+6Ee1jRwJxst9i3/LI/5oaNn7/d3eLvsDf2S18Xv+bzHnTf7vVHu2/5tUNPp16tnl6xJNwEvf9p+zJz7vd+TX/RN0bILzb/J/t2gSIhF/NFDI+XWvbPL/AxUN8Phf+89lTcsp/Ssq4M8g+YJj9c5uZtam2NYxwK6uAS6cXcczWzvZd3CYgnNUJKO8uKuHsniUvQeHaT0wSF/62C2OiEHBQcu0Mt6+uImhTJ6+dI51rT3EIxFuf9sC5tRXcGAgw4u7e+gZzHDd0pksaq5i1Y5uDgxkuGHFLCIG3/z5DnqGstz2lnkYRjIWIRIpQV9nLuMP4U+kv90HzdiurkLB/6ONyOd8f/Hsy4oh2OkDMV7mD8ML+eP3IzvnQzJR4Xd2Q90+fE/m+oeBLn9Etez9Pvij8WOf7B7PCfPXK5f2YfXLyuna5ruK5r7l5MvYv96v58wVfqd3ovMwR5qIbXCGUsBPAWt39/Cd53ex4uxa6iuS9AxleWl3DwcGMyyZWc19q1tHjwzK4lFS8Qjdg9lxvXdVKkZzTYrNbf5QvaEywYGBDNVlcebUV3BWXTnnNVVSU55gMJ2jrTdNvlAgnSuwZFYNVckYZtDem2bNrm6WzKzmg5fNoTIZo2sgw5OvdlCRjPKrC6eTjB19ctU5R0dfmunVuoZA5EgKeCFfcPSncxQKjmkVvvU7nM2ztb2f7Z0DJKIRLj2nHoAH1u1jR9cA8xoqKEtEee61A2zY28vbFk5nUXM1X3/6NeY0lNPRl6Gjb5i23jT7ew+foycW8S38kS6l8ahKxpjbWEEsYnT0p3nT/EbOaazgyc0d/GxLJ+WJKBfNqaNvOEs8GiFixrVLm5lZk+LlPX6E01VLmphTX0FNWZz1ew6yoKmS3QeGWHF2Lat3drO0pZaIQTbvOG9GFYOZHKt2dHPejCqmVyXpS+eoTk3CES4ix6GAl8C19w2Dg1zBETGjtjxOMhZh475eCgXo7E9TmYpRFo/y0Ib9bNrXSyoeZTCT5w/ffi5tvcM8sG4fuw4MEokYEYM1O3vI5As016TYd9DvQJa21FAWj1JwjtbuodHlp2JxczX7e4c5MJAhGYuMHlFUJKJMq0jgiucoGiuTdA2kqSmL81rnADVlcWZNK+O8pmqGsnle3tNDZ1+GskSU65Y201iV5JJ59by+BJwAAAqASURBVPzdQ68SNWMgk2NZSy3vWjaTl1p7qEzGOGtaOdGIkc0XGM7miUUjNFX70VRb2vqpSsWoKYuTLzjW7u5hZ9cgv3flfAbSOfrTOWrLE/QOZWmsSpKKh3BIqYybAl7OSD2DGfIFR31lksFMju7BLLNqy0YfLxQcz2zrpOAgakZDlT8y2dU1SEd/mlm1ZbT3pmmoSrBpXx/VZXHS2Ty9Q1lSiSgPb2ijviLB9StmsWZnN8PZPNMqEnx3TSuxSIQDAxmGsnnKE1FappUxrTxBVSrOutYe6iuTbGnrI1dwVKdiDGXzZPOv738pGYtQcO6k3qemLE4iFqGjL01FIkpTTYrOvjQza8uIRyPs7Rniknn1DGZylCWiNFWn6B7IkMkXeMeSGbR2DzGUydPaPUgqHuW1zgHevKCBuQ2V5AoFWruHSMYi5AqOre399AxmmD+9ijcvaMA5SMYjvNYxQMu0MuY3VVKditM1kGFzWx9LmqupSvkdfc+Q7w5s7xumrjzBtIoEg5k8qXiEgXSeuopD51R+vrWTc6ZX0nSCLrmu/jQVyZh2bijgRU7KyPDTfMGxt2fo0OyiRygUHC+19jCvsZJ8wVFbFmfT/l4A7l+zh2uXNjO3voKBTI61u3voHcpxblMlFckY61p72HdwmLqKBDOqU2xu6+PhjW2k4lF+6/K5bO/sxznoHcpSV5Fg/vRKHt7QRsE55jRUsHFfL4ubq3m59SCtPYOs3+PLnT+9knkNFbT1pWnvHWbfwWESMX900D2QxTlHdfHIoL0vPbouTdVJhrMFDg4d/7zMtPI46VyBwZPodgOoTMboP2IQwMjlDCMWNVfTXJPixV3do+eGzq4rJ53L01CZpLkmRXkiNroze2ZrJ3MbKnj/xWez7+AQq3Z2s6yl1j+2rZP3rTyL4Wyejft66ehL88a59dRXJsg7R+9QjpZpZQxn8zRVp1izq5tzGiuZ21DBzq5BNu3r5bwZVbxhVg3Pbu9iZm0Zc+sreLWtj20d/dQWzz399JV2Vpxdy/TqJM01ZcyuL6e2LMGrbX1UJmOUJaLs6hpkUXMVrd1D1JTF2dzWx1l15UTMeHRTG3t7hvjAJbOZ21Bx2Ci3k6GAFwm5/nSOZPFK6rHXRqRzeQoFKEsc3tLNFxzPbutiOJvn0nPqqUj6qyidc2zr6CeTc6zeeQAzY25DBQeHslxxXiMdfWk27u0l7xyVyRgOmF6VZHNbH139GQbSearLYsypr+CZrZ2UJaK09Q5zTmMlHX1pFjRVksk7trX301Sd4sBAmrJ4lFU7u0ePmHZ2+bHkIztD52AokyedyzOYydPel2bhjCr29gzRO5wjFjHmNFSwtb2fqmSMgnMMZPLEo8bSllqqUzFW7eymb9jvZFLxCMPZI64bGKOpOklbb/qYj0WLo8byhdObm9PK47z451ed0msV8CJyxsgXHNl84bjdL3t7hmiuSZEvOHqGstSWxYlGjJ1dgzRWJdnTM0T3QIZlZ9WOvkeh4MjkC2zvGODcpkoODGZwDh7asJ+rFs9g7e5uplenmF1XTn1lkm0d/bT1DrNoRjXbOwfoT+eYVVvGjJoUPYMZfralk4vm1FFwjnS2wO7uQXoGs7T3DXNuUxX96RzD2TzNNWWs2nmAZCxKdSpGY1WS9t40u7sHuWRePefPrOHZ7Z0MZfL8j8tP7W5xCngRkZDSLftERKYgBbyISEgp4EVEQkoBLyISUoEGvJldbWavmtlWM7sjyLJERORwgQW8mUWBLwPXAIuB95vZ4qDKExGRwwXZgr8Y2Oqc2+6cywDfAa4PsDwRERkjyICfBewe83trcdlhzOw2M1tlZqs6OjoCrI6IyNRyand5HZ9jzc5/1FVVzrk7gTsBzKzDzHaeYnkNQOcvfVa4aRtoG0z19Yeptw1mH++BIAO+FThrzO8twN4TvcA513iqhZnZquNdzTVVaBtoG0z19Qdtg7GC7KJ5AVhgZnPNLAHcDPwgwPJERGSMwFrwzrmcmf0e8BAQBe5yzm0IqjwRETlckF00OOceBB4Msowx7pygciYzbQNtg6m+/qBtMGpSzSYpIiKnj6YqEBEJKQW8iEhInfEBP1XmuzGzu8ys3czWj1lWZ2aPmNmW4vdpYx77ZHGbvGpm7yhNrU8vMzvLzB43s01mtsHMbi8unzLbwcxSZva8mb1U3AafKS6fMtsA/FQoZvaimf2o+PuUWv9xc86dsV/40TnbgHlAAngJWFzqegW0rm8BLgDWj1n2t8AdxZ/vAP6m+PPi4rZIAnOL2yha6nU4DdugGbig+HMVsLm4rlNmO+AvIKws/hwHngMumUrboLhefwj8F/Cj4u9Tav3H+3Wmt+CnzHw3zrmngANHLL4e+Gbx528CN4xZ/h3nXNo59xqwFb+tzmjOuX3OuTXFn/uATfjpL6bMdnBef/HXePHLMYW2gZm1AO8E/m3M4imz/ifjTA/4cc13E2JNzrl94MMPmF5cHvrtYmZzgBX4FuyU2g7F7om1QDvwiHNuqm2DLwJ/ChTGLJtK6z9uZ3rAj2u+myko1NvFzCqB+4CPOed6T/TUYyw747eDcy7vnFuOn/7jYjM7/wRPD9U2MLPrgHbn3OrxvuQYy87Y9T9ZZ3rAn/R8NyHTZmbNAMXv7cXlod0uZhbHh/vdzrnvFhdPue0A4JzrAZ4ArmbqbIPLgXeb2Q58l+yVZvYtps76n5QzPeCn+nw3PwA+WPz5g8D3xyy/2cySZjYXWAA8X4L6nVZmZsDXgU3OuX8c89CU2Q5m1mhmtcWfy4C3Aa8wRbaBc+6TzrkW59wc/P/7T51ztzJF1v+klfos7+v9Aq7Fj6bYBnyq1PUJcD2/DewDsvhWyYeBeuAxYEvxe92Y53+quE1eBa4pdf1P0zZ4E/7weh2wtvh17VTaDsBS4MXiNlgP/Hlx+ZTZBmPW6woOjaKZcus/ni9NVSAiElJneheNiIgchwJeRCSkFPAiIiGlgBcRCSkFvIhISCngRU4DM7tiZGZDkclCAS8iElIKeJlSzOzW4nzqa83sq8WJu/rN7B/MbI2ZPWZmjcXnLjezX5jZOjO7f2SOcTObb2aPFudkX2Nm5xTfvtLM7jWzV8zs7uKVtyIlo4CXKcPMFgHvAy53frKuPHALUAGscc5dADwJ/O/iS/4D+IRzbinw8pjldwNfds4tAy7DX2EMfnbLj+HnIJ+HnzdFpGRipa6AyAR6K3Ah8EKxcV2Gn5SqAPx38TnfAr5rZjVArXPuyeLybwL/z8yqgFnOufsBnHPDAMX3e94511r8fS0wB3g6+NUSOTYFvEwlBnzTOffJwxaa/dkRzzvR/B0n6nZJj/k5j/6/pMTURSNTyWPATWY2HUbv4zkb/39wU/E5vwE87Zw7CHSb2ZuLyz8APOn8/POtZnZD8T2SZlY+oWshMk5qYciU4ZzbaGafBh42swh+Zs7fBQaAJWa2GjiI76cHP+3sV4oBvh34UHH5B4CvmtlfFt/j1ydwNUTGTbNJypRnZv3OucpS10PkdFMXjYhISKkFLyISUmrBi4iElAJeRCSkFPAiIiGlgBcRCSkFvIhISP1/iUzhoDIwpL4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot loss\n",
    "plt.plot(result.history['loss'])\n",
    "plt.plot(result.history['val_loss'])\n",
    "plt.title('Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#Plot accuracy\n",
    "plt.plot(result.history['mse'])\n",
    "plt.plot(result.history['val_mse'])\n",
    "plt.title('mse')\n",
    "plt.ylabel('mse')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#Plot accuracy\n",
    "plt.plot(result.history['mae'])\n",
    "plt.plot(result.history['val_mae'])\n",
    "plt.title('mae')\n",
    "plt.ylabel('mae')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.8950067521758913, 1: 1.9684926439908486, 2: 2.561203360557556}\n",
      "0.8687379581996866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x2b99c4cd0>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dist = Counter(reduce_train['accuracy_group'])\n",
    "for k in dist:\n",
    "    dist[k] /= len(reduce_train)\n",
    "#reduce_train['accuracy_group'].hist()\n",
    "\n",
    "preds=list(eval_pred)\n",
    "acum = 0\n",
    "bound = {}\n",
    "for i in range(3):\n",
    "    acum += dist[i]\n",
    "    bound[i] = np.percentile(preds, acum * 100)\n",
    "print(bound)\n",
    "def classify(x):\n",
    "    if x <= bound[0]:\n",
    "        return 0\n",
    "    elif x <= bound[1]:\n",
    "        return 1\n",
    "    elif x <= bound[2]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "    \n",
    "test_class = list(map(classify, preds))\n",
    "print(cohen_kappa_score(test_class, test_labels, weights = 'quadratic'))\n",
    "pd.DataFrame(list(test_class)).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.01999819092452526, 1: 0.07516937814084339, 2: 0.7311442494392395}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x2bad6ff90>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dist = Counter(reduce_train['accuracy_group'])\n",
    "for k in dist:\n",
    "    dist[k] /= len(reduce_train)\n",
    "#reduce_train['accuracy_group'].hist()\n",
    "\n",
    "preds=list(test_pred)\n",
    "acum = 0\n",
    "bound = {}\n",
    "for i in range(3):\n",
    "    acum += dist[i]\n",
    "    bound[i] = np.percentile(preds, acum * 100)\n",
    "print(bound)\n",
    "def classify(x):\n",
    "    if x <= bound[0]:\n",
    "        return 0\n",
    "    elif x <= bound[1]:\n",
    "        return 1\n",
    "    elif x <= bound[2]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "    \n",
    "test_class = list(map(classify, preds))\n",
    "pd.DataFrame(list(test_class)).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion - \n",
    "<br>Much better class separation can be found after Keras regression and\n",
    "assigned to classes by distribution<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kaggle_bowl] *",
   "language": "python",
   "name": "conda-env-kaggle_bowl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
